Keyword: RSA
Occurrences: 2911
================================================================================

Page    4: 2.4.3   Efficient adversaries and attack games . . . . . . . . . . . . . . . . . . . . .        32
Page    7: 7 Message integrity from universal hashing                                                         248
Page    7: 7.1 Universal hash functions (UHFs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
Page    9: 10.3 A trapdoor permutation scheme based on RSA . . . . . . . . . . . . . . . . . . . . . 398
Page    9: 10.3.1 Key exchange based on the RSA assumption . . . . . . . . . . . . . . . . . 400
Page    9: 10.6.2 Collision resistance based on RSA . . . . . . . . . . . . . . . . . . . . . . . 410
Page    9: 11.4.1 Instantiating ETDF with RSA . . . . . . . . . . . . . . . . . . . . . . . . . . 434
Page   10: 12.3.1 Instantiating ETDF    with RSA . . . . . . . . . . . . . . . . . . . . . . . . . . 468
Page   10: 12.5.1 Universal projective hash functions . . . . . . . . . . . . . . . . . . . . . . . 474
Page   10: 12.5.2 Universal2 projective hash functions . . . . . . . . . . . . . . . . . . . . . . 476
Page   10: 12.8.3 Bleichenbacher’s attack on the RSA-PKCS1 encryption scheme . . . . . . . 494
Page   10: 13.3.1 Signatures based on the RSA trapdoor permutation . . . . . . . . . . . . . 522
Page   10: 13.5 An RSA-based signature scheme with tighter security proof . . . . . . . . . . . . . . 530
Page   12: 17.1 How reasonable are the factoring and RSA assumptions? . . . . . . . . . . . . . . . 607
Page   13: 19.5.4 A Sigma protocol for RSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . 682
Page   18: a network while maintaining the secrecy of m in the presence of an eavesdropping adversary. This
Page   22: If Alice encrypts a message m under a key k, and an eavesdropping adversary obtains the
Page   22: c, without knowledge of k; however, this is not really enough. Even though the adversary may
Page   22: uniformly from among all keys in the key space. The adversary may also have some knowledge of
Page   22: and that based on the adversary’s available intelligence, Alice is equally likely to choose either one
Page   22: of these two messages. This, without seeing the ciphertext c, the adversary would only have a
Page   22: 50% chance of guessing which message Alice sent. But we are assuming the adversary does know
Page   22: that E(k0 , m0 ) = c, and 600 keys k1 such that E(k1 , m1 ) = c. If that is the case, the adversary’s
Page   24: “standard header,” such as "FROM". Suppose the ciphertext is c 2 ⌃L is intercepted by an adversary.
Page   24: The secret key is actually a permutation k on ⌃. The adversary knows that
Page   24: Thus, if the original message is m 2 ⌃L , the adversary can now locate all positions in m where
Page   24: information about letter frequencies, the adversary may be able to deduce quite a bit about the
Page   24: the first, suppose an eavesdropping adversary applies some predicate           to a ciphertext he has
Page   25: it is a very bad idea to choose the key in a way that depends on the message, or vice versa (see
Page   27: way we shall do this is to consider not all possible adversaries, but only computationally feasible
Page   27: adversaries, that is, “real world” adversaries that must perform their calculations on real computers
Page   27: useful information about an encrypted message to an adversary other than the length of message.
Page   29: attack game played between two parties: the challenger and an adversary. As we will see, the
Page   29: challenger follows a very simple, fixed protocol. However, an adversary A may follow an arbitrary
Page   29: (but still efficient) protocol. The challenger and the adversary A send messages back and forth
Page   29: or “experiments” — in both experiments, the adversary follows the same protocol; however, the
Page   29: probability space, and this in turn defines the adversary’s advantage, which measures the di↵erence
Page   29: and for a given adversary A, we define two experiments, Experiment 0 and Experiment 1. For
Page   30: • The adversary computes m0 , m1 2 M, of the same length, and sends them to the challenger.
Page   30: • The challenger computes k            K, c R E(k, mb ), and sends c to the adversary.
Page   30: • The adversary outputs a bit b̂ 2 {0, 1}.
Page   30: algorithm, and the random choices made (if any) by the adversary. The value SSadv[A, E] is a
Page   30: adversaries A, the value SSadv[A, E] is negligible.
Page   30: “messages of the same length”, “efficient adversaries”, and “negligible”. We will come back to this
Page   30: Let us relate this formal definition to the discussion preceding it. Suppose that the adversary
Page   30: A in Attack Game 2.1 is deterministic. First, the adversary computes in a deterministic fashion
Page   31: computed by the adversary Attack Game 2.1 be of the same length.
Page   31: • Second, the requirement that m0 and m1 be of the same length means that the adversary is not
Page   31: for every adversary A (efficient or not), we have SSadv[A, E] = 0. This follows almost immediately
Page   31: from Theorem 2.3 (the only slight complication is that our adversary A in Attack Game 2.1 may
Page   31: the one-time pad (see Example 2.1), we have SSadv[A, E] = 0 for all adversaries A; in particular,
Page   31: length one-time pad (see Example 2.2), then SSadv[A, E] = 0 for all adversaries A; in particular,
Page   31: • An efficient adversary is one that runs in a “reasonable” amount time.
Page   32: running time of an efficient adversary is poly-bounded.
Page   32: Intuitively, in a message recovery attack, an adversary is given an encryption of a random message,
Page   32: adversary A that can e↵ectively mount a message recovery attack on E can be used to build an
Page   32: efficient adversary B that breaks the semantic security of E; since semantic security implies that no
Page   32: an adversary.
Page   32: and for a given adversary A, the attack game proceeds as follows:
Page   32: K, c R E(k, m), and sends c to the adversary.
Page   32: • The adversary outputs a message m̂ 2 M.
Page   32: recovery if for all efficient adversaries A, the value MRadv[A, E] is negligible.
Page   33: versary A has negligible advantage in Attack Game 2.2. To show this, we let an arbitrary but
Page   33: efficient adversary A be given, and our goal now is to show that A’s message recovery advantage,
Page   33: We shall show how to construct an efficient adversary B whose semantic security advantage in
Page   33: Here is how B works. Adversary B generates two random messages, m0 and m1 , and sends
Page   33: it were coming from A’s MR challenger. When A outputs a message m̂, our adversary B compares
Page   33: encryption of m1 , the adversary A’s output is independent of m0 , and so p1 = 1/|M|. It follows
Page   33: The core of the proof was establishing the following fact: for every efficient MR adversary A
Page   33: that attacks E as in Attack Game 2.2, there exists an efficient SS adversary B that attacks E as in
Page   34: secure against message recovery. This means there exists an efficient adversary A whose message
Page   34: recovery advantage is non-negligible. Using A we build an efficient adversary B that satisfies (2.6).
Page   34: we show how to turn an efficient adversary that breaks message recovery into an efficient adversary
Page   34: We also stress that the adversary B constructed in the proof just uses A as a “black box.” In
Page   34: Thus, we will say adversary B is an elementary wrapper around adversary A when it can be
Page   34: m, it is hard to predict parity(m). Now, since parity(m) is a single bit, any adversary can predict
Page   34: that no efficient adversary can do significantly better than random guessing.
Page   34: As a warm up, suppose there were an efficient adversary A that could predict parity(m) with
Page   34: adversary B that works as follows. Our adversary chooses two messages, m0 and m1 , arbitrarily,
Page   34: challenger, obtaining a ciphertext c, which it then forwards to it A. After receiving c, adversary
Page   35: This shows that if E is semantically secure, there is no efficient adversary that can predict
Page   35: efficient adversary that can predict parity with probability significantly better than 1/2. To make
Page   35: and for a given adversary A, the attack game proceeds as follows:
Page   35: K, c R E(k, m), and sends c to the adversary.
Page   35: • The adversary outputs b̂ 2 {0, 1}.
Page   35: efficient adversaries A, the value Parityadv[A, E] is negligible.
Page   35: that for every parity prediction adversary A that attacks E as in Attack Game 2.3, there exists an
Page   35: SS adversary B that attacks E as in Attack Game 2.1, where B is an elementary wrapper around
Page   35: Let A be a parity prediction adversary that predicts parity with probability 1/2 + ✏, so
Page   35: Here is how we construct our SS adversary B.
Page   35: Our adversary B generates a random message m0 , and sets m1          m0 (0L 1 k 1); that is, m1
Page   35: Our adversary B sends the pair m0 , m1 to its own SS challenger, receives a ciphertext c from
Page   35: that challenger, and forwards c to A. When A outputs a bit b̂, our adversary B outputs 1 if
Page   36: We have shown that if an adversary can e↵ectively predict the parity of a message, then it can
Page   36: be used to break semantic security. Conversely, it turns out that if an adversary can break semantic
Page   36: the adversarial nature of the player, and assume that A’s strategy can be modeled as an efficient
Page   37: every player A, there exists an SS adversary B, where B is an elementary wrapper around A, such
Page   37: efficient SS adversary B that breaks the semantic security of E, which we are assuming is impossible.
Page   37: To motivate and analyze our new adversary B, consider an “idealized” version of Internet
Page   37: Our adversary B is designed to play in Attack Game 2.1 so that if b̂ denotes B’s output in that
Page   37: The logic of adversary B is illustrated in Fig. 2.4. It is clear by construction that B satisfies the
Page   38: Figure 2.4: The SS adversary B in Attack Game 2.1
Page   39: a system, what we view as “the adversary” depends on what we are trying to do. In the above
Page   39: analysis, we cobbled together a new adversary B out of several components: one component was
Page   39: the original adversary A, while other components were scavenged from other parts of the system
Page   39: we consider to be “the adversary” at that point in the analysis.
Page   39: this alternative characterization, we define a new attack game. The role played by the adversary
Page   39: b 2 {0, 1} at random and runs Experiment b of Attack Game 2.1; it is the adversary’s goal to guess
Page   39: defined over (K, M, C), and for a given adversary A, the attack game runs as follows:
Page   39: • The adversary computes m0 , m1 2 M, of the same length, and sends them to the challenger.
Page   39: K, c R E(k, mb ), and sends c to the adversary.
Page   39: • The adversary outputs a bit b̂ 2 {0, 1}.
Page   39: any) by the adversary.
Page   39: Of course, any adversary can win the game with probability 1/2, simply by ignoring c completely
Page   39: 1). What we are interested in is how much better than random guessing an adversary can do. If
Page   39: W denotes the event that the adversary wins the bit-guessing version of the attack game, then we
Page   40: Theorem 2.10. For every cipher E and every adversary A, we have
Page   40: Proof. This is just a simple calculation. Let p0 be the probability that the adversary outputs 1 in
Page   40: Experiment 0 of Attack Game 2.1, and let p1 be the probability that the adversary outputs 1 in
Page   40: of the other random choices made by the challenger and the adversary are distributed in exactly
Page   40: the output of the adversary in Attack Game 2.4, we have
Page   41: Experiment 0 and Experiment 1, where the adversary A’s protocol is the same in both experiments,
Page   41: protocol. If W is the event that the adversary’s output is equal to b, then we define
Page   41: • for a semantically secure cipher, we required that any efficient adversary have a negligible
Page   42: • Second, when we speak of an efficient adversary, we usually mean an algorithm that runs in
Page   42: that an adversary that is trying to break a cryptosystem is willing to expend many more
Page   42: patient, and financially well-o↵ adversary. However, in some settings, like the Internet roulette
Page   42: example in Section 2.3.4, the adversary may have a much more limited amount of time to
Page   42: • Third, when we speak of an adversary’s advantage as being negligible, we mean that it is so
Page   42: we saw in the Internet roulette example, if no efficient adversary has an advantage better
Page   43: of and ⇤ are public and known to everyone (including the adversary).
Page   46: 2.4.3     Efficient adversaries and attack games
Page   46: In defining the notion of semantic security, we have to define what we mean by an efficient adversary.
Page   46: between an adversary A and a challenger: A follows an arbitrary protocol, while the challenger
Page   46: security under discussion. Furthermore, both adversary and challenger take as input a common
Page   46: parameter ⇤, and sending this to the adversary.
Page   46: technicalities. We assume this restriction from now on, for adversaries as well as for any other type
Page   47: We naturally model an adversary as an interactive machine. An efficient adversary is simply
Page   47: Naturally, we can model the interaction of a challenger and an adversary by connecting two
Page   47: such machines together as above: the challenger becomes the open machine, and the adversary
Page   47: In our security reductions, we typically show how to use an adversary A that breaks some
Page   47: system to build an adversary B that breaks some other system. The essential property that we
Page   48: Thus, we will say adversary B is an elementary wrapper around adversary A when it can be
Page   48: interface. For such a challenger and any efficient adversary A, we can view their entire interaction
Page   48: Query bounded adversaries. In the attack games we have seen so far, the adversary makes
Page   48: just a fixed number of queries. Later in the text, we will see attack games in which the adversary
Page   48: In defining any type of security, we will define the adversary’s advantage in the attack game as a
Page   48: challenger, and the random choices made the adversary. Security will mean that for every efficient
Page   48: adversary, the function Adv(·) is negligible.
Page   48: proper interpretation of Definition 2.2 is that E is secure if for all efficient adversaries A (modeled as
Page   48: is negligible (as defined in Definition 2.5). Recall that both challenger and adversary receive
Page   48: adversary. The adversary then sends its query to the challenger, which consists of two plaintexts,
Page   48: who responds with a ciphertext. Finally, the adversary outputs a bit (technically, in our formal
Page   48: choice of system parameter) and the random choices of the adversary. See Fig. 2.6 for a complete
Page   49: Also, in Attack Game 2.1, the requirement that the two messages presented by the adversary
Page   49: definition. This means that there exists an adversary A such that SSadv[A, E] is a non-negligible
Page   51: Let us argue that if E is semantically secure then no efficient adversary can learn any information
Page   51: • The adversary gives the challenger (m0 , m1 , d) where m0 , m1 2 M are equal length messages
Page   51: It sends c to the adversary along with all keys k0 , . . . , kn 1 , but excluding the key kd .
Page   51: • The adversary outputs a bit b̂ 2 {0, 1}.
Page   51: This game captures the fact that the adversary sees all keys k0 , . . . , kn 1 except for kd and tries to
Page   51: We define the adversary’s advantage, NE(n) adv[A, E], as in the definition of semantic security:
Page   51: In particular, for every n-way nested adversary A attacking En , there exists a semantic security
Page   51: adversary B attacking E, where B is an elementary wrapper around A, such that
Page   52: efficient adversary whose semantic security advantage is 1.
Page   52: rem 2.5). Let E be a cipher defined over (K, M, C). Suppose that SSadv[A, E]  ✏ for all adversaries
Page   52: (a) there is an efficient adversary A such that SSadv[A, E] = ✏/(1        ✏);
Page   52: (b) for all adversaries A, even including computationally unbounded ones, SSadv[A, E]  ✏/(1 ✏).
Page   53: SSadv[A, E] = 0 for every adversary A (bearing in mind that A may be probabilistic); also show
Page   53: that if E is the variable length one-time pad, then SSadv[A, E] = 0 for all adversaries A.
Page   53: attack is modeled by the following game between a challenger and an adversary A: the challenger
Page   53: key recovery attacks if for all efficient adversaries A the advantage KRadv[A, E] is negligible.
Page   53: recovery attacks. In particular, show that for every efficient key-recovery adversary A there
Page   53: is an efficient semantic security adversary B, where B is an elementary wrapper around A,
Page   54: Hint: Your semantic security adversary B will output 1 with probability KRadv[A, E] in the
Page   54: TAILS) and sends the result to the adversary A.
Page   54: • In Experiment 1 the challenger always sends TAILS to the adversary.
Page   54: The adversary’s goal is to distinguish these two experiments: at the end of each experiment the
Page   54: adversary outputs a bit 0 or 1 for its guess for which experiment it is in. For b = 0, 1 let Wb
Page   54: be the event that in experiment b the adversary output 1. The adversary tries to maximize its
Page   54: If the advantage is negligible for all efficient adversaries then we say that the two experiments are
Page   54: (a) Calculate the advantage of each of the following adversaries:
Page   54: showing an adversary that achieves advantage 1.
Page   55: adversary is given one of the keys k0 or k1 .
Page   55: ment b, for b = 0, 1, the adversary generates two messages m0 and m1 and gets back k1 and
Page   55: E 0 (k0 , k1 ), mb ). The adversary outputs b̂ in {0, 1} and we define its advantage, NEadv[A, E]
Page   55: adversary A attacking E 0 , there exists a semantic security adversary B attacking E, where B
Page   55: (b) Repeat part (a), but now when the adversary gets back k0 (instead of k1 ) and E 0 (k0 , k1 ), mb )
Page   55: insecure if the adversary is given Ẽ(k, k). You have just shown that semantic security does
Page   55: semantically secure (provably) even if the adversary is given Ê(k, k). To prove that Ê is
Page   55: semantically secure, you should show the following: for every adversary A that attacks Ê,
Page   55: there exists and adversary B that attacks E such that (i) the running time B is about the
Page   56: (a) Formulate a security notion that captures the advantage that an adversary has in break-
Page   56: (b) Show that for every 2-way key splitting adversary A there is a semantic security adversary B
Page   57: game (i.e., Attack Game 2.4). Suppose an efficient adversary A wins the game (i.e., guesses the
Page   57: another efficient adversary B that wins the game with probability 1/2+✏0 , where ✏0 is non-negligible
Page   57: (a) Consider the following adversary B that uses A as a subroutine in Attack Game 2.4 in the
Page   58: (b) One might be tempted to argue as follows. Just construct an adversary B that runs A, and
Page   58: when A outputs b̂, adversary B outputs b̂ 1. Now, we do not know if ✏ is positive or
Page   58: our requirements. Although we do not know which one of these two adversaries satisfies our
Page   58: (c) Can you come up with another efficient adversary B 0 that wins the bit-guessing game with
Page   58: probability at least 1 + |✏|/2? Your adversary B 0 will be less efficient than B.
Page   59: Suppose s is a random `-bit string and r is a random L-bit string. Intuitively, if an adversary cannot
Page   59: that an adversary cannot “e↵ectively tell the di↵erence between G(s) and r.”
Page   60: not tend to exploit deeper mathematical properties of the algorithm G that a malicious adversary
Page   60: G(s) of sufficient length, the adversary can compute all the remaining bits of G(s), or perhaps even
Page   60: from S and r is chosen at random from R, then no efficient adversary can e↵ectively tell the
Page   60: Attack Game 3.1 (PRG). For a given PRG G, defined over (S, R), and for a given adversary
Page   61: and sends r to the adversary.
Page   61: • Given r, the adversary computes and outputs a bit b̂ 2 {0, 1}.
Page   61: all efficient adversaries A.
Page   61: runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
Page   62: determined by the random choices of the challenger and the random choices of the adversary.
Page   62: Second, the challenger generates a system parameter ⇤, and sends this to the adversary at the very
Page   63: In particular, for every SS adversary A that attacks E as in Attack Game 2.1, there exists a
Page   63: PRG adversary B that attacks G as in Attack Game 3.1, where B is an elementary wrapper
Page   63: string, without a↵ecting the adversary’s advantage by more than a negligible amount. However,
Page   63: after making this replacement, the adversary’s advantage is zero. 2
Page   63: Proof. Let A be an efficient adversary attack E as in Attack Game 2.1. We want to show that
Page   63: for some efficient adversary B. Then (3.2) follows from Theorem 2.10. Moreover, by the assumption
Page   63: So consider the adversary A’s attack of E in the bit-guessing version of Attack Game 2.1. In
Page   63: ciphertext c to A; finally, A outputs a bit b̂. The adversary A wins the game if b̂ = b.
Page   65: Figure 3.4: The PRG adversary B in the proof of Theorem 3.1
Page   65: This is because in Game 1, the adversary is attacking the variable length one-time pad. In particu-
Page   65: lar, it is easy to see that the adversary’s output b̂ and the challenger’s hidden bit b are independent.
Page   65: Finally, we show how to construct an efficient PRG adversary B that uses A as a subroutine,
Page   65: This is actually quite straightforward. The logic of our new adversary B is illustrated in Fig. 3.4.
Page   65: In words, adversary B, which is a PRG adversary designed to attack G (as in Attack Game 3.1),
Page   66: the PRG challenger is running Experiment 0, then adversary A is essentially playing our Game 0,
Page   66: efficient SS adversary that attacks E, then there exists an efficient PRG adversary B that attacks
Page   66: that SSadv[A, E] is also negligible. Since this holds for all efficient adversaries A, we conclude that
Page   66: efficient adversary A such that SSadv[A, E] is non-negligible, and the reduction (and the above
Page   66: inequality) gives us an efficient adversary B such that PRGadv[B, G] is also non-negligible. That
Page   66: “feeling”: one starts with an adversary A that breaks E, and shows how to use A to construct a
Page   66: new adversary B that breaks G.
Page   66: this new attack game, it was quite easy to compute the adversary’s advantage. Also, we used an
Page   66: appropriate security assumption to show that the di↵erence between the adversary’s advantages in
Page   66: the original and the modified games was negligible. This was done by exhibiting a new adversary
Page   66: negligible, the new adversary would “break” the underlying cryptographic primitive.
Page   67: A moments reflection shows that this construction is insecure in a very strong sense. An adversary
Page   67: given = m1 m2 the adversary can recover both m1 and m2 in the clear. Hence, the construction
Page   67: Although semantic security ensures that an adversary cannot read the plaintext, it provides no
Page   67: guarantees for integrity. When using a stream cipher, an adversary can change a ciphertext and
Page   69: In particular, for every PRG adversary A that attacks G0 as in Attack Game 3.1, there exists
Page   69: a PRG adversary B that attacks G as in Attack Game 3.1, where B is an elementary wrapper
Page   69: adversary that has advantage ✏ in attacking G0 in Attack Game 3.1. We want to show that ✏ is
Page   69: highlighted). Intuitively, under the assumption that G is a secure PRG, the adversary A should
Page   69: we can easily construct an efficient PRG adversary B1 whose advantage in attacking G in Attack
Page   69: Game 3.1 is precisely equal to 1 . The adversary B1 works as follows:
Page   70: assuming that G is a secure PRG. Indeed, we can easily construct an efficient PRG adversary B2
Page   70: whose advantage in Attack Game 3.1 with respect to G is precisely equal to 2 . The adversary B2
Page   70: of two adversaries B1 and B2 , our second proof combines these two adversaries into a single PRG
Page   70: adversary B that plays Attack Game 3.1 with respect to G, and which runs as follows:
Page   70: upon receiving r 2 R from its challenger, adversary B chooses ! 2 {1, 2} at random,
Page   71: R. Intuitively, the adversary should not notice any of these replacements, since G is a secure
Page   71: PRG; however, proving this formally would require the construction of n di↵erent adversaries,
Page   71: more convenient to extend the second strategy outlined above, constructing a single adversary that
Page   71: Proof. Let A be an efficient PRG adversary that plays Attack Game 3.1 with respect to G0 . We
Page   71: We next define a PRG adversary B that plays Attack Game 3.1 with respect to G, and which
Page   73: In particular, for every PRG adversary A that plays Attack Game 3.1 with respect to G0 , there
Page   73: exists a PRG adversary B that plays Attack Game 3.1 with respect to G, where B is an elementary
Page   74: Theorem 3.2. The intuition behind the proof is as follows: Consider a PRG adversary A who
Page   74: a single PRG adversary that attacks G. 2
Page   74: Proof. Let A be a PRG adversary that plays Attack Game 3.1 with respect to G0 . We first introduce
Page   75: We next define a PRG adversary B that plays Attack Game 3.1 with respect to G, and which
Page   77: to show that if there is an efficient adversary A that breaks G0 , then there is an efficient adversary
Page   77: that breaks G. Suppose that A is an efficient adversary that breaks G0 , so that its advantage ✏( )
Page   77: and showed that there exist efficient adversaries B1 and B2 , such that ✏( )  1 ( ) + 2 ( ) for all ,
Page   77: G (or possibly both). Thus, there exists an efficient adversary that breaks G: it is either B1 or
Page   77: adversary that is defined uniformly for all ; that is, it is a fixed machine that takes as input.
Page   77: saries B1 , . . . , Bn , and argue that for some j = 1, . . . , n, adversary Bj must have advantage 1/n c
Page   77: constructed a fixed adversary B that is defined uniformly for all ; that is, B is a fixed machine
Page   77: it means to talk about n( ) adversaries B1 , . . . , Bn( ) : our adversaries are supposed to be fixed
Page   77: confusion aside, our proof for the constant case only shows that there exists an “adversary” that for
Page   77: adversary B, as we did in the proofs of Theorems 3.2 and 3.3. However, we reiterate that the original
Page   77: finite collection of adversaries, each of which is a fixed machine. We reiterate this because in the
Page   78: There are a number of ways an adversary might be able to distinguish a pseudo-random output of
Page   78: G from a truly random bit string. Indeed, suppose that an efficient adversary were able to compute,
Page   78: such an adversary would imply that G is insecure, since given the first L 1 bits of a truly random
Page   78: bit) with probability significantly better that 1/2 (here, i is an adversarially chosen index). We
Page   78: given adversary A, the attack game proceeds as follows:
Page   78: • The adversary sends an index i, with 0  i  L          1, to the challenger.
Page   78: and sends r[0 . . i   1] to the adversary.
Page   78: • The adversary outputs g 2 {0, 1}.
Page   78: is negligible for all efficient adversaries A.
Page   78: In particular, for every adversary A breaking the unpredictability of G, as in Attack Game 3.2,
Page   78: there exists an adversary B breaking the security of G as in Attack Game 3.1, where B is an
Page   79: Proof. Let A be an adversary breaking the unpredictability of G, and let i denote the index chosen
Page   79: We build an adversary B breaking the security of G, using A as a subroutine, as follows:
Page   79: an efficient adversary that can e↵ectively distinguish a pseudo-random L-bit string from a random
Page   79: L-bit string, then we can construct an efficient adversary B that can e↵ectively distinguish
Page   79: Thus, adversary B can distinguish the pseudo-random bit xj+1 from the random bit rj+1 , given
Page   79: as any random coins used by the adversary B;
Page   81: In particular, for every adversary A breaking the security of G as in Attack Game 3.1, there
Page   81: exists an adversary B, breaking the unpredictability of G as in Attack Game 3.2, where B is an
Page   82: generally in the Chapter 5, builds them from a more versatile primitive called a pseudorandom
Page   93: In response, RSA Labs issued a recommendation suggesting that one discard the first 1024 bytes
Page   95: adversary should not be able to e↵ectively distinguish between G(s) and r, where s is a randomly
Page   95: that an adversary cannot e↵ectively distinguish between P0 and P1 . As usual, this is done via an
Page   95: P1 on a finite set R, and for a given adversary A, we define two experiments, Experiment 0 and
Page   96: and sends x to the adversary.
Page   96: • Given x, the adversary computes and outputs a bit b̂ 2 {0, 1}.
Page   96: adversaries A.
Page   96: and then runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing
Page   96: that no adversary can e↵ectively distinguish between them, regardless of how much computing
Page   96: power the adversary may have. To make this notion of “similarity” precise, we introduce a useful
Page   98: adversary A, we have
Page   98: Proof. Consider an adversary A that tries to distinguish P0 from P1 , as in Attack Game 3.3.
Page   99: be the uniform distribution on {0, . . . , m 1}, and let A be an adversary trying to distinguish P0
Page  100: in Game 1. One can easily construct an efficient adversary B that attacks G as in Attack Game 3.1,
Page  100: security parameter is an input to both the challenger and adversary, and in Experiment b, the
Page  100: all distribution families P0 = {P0, } and P1 = {P1, } , for all adversaries (even computationally
Page  102: rity for random messages. Here, one modifies Attack Game 2.1 so that instead of the adversary
Page  103: attack game between an adversary A and a challenger as follows. The adversary selects a message
Page  103: efficient adversaries.
Page  103: cipher, called psuedo-random ciphertext security, which intuitively says that no efficient adversary
Page  103: from the ciphertext space C at random. We define an attack game between an adversary A and a
Page  103: challenger as follows. The adversary selects a message m 2 M and sends m to the challenger. The
Page  103: negligible for all efficient adversaries.
Page  103: 2|S|. Let us show that |S| must be super-poly. To do so, show that there is an adversary that
Page  105: Next, define the following PRG adversary B0 that attacks G0 :
Page  105: Now, let G00 be the n-wise parallel composition of G0 . Using B0 , we construct a PRG adversary A0
Page  105: (c) Show that no adversary attacking G0 has a better advantage than B0 (hint: make an argument
Page  106: adversary A, there exists an efficient adversary B such that
Page  106: defined over (K, M, C), where M = {0, 1}. Show that for every efficient adversary A that receives
Page  106: suppose that A is an efficient adversary whose advantage in Attack Game 3.2 is non-negligible.
Page  106: that the challenger sends r[i + 1 . . L 1] to the adversary. Also, express B’s previous-bit prediction
Page  107: and suppose A is an adversary that given G(s) outputs s with non-negligible probability. Show
Page  107: how to use A to construct a PRG adversary B that has non-negligible advantage in attacking G as
Page  108: ition is the following: an efficient adversary is given a “black box.” Inside the box is a permutation
Page  109: The adversary cannot see inside the box, but he can “probe” it with questions: he can give the
Page  109: box a value x 2 X , and obtain the value y := f (x) 2 X . We allow the adversary to ask many
Page  109: Security means that the adversary should not be able to tell which type of function is inside the
Page  109: the adversary follows the same protocol; namely, it submits a sequence of queries x1 , x2 , . . . to
Page  109: When the adversary tires of querying the challenger, it outputs a bit.
Page  109: a given adversary A, we define two experiments, Experiment 0 and Experiment 1. For b = 0, 1, we
Page  110: • The adversary submits a sequence of queries to the challenger.
Page  110: The challenger computes yi          f (xi ) 2 X , and gives yi to the adversary.
Page  110: • The adversary computes and outputs a bit b̂ 2 {0, 1}.
Page  110: Finally, we say that A is a Q-query BC adversary if A issues at most Q queries. 2
Page  110: Definition 4.1 (secure block cipher). A block cipher E is secure if for all efficient adversaries
Page  110: that is, the adversary need not choose all its queries in advance; rather, it is allowed to concoct
Page  110: runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
Page  110: means that every efficient adversary wins the following prediction game with negligible probability.
Page  110: In this game, the challenger chooses a random key k, and the adversary submits a sequence of
Page  110: the adversary outputs a pair of values (xQ+1 , y), where xQ+1 2       / {x1 , . . . , xQ }. The adversary wins
Page  110: adversary A that wins the above prediction game with non-negligible probability p. Then we can
Page  112: use A to break the security of E in the sense of Definition 4.1. To this end, we design an adversary
Page  112: Whenever A makes a query xi , adversary B passes xi through to its own challenger, obtaining a
Page  112: response yi , which it passes back to A. Finally, when A outputs (xQ+1 , y), adversary B submits
Page  112: every efficient adversary wins the following key-recovery game with negligible probability. In this
Page  112: game, the adversary interacts with the challenger exactly as in the prediction game, except that at
Page  112: there is an efficient adversary A that wins the key-recovery game with non-negligible probability p.
Page  112: Then we can use A to build an efficient adversary B that wins the prediction game with probability
Page  112: at least p. Adversary B simply runs A’s attack, and when A outputs k , adversary B chooses an
Page  112: One way to see this is as follows. An adversary can always win the key-recovery game with
Page  112: is non-negligible. Hence, when |K| is not super-poly this simple key guessing adversary wins the
Page  112: search attack. In this attack, our adversary makes a few, arbitrary queries x1 , . . . , xQ in the key-
Page  112: So our adversary simply tries all possible keys to find one that satisfies (4.2). If there is only
Page  112: one such key, then the key that our adversary finds will be the key chosen by the challenger, and
Page  112: the adversary will win the game. Thus, our adversary wins the key-recovery game with all but
Page  112: This time/advantage trade-o↵ can be easily generalized. Indeed, consider an adversary that
Page  112: adversary is linear in t, and it wins the key-recovery game with probability ⇡ t/|K|.
Page  113: ensures that the function is a permutation); finally, he sends yi to the adversary. We can write the
Page  113: a stronger notion of security by defining an attack game in which the adversary is allowed to make
Page  113: forward queries: the adversary sends a value xi 2 X to the challenger, who sends yi := f (xi ) to
Page  113: the adversary;
Page  114: inverse queries: the adversary sends a value yi 2 X to the challenger, who sends xi := f 1 (yi )
Page  114: to the adversary (in Experiment 0 in the attack game, this is done using algorithm D).
Page  114: strongly secure if for all efficient adversaries, this advantage is negligible. We leave it to the
Page  115: example, an adversary can easily distinguish an encryption of two messages m0 , m1 2 X 2 , where
Page  116: Therefore, an adversary will not be able to trivially locate positions where individual characters
Page  117: In particular, for every SS adversary A that plays Attack Game 2.1 with respect to E 0 , there
Page  117: exists a BC adversary B that plays Attack Game 4.1 with respect to E, where B is an elementary
Page  117: Proof idea. The basic idea is that if an adversary is given an encryption of a message, which is a
Page  117: Let A be an efficient adversary that attacks E 0 as in Attack Game 2.1. Our goal is to show that
Page  117: for some efficient adversary B. Then (4.3) follows from Theorem 2.10.
Page  117: So consider the adversary A’s attack of E 0 in the bit-guessing version of Attack Game 2.1. In
Page  117: ciphertext c to A; finally, A outputs a bit b̂. The adversary A wins the game if b̂ = b.
Page  117: Intuitively, the fact that E is a secure block cipher implies that the adversary should not notice
Page  117: the switch. To prove this rigorously, we show how to build a BC adversary B that is an elementary
Page  118: The design of B follows directly from the logic of Games 0 and 1. Adversary B plays Attack
Page  118: This follows from the fact that in Game 2, the adversary’s output b̂ is a function of its own random
Page  119: the challenger and the random choices of the adversary. Second, the challenger generates a system
Page  119: parameter ⇤, and sends this to the adversary at the very start of the game. Third, the advantage
Page  125: following attack: the adversary is given a small number of plaintext blocks x1 , . . . , xQ 2 X and
Page  125: their encryption y1 , . . . , yQ using a block cipher key k in K. The adversary finds k by trying all
Page  125: found by the adversary.
Page  126: To prove that exhaustive search on DES is feasible, RSA data security setup a sequence of
Page  126: challenges, called the DES challenges. The rules were simple: on a pre-announced date RSA
Page  126: chips worked in parallel, each searching through an assigned segment of the key space. When RSA
Page  133: • Key recovery: Key recovery attacks refer to an adversary who is given multiple plain-
Page  133: • Related key attacks: In an `-way related key attack the adversary is given ` lists of
Page  133: point is that all ` keys k1 , . . . , k` must satisfy some fixed relation chosen by the adversary.
Page  134: analyzing any cryptosystem, we consider scenarios in which an adversary interacts with the users
Page  134: of a cryptosystem. During the course of these interactions, the adversary collects information that
Page  135: see, in some scenarios it is possible for the adversary to break a cryptosystem by measuring physical
Page  135: algorithmic attacks, in which the adversary can harness the laws of quantum mechanics to speed
Page  138: on the victim machine. We will assume that the adversary can accurately measure the victim’s
Page  141: works quite well against simple implementations of certain cryptosystems (such as RSA, which is
Page  143: where we show how they result in a complete break of some implementations of RSA.
Page  144: Attack Game 4.2 (PRF). For a given PRF F , defined over (K, X , Y), and for a given adversary
Page  145: • The adversary submits a sequence of queries to the challenger.
Page  145: The challenger computes yi          f (xi ) 2 Y, and gives yi to the adversary.
Page  145: • The adversary computes and outputs a bit b̂ 2 {0, 1}.
Page  145: Finally, we say that A is a Q-query PRF adversary if A issues at most Q queries. 2
Page  145: Definition 4.2 (secure PRF). A PRF F is secure if for all efficient adversaries A, the value
Page  145: Again, we stress that the queries made by the adversary in Attack Game 4.2 are allowed to be
Page  145: adaptive: the adversary is allowed to concoct each query in a way that depends on the previous
Page  145: runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
Page  145: adversary can distinguish the PRF from a random function when its queries are severely restricted:
Page  145: it can only query the function at random points in the domain. Restricting the adversary’s queries
Page  145: over (K, X , Y). We modify the way in which an adversary A interacts with the challenger: whenever
Page  145: the adversary queries the function, the challenger chooses a random x 2 X and sends both x and
Page  145: f (x) to the adversary. In other words, the adversary sees evaluations of the function f at random
Page  145: the adversary’s advantage in this game, denoted wPRFadv[A, F ], as in (4.21).
Page  146: he sends yi to the adversary. We can write the logic of this implementation of the challenger as
Page  146: no efficient adversary can e↵ectively distinguish E from a random permutation. Does this imply
Page  146: that E is also a secure PRF? That is, does this imply that no efficient adversary can e↵ectively
Page  146: Consider a PRF adversary playing Attack Game 4.2 with respect to E. Let f be the function
Page  146: f is randomly chosen from Funs[X , X ]. Suppose that N is so small that an efficient adversary can
Page  146: a↵ord to obtain the value of f (x) for all x 2 X . Moreover, our adversary A outputs 1 if it sees that
Page  146: bounded Q, we can define an efficient PRF adversary A that plays Attack Game 4.2 with respect
Page  146: to E, as follows. Adversary A simply makes Q distinct queries to its challenger, and outputs 1 i↵
Page  146: just O(N 1/2 ) queries, an adversary can easily see that a permutation does not behave like a random
Page  146: It turns out that the “birthday attack” is about the best that any adversary can do, and when
Page  146: and let N := |X |. Let A be an adversary that makes at most Q queries to its challenger. Then
Page  147: Proof. By definition, if A is an efficient adversary, the maximum number of queries Q it makes to
Page  147: attack game that tests an adversary’s ability to distinguish a random permutation from a random
Page  147: adversary A, we define two experiments, Experiment 0 and Experiment 1. For b = 0, 1, we define:
Page  147: • The adversary submits a sequence of queries to the challenger.
Page  147: The challenger computes yi             f (xi ) 2 Y, and gives yi to the adversary.
Page  147: • The adversary computes and outputs a bit b̂ 2 {0, 1}.
Page  147: Theorem 4.6. Let X be a finite set of size N . Let A be an adversary that makes at most Q queries
Page  147: Proof of Theorem 4.4. Let E = (E, D) be a block cipher defined over (K, X ). Let A be an adversary
Page  148: adversary outputs 1 in some game against a certain challenger, while W1 will be the event that the
Page  148: same adversary outputs 1 in a game played against a di↵erent challenger. To apply the Di↵erence
Page  148: space. This means that we view the random choices made by both the adversary and the challenger
Page  148: to compute its responses to the adversary’s queries.
Page  148: Proof of Theorem 4.6. Consider an adversary A that plays Attack Game 4.3 with respect to
Page  149: on the same underlying probability space. All of the random choices made by the adversary and
Page  149: the adversary’s random choices are the same in both games, its first query in both games is the
Page  149: same, and therefore the challenger’s response is the same in both games. The adversary’s second
Page  149: both games. Continuing this argument, one sees that each of the adversary’s queries and each of
Page  149: the challenger’s responses are the same in both games, and therefore the adversary’s output is the
Page  150: same in both games. Thus, if Z does not occur and the adversary outputs 1 in Game 0, then the
Page  150: adversary also outputs 1 in Game 1. Likewise, if Z does not occur and the adversary outputs 1 in
Page  150: Game 1, then the adversary outputs 1 in Game 0. More succinctly, we have W0 ^ Z̄ occurs if and
Page  150: In particular, for every PRG adversary A that plays Attack Game 3.1 with respect to G, there
Page  150: is a PRF adversary B that plays Attack Game 4.2 with respect to F , where B is an elementary
Page  150: Proof. Let A be an efficient PRG adversary that plays Attack Game 3.1 with respect to G. We
Page  150: describe a corresponding PRF adversary B that plays Attack Game 4.2 with respect to F . Adversary
Page  150: B queries its challenger at x1 , . . . , x` , obtaining responses y1 , . . . , y` . Adversary B then
Page  150: plays the role of challenger to A, giving A the value (y1 , . . . , y` ). Adversary B outputs
Page  151: ticular, for any efficient SS adversary A, there exists an efficient BC adversary B such that
Page  151: the adversary A that constructs m0 and m1 in this way, and outputs 1 if and only if the ciphertext
Page  155: In particular, for every Q-query BC adversary A that attacks E as in Attack Game 4.1, there
Page  155: exists a PRF adversary B that plays Attack Game 4.2 with respect to F , where B is an elementary
Page  155: is a secure PRF. So we want to show that if an adversary is playing in Experiment 0 of Attack
Page  155: bit strings. We may assume that the adversary never makes the same query twice. Moreover, as F
Page  155: and the adversary should hardly notice the di↵erence.
Page  155: Proof. Let A be an efficient BC adversary that plays Attack Game 4.1 with respect to E, and which
Page  155: To simplify things a bit, we replace A with an adversary A0 with the following properties:
Page  155: Adversary A0 simply runs the same protocol as A; however, it keeps a table of query/response
Page  156: send (xi , yi ) to the adversary.
Page  156: Recall that the adversary A0 is guaranteed to always make Q distinct queries (u1 , v1 ), . . . , (uQ , vQ );
Page  156: truly random functions f1 , f2 , f3 . Intuitively, since F is a secure PRF, the adversary A0 should not
Page  156: send (xi , yi ) to the adversary.
Page  156: X , X ), and F 0 ((k1 , k2 , k3 ), (s, x)) := F (ks , x). We can easily construct an adversary B 0 , just as
Page  156: Adversary B 0 simply runs A0 and outputs whatever A0 outputs; when A0 queries its challenger
Page  156: with a pair (ui , vi ), adversary B 0 computes the response (xi , yi ) for A0 by computing
Page  157: By Exercise 4.26, there exists an adversary B, just as efficient as B 0 , such that
Page  157: send (xi , yi ) to the adversary.
Page  157: send (xi , yi ) to the adversary.
Page  157: • the random choices made by the adversary, which we denote by Coins, and
Page  157: to the queries made by the adversary.
Page  160: In particular, for every PRF adversary A that plays Attack Game 4.2 with respect to F , and
Page  160: which makes at most Q queries to its challenger, there exists a PRG adversary B that plays
Page  161: Hybrid 0, . . . , Hybrid `. Each of these games is played between a given PRF adversary, attacking
Page  161: In response to a query x 2 {0, 1}` in Hybrid j, the challenger sends to the adversary the label of
Page  161: to Experiment 1. Intuitively, under the assumption that G is a secure PRG, the adversary should
Page  161: PRG adversary that attacks G, we cannot a↵ord to write down the entire tree (or even one level
Page  161: of the tree). Instead, we use the fact that if the PRF adversary makes at most Q queries to its
Page  161: first, third, and fourth nodes at level 2 for the given inputs). The PRG adversary we construct
Page  161: Proof. Let A be an efficient adversary that plays Attack Game 4.2 with respect to F . Let us assume
Page  162: We now build an efficient PRG adversary B 0 that attacks G0 , such that
Page  163: Finally, by Theorem 3.2, there exists an efficient PRG adversary B such that
Page  164: that F̃ is a PRF against restricted set of adversaries called prefix-free adversaries.
Page  164: Definition 4.5. Let F be a PRF defined over (K, X ` , Y). We say that a PRF adversary A playing
Page  164: Attack Game 4.2 with respect to F is a prefix-free adversary if all of its queries are non-empty
Page  164: secure PRF if PRFpf adv[A, F ] is negligible for all efficient, prefix-free adversaries A.
Page  164: For example, if a prefix-free adversary issues a query for the sequence (a1 , a2 , a3 ) then it cannot
Page  164: In particular, for every prefix-free adversary A that plays Attack Game 4.2 with respect to F̃ ,
Page  164: and which makes at most Q queries to its challenger, there exists a PRG adversary B that plays
Page  164: Let A be an efficient, prefix-free adversary that plays Attack Game 4.2 with respect to F̃ .
Page  165: Next, we define an efficient PRG adversary B 0 that attacks the Q-wise parallel composition G0
Page  165: Adversary B runs as follows:
Page  166: adversary that exploits the design of a particular block cipher, even one that is secure in the sense
Page  166: to property X) and an arbitrary adversary A. Presumably, in responding to certain queries, the
Page  166: for all efficient adversaries A.
Page  166: above, to which both the adversary and the challenger have oracle access. More precisely, the game
Page  166: • In addition to its standard queries, the adversary A may submit ideal cipher queries. There
Page  166: – For a ⇧-query, the adversary submits a pair (k , a ) 2 K ⇥ X , to which the challenger
Page  167: – For a ⇧ 1 -query, the adversary submits a pair (k , b ) 2 K ⇥ X , to which the challenger
Page  167: The adversary may make any number of ideal cipher queries, arbitrarily interleaved with
Page  167: The adversary’s advantage is defined using the same rule as before, but is denoted Xic adv[A, S] to
Page  167: means that Xic adv[A, S] should be negligible for all efficient adversaries A.
Page  167: ability of an adversary to make “o✏ine” evaluations of E and D.
Page  167: an adversary is able to intercept a small number of input/output pairs (xi , yi ) generated using k:
Page  167: The adversary can now recover k by trying all possible keys in k 2 K until a key k satisfying
Page  167: and for a given adversary A, define the following game:
Page  168: we allow the adversary to make arbitrary ⇧- and ⇧ 1 -queries, in addition to its standard queries
Page  168: to E(k, ·). We let KRic adv[A, E] denote the adversary’s key-recovery advantage when E is an ideal
Page  168: E(k, x) = x for which key-recovery is not possible (the adversary obtains no information about k),
Page  168: adversary AEX that plays Attack Game 4.4 with respect to E, modeled as an ideal cipher, making
Page  168: adversary may make standard queries to obtain the value E(k, x) = ⇧k (x) at points x 2 X of his
Page  168: choosing. An adversary may also make ideal cipher queries, obtaining the values ⇧k (a ) and ⇧k 1 (b )
Page  168: Our adversary AEX works as follows:
Page  169: attack against the 3E construction in the ideal cipher model. If A is an adversary that makes at
Page  169: for a constant C. Ignoring the log X term, this says that an adversary must make roughly |K|1.5
Page  169: To achieve a significant advantage, that adversary must make roughly |K|2 queries. Thus, meet-in-
Page  170: that in the ideal cipher model, for all adversaries A:
Page  171: distributed over a subset X 0 of X . If we model E as an ideal cipher, and if A is an adversary in
Page  171: The bounds in Theorem 4.14 are tight: there is an adversary A that achieves the advantage
Page  171: types of queries interact with each other, the adversary has to make
Page  171: (k , a , b ) corresponding to an ideal cipher query. Essentially, the adversary will have to simultane-
Page  172: the “split experiment”, an adversary has oracle access to two random permutations ⇧1 , ⇧2 on a
Page  172: set X . The adversary can make a series of queries, each of the form (µ, d, z ), where µ 2 {1, 2}
Page  172: condition can the adversary distinguish between these two experiments?
Page  172: Obviously, if the adversary can submit a query (1, +1, a ) and a query (2, +1, a ), then in the split
Page  172: will surely be the same. Another type of attack is possible as well: the adversary could make a query
Page  172: all the queries. The Domain Separation Lemma will basically say that unless the adversary makes
Page  172: Of course, the Domain Separation Lemma is only useful in contexts where the adversary is
Page  172: it inside of the proof of a security theorem where the “adversary” in the Domain Separation Lemma
Page  172: comprises components of a challenger and an adversary in a more interesting attack game.
Page  172: In the generalized version of the distinguishing game, if the adversary makes a query (µ, d, z ),
Page  172: queries made by the adversary in the split experiment. That is, we build up sets Domµ for each
Page  172: µ 2 U and d 2 ±1, so that a 2 Domµ               if and only if the adversary issues a query of the form
Page  172: adversary issues a query of the form (µ, 1, b ) or a query of the form (µ, +1, a ) that yields b . We
Page  172: U ! V be a function. For a given adversary A, we define two experiments, Experiment 0 and
Page  172: • The adversary submits a sequence of queries to the challenger.
Page  173: In either case, the challenger then sends z 0i to the adversary.
Page  173: • Finally, the adversary outputs a bit b̂ 2 {0, 1}.
Page  173: Theorem 4.15 (Domain Separation Lemma). In Attack Game 4.5, an adversary’s domain
Page  173: Proof of Theorem 4.14. Let A be an adversary as in the statement of the theorem. For b = 0, 1
Page  173: in addition to standard queries that probe the function Ek (·), the adversary may also probe the
Page  175: the random variables k, P1 , and P2 are completely independent of the adversary’s view.
Page  175: the ideal cipher model: the EX-queries present to the adversary the random permutation ⇧0 (x) :=
Page  176: event that the adversary outputs 1 in this game.
Page  176: Let W1 be the event that the adversary outputs 1 in this game.
Page  177: the adversary outputs 1 in this game.
Page  177: |Pr[W2 ] Pr[W1 ]| is equal to the adversary’s advantage in Attack Game 4.5. We want to use the
Page  177: variables representing the coins of the adversary, as well as the various random samples from
Page  179: particular, for any Q-query weak PRF adversary A attacking F2 (i.e., an adversary that only
Page  179: queries the function at random points in X ) there is a weak PRF adversary B attacking F ,
Page  179: attack game and allow the adversary A to query F2 at one chosen point in addition to the Q
Page  179: adversary A to query F2 at two chosen points in addition to the Q random points.
Page  180: a PRF may be secure against every adversary that makes its queries non-adaptively, (i.e., all at
Page  180: once) but is insecure against adaptive adversaries (i.e., the kind allowed in Attack Game 4.2).
Page  180: versary submits all at once the query (x1 , . . . , xQ ) to the challenger, who responds with (y1 , . . . , yQ ),
Page  180: while in Experiment 1, f R Funs[X , Y]. Security against non-adaptive adversaries means that all
Page  180: efficient adversaries have only negligible advantage; advantage is defined as usual: |Pr[W0 ] Pr[W1 ]|,
Page  180: where Wb is the event that the adversary outputs 1 in Experiment b.
Page  181: (a) Show that F̃ is not a secure PRF against adaptive adversaries.
Page  181: (b) Show that F̃ is a secure PRF against non-adaptive adversaries.
Page  181: cipher (Ẽ, D̃) that is secure against non-adaptive adversaries, but insecure against adaptive
Page  181: adversaries.
Page  181: an adversary A and a challenger. Initially, the challenger generates
Page  181: y    F (k, x). The adversary may make any (poly-bounded) number of function queries.
Page  181: responds with ỹb . The adversary is allowed to make only a single test query (with any number
Page  181: for all efficient adversaries. Show that F is a secure PRF if and only if F is Alt-PRF secure.
Page  181: challenger and an adversary.
Page  182: publicly known, and may even be adversarially chosen.
Page  182: Both experiments then proceed identically. The adversary issues a series of queries. Each query is
Page  182: forward query: the adversary sends (x, t) 2 X ⇥ T , and the challenger responds with y := ⇧t (x);
Page  182: inverse queries: the adversary sends (y, t) 2 X ⇥ T , and the challenger responds with x :=
Page  182: At the end of the game, the adversary outputs a bit. If pb is the probability that the adversary
Page  182: outputs 1 in Experiment b, the adversary’s advantage is defined to be |p0 p1 |. We say that (E, D)
Page  182: is a secure tweakable block cipher if every efficient adversary has negligible advantage.
Page  182: Hint: In addition to the ⇡ |K| queries, your adversary should make an additional ⇡ |K|
Page  183: property: the PRF F1 is secure; however, if the adversary learns the last bit of the key then
Page  183: adversary.
Page  185: (b) Use part (a) to construct an adversary A that wins the block cipher security game against
Page  186: 2 X , the adversary A queries the cipher at 2q random points mi , mi           2 X and queries
Page  186: block cipher adversary A attacking (E, D) we have
Page  186: Exercise 4.22 to show that if we model ⇡ as an ideal permutation ⇧, then for every PRF adversary
Page  186: Exercise 3.13. Consider an adversary A in Attack Game 4.3 that makes at most Q queries to its
Page  186: some settings. Consider an adversary A in Attack Game 4.3 that makes at most Q queries to its
Page  187: an adversary is given n black boxes (where n                1 is poly-bounded): the boxes either contain
Page  187: where the fi are random elements of Funs[X , Y], and the adversary should not be able to tell the
Page  187: sary A, then exist a PRF adversary B, where B is an elementary wrapper around A, such that
Page  187: 4.27 (Universal attacker on PRFs). Let F be a PRF defined over (K, X , Y) where |K| < |X |.
Page  187: Let Q < |K|. Show that there is a PRF adversary A that runs in time proportional to Q, makes
Page  187: adversary is given k1 . Argue that the same holds for k2 .
Page  187: a secure PRF when the adversary is given a single share, namely si for some i 2 {1, 2, 3}.
Page  188: adversary who eavesdrops, and who may even influence the choice of some messages in order to
Page  188: uses this same seed s to encrypt many files, an adversary can easily mount an attack. For example,
Page  188: if an adversary knows some of the bits of one file, he can directly compute the corresponding bits
Page  188: of the key stream, and hence obtain the corresponding bits of any file. How might an adversary
Page  188: information (see Example 2.6), and so if the adversary knows that a given ciphertext is an encryption
Page  189: standard header. To mount an even more devastating attack, the adversary may try something even
Page  189: Alice’s software automatically stores an encryption of this email on her server, when the adversary
Page  189: a chosen plaintext attack, because the adversary forces Alice to give him the encryption of one or
Page  189: from an inherent weakness: an adversary will always be able to tell when two files are identical
Page  189: type of attack is certainly not as dramatic as those discussed above, in which the adversary can
Page  190: (K, M, C), and for a given adversary A, we define two experiments, Experiment 0 and Experiment 1.
Page  190: • The adversary submits a sequence of queries to the challenger.
Page  190: The challenger computes ki         K, ci       E(ki , mib ), and sends ci to the adversary.
Page  190: • The adversary outputs a bit b̂ 2 {0, 1}.
Page  190: We stress that in the above attack game, the adversary’s queries are adaptively chosen, in the
Page  190: sense that for each i = 1, 2, . . . , the message pair (mi0 , mi1 ) may be computed by the adversary in
Page  190: secure if for all efficient adversaries A, the value MSSadv[A, E] is negligible.
Page  190: runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
Page  190: In particular, for every MSS adversary A that attacks E as in Attack Game 5.1, and which
Page  190: makes at most Q queries to its challenger, there exists an SS adversary B that attacks E as in
Page  191: if we modify the challenger so that it encrypts m11 instead of m10 , the adversary should not behave
Page  191: of m20 , and the adversary should not notice the di↵erence. If we continue in this way, making a
Page  191: adversary should not notice the di↵erence. 2
Page  191: Proof. Suppose E = (E, D) is defined over (K, X , Y). Let A be an MSS adversary that plays Attack
Page  191: We next devise an SS adversary B that plays Attack Game 2.1 with respect to E, as follows:
Page  191: Put another way, adversary B encrypts
Page  192: semantically secure cipher, then an adversary who sees the ciphertexts stored on the file server will
Page  192: Notice that this holds even if the adversary plays an active role in determining the contents of some
Page  192: for a given adversary A, we define two experiments, Experiment 0 and Experiment 1. For b = 0, 1,
Page  193: • The adversary submits a sequence of queries to the challenger.
Page  193: The challenger computes ci        E(k, mib ), and sends ci to the adversary.
Page  193: • The adversary outputs a bit b̂ 2 {0, 1}.
Page  193: key is chosen for each encryption. In particular, the adversary’s queries may be adaptively chosen
Page  193: plaintext attack, or simply CPA secure, if for all efficient adversaries A, the value CPAadv[A, E]
Page  193: Experiment b against the adversary A; we define A’s bit-guessing advantage as CPAadv⇤ [A, E] :=
Page  193: a CPA secure cipher, then an adversary who sees the ciphertexts stored on the file server will
Page  193: Again, notice that this holds even if the adversary plays an active role in determining the contents
Page  193: We construct a CPA adversary A as follows. Let m, m0 be any two, distinct messages in the
Page  193: message space of E. The adversary A makes two queries to its challenger: the first is (m, m0 ),
Page  193: challenger’s response to the second query. Adversary A outputs 1 if c1 = c2 , and 0 otherwise.
Page  195: In particular, for every CPA adversary A that attacks E 0 as in the bit-guessing version of Attack
Page  195: Game 5.2, and which makes at most Q queries to its challenger, there exists a PRF adversary
Page  195: BF that attacks F as in Attack Game 4.2, and an SS adversary BE that attacks E as in the bit-
Page  195: Proof. Let A be an efficient CPA adversary that attacks E 0 as in Attack Game 5.2. Assume that
Page  195: for efficient adversaries BF and BE . Then (5.5) follows from (5.4) and Theorem 2.10.
Page  196: send (xi , ci ) to the adversary.
Page  196: send (xi , ci ) to the adversary.
Page  196: where BF is an efficient PRF adversary; moreover, since we are assuming that F is a secure PRF,
Page  196: denotes the function chosen by its challenger in Attack Game 4.2 with respect to F , adversary BF
Page  196: Next, adversary BF plays the role of challenger to A; specifically, when A makes its ith
Page  196: query (mi0 , mi1 ), adversary BF computes
Page  197: Figure 5.1: Adversary BF in the proof of Theorem 5.2
Page  197: Eventually, A halts and outputs a bit b̂, at which time adversary BF halts and outputs
Page  197: See Fig. 5.1 for a picture of adversary BF . As usual, (x, y) is defined to be 1 if x = y, and 0
Page  197: send (xi , ci ) to the adversary.
Page  198: send (xi , ci ) to the adversary.
Page  198: choices made by the adversary and the challenger are identical in both games — all that di↵ers is
Page  198: where B̄E is an efficient adversary that plays the bit-guessing version of Attack Game 5.1 with
Page  198: adversary B̄E submits (mi0 , mi1 ) to its own challenger, obtaining a ciphertext ci 2 C;
Page  198: See Fig. 5.2 for a picture of adversary B̄E .
Page  198: where BE is an efficient adversary playing the bit-guessing version of Attack Game 2.1 with respect
Page  199: Figure 5.2: Adversary B̄E in the proof of Theorem 5.2
Page  200: evaluates the PRF at random points in X . Therefore, the adversary’s advantage in distinguishing
Page  202: In particular, for every CPA adversary A that attacks E as in Attack Game 5.2, and which
Page  202: makes at most Q queries to its challenger, there exists a PRF adversary B that attacks F as in
Page  202: Proof idea. Suppose we start with an adversary that plays the CPA attack game with respect to
Page  202: using an independent one-time pad, and so we can conclude that the adversary’s advantage in the
Page  202: Proof. Let A be an efficient adversary that plays Attack Game 5.2 with respect to E, and which
Page  202: for an efficient adversary B. Then (5.14) follows from (5.4).
Page  202: send (xi , ci ) to the adversary.
Page  203: where B is an efficient adversary; moreover, since we are assuming that F is a secure PRF, it must
Page  204: choices made by the adversary and the challenger are identical in both games — all that di↵ers
Page  204: it is easy to see that the adversary’s output in this game is independent of b. Therefore,
Page  207: In particular, for every CPA adversary A that attacks E 0 as in the bit-guessing version of Attack
Page  207: Game 5.2, and which makes at most Q queries to its challenger, there exists BC adversary B
Page  207: adversary that plays the CPA attack game with respect to E 0 . We then replace E by a truly random
Page  207: at the same point twice. But then what the adversary sees is nothing but a bunch of random bits,
Page  207: Proof. Let A be an efficient CPA adversary that attacks E 0 as in Attack Game 5.2. Assume that
Page  207: for an efficient adversary B. Then (5.22) follows from (5.4).
Page  207: send ci to the adversary.
Page  208: send ci to the adversary.
Page  208: where B is an efficient adversary; moreover, since we are assuming that E is a secure block cipher,
Page  208: send ci to the adversary.
Page  209: send ci to the adversary.
Page  209: choices made by the adversary and the challenger are identical in both games — all that di↵ers is
Page  209: are independently distributed, and the fact that the adversary’s output b̂ is a function of
Page  211: Here, A is any CPA adversary making at most Q queries to its challenger, ` is the maximum length
Page  211: so that ` = 216 blocks. If we want to keep the adversary’s advantage below 2 32 , then for counter
Page  213: over (K, M, C, N ), and for a given adversary A, we define two experiments, Experiment 0 and
Page  213: • The adversary submits a sequence of queries to the challenger.
Page  213: The challenger computes ci          E(k, mib , N i ), and sends ci to the adversary.
Page  213: • The adversary outputs a bit b̂ 2 {0, 1}.
Page  213: Note that in the above game, the nonces are completely under the adversary’s control, subject
Page  213: secure against chosen plaintext attack, or simply CPA secure, if for all efficient adversaries
Page  214: In particular, for every nCPA adversary A that attacks E 0 as in the bit-guessing version of
Page  214: adversary BF that attacks F as in Attack Game 4.2, and an SS adversary BE that attacks E as
Page  215: In particular, for every nCPA adversary A that attacks E as in Attack Game 5.3, there exists
Page  215: a PRF adversary B that attacks F as in Attack Game 4.2, where B is an elementary wrapper
Page  215: attack game, the adversary could make two queries:
Page  215: In particular, for every nCPA adversary A that attacks E 0 as in the bit-guessing version of Attack
Page  215: Game 5.3, and which makes at most Q queries to its challenger, there exists BC adversary B
Page  215: that attacks E as in Attack Game 4.1, and a PRF adversary BF that attacks F as in Attack
Page  219: that for every CPA adversary A attacking E2 there is a CPA adversary B attacking E with
Page  219: setting, analogous to Definition 5.1. In this attack game, the adversary gets to obtain encryptions
Page  219: of many messages under many keys. The game begins with the adversary outputting a number Q
Page  219: every subsequent encryption query, the adversary submits a pair of messages and specifies under
Page  219: security degrades linearly in Q. That is, the advantage of any adversary A in breaking the multi-key
Page  219: CPA security of a scheme is at most Q · ✏, where ✏ is the advantage of an adversary B (which is an
Page  219: define an attack game between an adversary A and a challenger. Initially, the challenger generates
Page  220: with a ciphertext c R E(k, m). The adversary may make any (poly-bounded) number of
Page  220: with a ciphertext c R E(k, mb ). The adversary is allowed to make only a single test query
Page  220: E is Alt-CPA secure if this advantage is negligible for all efficient adversaries.
Page  221: the challenger first picks a random key k R K and then the adversary submits a sequence of
Page  221: Experiment 1 is the same as Experiment 0 except that the challenger responds to the adversary’s
Page  221: secure if no efficient adversary can distinguish between these two experiments with a non-negligible
Page  221: be probabilistic, since for a deterministic cipher, an adversary can always see if the same message
Page  221: thing the adversary can see. This is easily done by placing the following restriction on the adversary
Page  221: that a cipher is deterministic CPA secure if every efficient adversary has negligible advantage
Page  222: infeasible for an adversary to make an encryptor generate the same ciphertext twice. The precise
Page  222: attack game is as follows. The challenger chooses k 2 K at random and the adversary makes a series
Page  222: The adversary wins the game if any two ci ’s are the same. Show that if E is CPA secure, then
Page  222: every efficient adversary wins this game with negligible probability. In particular, show that the
Page  222: advantage of any adversary A in winning the repeated-ciphertext attack game is at most 2✏, where
Page  222: ✏ is the advantage of an adversary B (which is an elementary wrapper around A) that breaks the
Page  222: adversary that wins the CPA game against this implementation with advantage close to 1. We note
Page  224: the adversary to submit a complete pair of messages in every encryption query, the adversary
Page  224: b = 0, 1, the adversary begins by outputing a subset B of {1, . . . , n}. The challenger then
Page  225: runs G(n) and sends to the adversary all the keys named in B, namely {ki }i2B . Now the
Page  225: adversary issues chosen plaintext queries, where query number j is a triple (Sj , mj,0 , mj,1 ) for
Page  225: secure if the adversary cannot distinguish these two experiments.
Page  226: In previous chapters we focused on security against an eavesdropping adversary. The adversary
Page  226: In this chapter we turn our attention to active adversaries. We start with the basic question
Page  227: ing party has a secret key unknown to the adversary.
Page  227: Without a secret key, ensuring message integrity is not possible: the adversary has enough infor-
Page  227: message integrity mechanisms require a secret key unknown to the adversary. In this chapter,
Page  227: malicious errors. The argument in the previous paragraph shows that an adversary can easily defeat
Page  227: adversary knows exactly how the CRC32 algorithm works and this lets him compute valid tags for
Page  228: makes sense. Suppose an adversary is attacking a MAC system I = (S, V ). Let k be some
Page  229: MAC Challenger                                Adversary A
Page  229: come up in real world settings. We refer to message-tag pairs (m, t) that the adversary obtains
Page  229: We say that a MAC system is secure if even an adversary who can mount a chosen message
Page  229: attack cannot create an existential forgery. This definition gives the adversary more power than it
Page  229: adversary A. The game is described below and in Fig. 6.2.
Page  229: (K, M, T ), and a given adversary A, the attack game runs as follows:
Page  230: the game. Finally, we say that A is a Q-query MAC adversary if A issues at most Q signing
Page  230: Definition 6.2. We say that a MAC system I is secure if for all efficient adversaries A, the value
Page  230: In case the adversary wins Attack Game 6.1, the pair (m, t) it sends the challenger is called
Page  230: adversary requests one or more valid tags t1 , t2 , . . . for m. Can the adversary produce a new valid
Page  230: for an adversary to produce a new valid tag t0 for a previously signed message m. This may seem like
Page  230: an odd thing to require of a MAC. If the adversary already has valid tags for m, why should we care
Page  230: the adversary from producing new tags on signed messages, is necessary for the applications we
Page  230: order for $100 and m2 is a transfer order for $101. Clearly, an adversary who intercepts a valid
Page  230: Definition 6.2 ensures this. To see why, suppose an adversary A can forge the tag for m2 given the
Page  230: Example 6.4. Our definition of secure MACs gives the adversary the ability to obtain the tag for
Page  230: arbitrary messages. This may seem like giving the adversary too much power. In practice, however,
Page  231: given to both the adversary and the challenger. The advantage MACadv[A, I] is then a function
Page  231: In our definition of secure MACs (Attack Game 6.1) the adversary has no way of testing whether a
Page  231: given message-tag pair is valid. In fact, the adversary cannot even tell if it wins the game, since only
Page  232: Consequently, it makes sense to extend Attack Game 6.1 by giving the adversary the extra
Page  232: power to verify message-tag pairs. Of course, we continue to allow the adversary to request tags
Page  232: I = (S, V ), defined over (K, M, T ), and a given adversary A, the attack game runs as follows:
Page  232: power does not help the adversary.
Page  232: In particular, for every MAC adversary A that attacks I as in Attack Game 6.2, and which
Page  232: MAC adversary B that attacks I as in Attack Game 6.1, where B is an elementary wrapper
Page  232: Proof idea. Let A be a MAC adversary that attacks I as in Attack Game 6.2, and which makes
Page  232: at most Qv verification queries and at most Qs signing queries. From adversary A, we build an
Page  232: adversary B that attacks I as in Attack Game 6.1 and makes at most Qs signing queries. Adversary
Page  233: Proof. In more detail, adversary B plays the role of challenger to A in Attack Game 6.2, while
Page  233: at the same time, it plays the role of adversary in Attack Game 6.1, interacting with the MAC
Page  233: To rigorously justify the construction of adversary B, we analyze the the behavior of A in three
Page  233: and adversary A. Here is the logic of the challenger in this game:
Page  234: In summary, we showed that Attack Game 6.2, which gives the adversary more power, is
Page  234: • When constructing secure MACs it easier to use Attack Game 6.1 which restricts the adversary
Page  234: is more convenient to assume that the MAC is secure with respect to the stronger adversary
Page  234: We also point out that if we had used a weaker notion of security, in which the adversary only
Page  235: In particular, for every Q-query MAC adversary A that attacks I as in Attack Game 6.1, there
Page  235: exists a (Q + 1)-query PRF adversary B that attacks F as in Attack Game 4.2, where B is an
Page  235: Proof idea. Let A be an efficient MAC adversary. We derive an upper bound on MACadv[A, I]
Page  235: But now that the adversary A is interacting with a truly random function it is faced with a hopeless
Page  235: send ti to the adversary
Page  235: At the end of the game, the adversary outputs a message-tag pair (m, t). We define W0 to be the
Page  235: function f in Funs[X , Y]. Intuitively, since F is a secure PRF, the adversary A should not notice
Page  235: adversary B such that:
Page  235: Adversary B responds to A’s chosen message queries by querying its own PRF challenger. Eventu-
Page  235: Next, we directly bound Pr[W1 ]. The adversary A sees the values of f at various points
Page  236: m 62 {m1 , m2 , . . .}, adversary A will guess f (m) with probability 1/|Y|. Therefore, Pr[W1 ]  1/|Y|.
Page  236: adversaries cannot distinguish the PRF from a random function. A prefix-free PRF adversary
Page  237: In particular, for every prefix-free PRF adversary A that attacks FCBC as in Attack Game 4.2,
Page  237: and issues at most Q queries, there exists a PRF adversary B that attacks F as in Attack
Page  238: Proof idea. We represent the adversary’s queries in a rooted tree, where edges in the tree are labeled
Page  238: restriction is critical, as it guarantees that the adversary never sees p and p0 , and hence a and
Page  238: of FCBC seen by the adversary. Therefore, the adversary cannot distinguish FCBC from a random
Page  238: for an efficient adversary B.
Page  239: As the adversary makes queries, our challenger will dynamically build up the query tree. Ini-
Page  239: tially, the tree contains only the root. Whenever the adversary makes a query, the challenger traces
Page  239: versary, Coins, and the list of values 1 , . . . , B . Observe that for any fixed choice of values
Page  239: Case 2: p 6= p0 . The requirement that the adversary’s queries are prefix free implies that in
Page  239: Game 3, the adversary never sees — or learns anything about — the values p and p0 . One of p or
Page  240: In particular, for every prefix-free PRF adversary A that attacks F ⇤ as in Attack Game 4.2, and
Page  240: issues at most Q queries, there exists a PRF adversary B that attacks F as in Attack Game 4.2,
Page  240: (6.15). However, rest assured there is no contradiction here. p The adversary A from Exercise 6.6,
Page  240: Theorem 6.4 we obtain a PRF adversary B that attacks the PRF F making      p about Q queries to
Page  240: surprising about this adversary B: it is essentially the universal PRF attacker from Exercise 4.27.
Page  243: In particular, for every PRF adversary A that attacks EF as in Attack Game 4.2, and issues
Page  243: at most Q queries, there exist a PRF adversary B1 attacking F as in Attack Game 4.2, and
Page  243: a prefix-free PRF adversary B2 attacking PF as in Attack Game 4.2, where B1 and B2 are
Page  243: Our attack will, in fact, break EF as a MAC. The adversary picks Q random inputs x1 , . . . , xQ 2 X 2
Page  243: that for all a 2 X , we have PF k1 , (xi k a) = PF k1 , (xj k a) . Therefore, our adversary can
Page  245: In particular, for every PRF adversary A that attacks ECBC as in Attack Game 4.2, and issues
Page  245: at most Q queries, there exist PRF adversaries B1 , B2 that attack F as in Attack Game 4.2,
Page  245: In particular, for every PRF adversary A that attacks NMAC as in Attack Game 4.2, and issues
Page  245: at most Q queries, there exist PRF adversaries B1 , B2 that attack F as in Attack Game 4.2,
Page  247: vice versa). The function rpf (k, ·) need not even be injective.
Page  248: In particular, for every PRF adversary A that attacks F as in Attack Game 4.2, and issues at
Page  248: most Q queries, there exist prefix-free PRF adversaries B1 and B2 that attack PF as in Attack
Page  248: Proof idea. If the adversary’s set of inputs to F give rise to a prefix-free set of inputs to PF , then
Page  248: the adversary sees just some random looking outputs. Moreover, if the adversary sees random
Page  249: for efficient prefix-free PRF adversaries B1 and B2 . These two adversaries are basically the same,
Page  255: In particular, for every PRF adversary A that attacks PMAC0 as in Attack Game 4.2, and
Page  255: issues at most Q queries, there exist PRF adversaries B1 and B2 , which are elementary wrappers
Page  257: let I be the MAC system derived from F , as discussed in Section 6.3. Let A be an adversary
Page  257: at most Qs signing queries. Theorem 6.1 says that there exists a Qs -query MAC adversary B
Page  257: adversary B 0 that attacks F as in Attack Game 4.2, where B 0 is an elementary wrapper around B,
Page  257: query PRF adversary B 00 , where B 00 is an elementary wrapper around A, such that
Page  257: game the adversary outputs a number Q indicating the number of keys it wants to attack
Page  257: (b) Show that every efficient adversary A that wins your multi-key MAC attack game with
Page  257: probability ✏ can be transformed into an efficient adversary B that wins Attack Game 6.2
Page  257: analogous to that used in the proof of Theorem 6.1. Adversary B plays the role of challenger
Page  257: to adversary A. Once A outputs a number Q, B chooses Q random keys k1 , . . . , kQ and a
Page  258: random index ! 2 {1, . . . , Q}. When A issues a query for key number j 6= !, adversary B
Page  258: uses its key kj to answer the query. When A issues a query for the key k! , adversary B
Page  259: the FCBC prefix-free PRF with input space X 3 . Suppose an adversary queries the challenger
Page  259: randomly from X . Show that if Q ⇡ |X |, the adversary can predict the PRF at a new
Page  259: queries in X 3 , your adversary should be able to predict the PRF at a new point in X 3 with
Page  259: in which we make it harder for the adversary to win; specifically, in order to win, the adversary
Page  259: win, in addition to being a valid pair, the adversary’s candidate forgery pair (m, t) must satisfy
Page  259: adversary wins if the challenger ever responds to a verification query (m̂j , t̂j ) with accept, where m̂j
Page  260: length CBC and cascade are secure PRFs against non-adaptive adversaries, i.e., adversaries that
Page  260: (a) Show that CBC is a secure PRF against non-adaptive adversaries, assuming the underlying
Page  262: Message integrity from universal
Page  262: 7.1    Universal hash functions (UHFs)
Page  263: the key an adversary gets in trying to find a collision. In this chapter, we focus on the weakest
Page  263: formulation of this collision resistance property, in which the adversary must find a collision with no
Page  263: on the computational power of the adversary. On the other hand, this property is strong enough
Page  263: Hash functions that satisfy this very weak collision resistance property are called universal
Page  263: hash functions, or UHFs. Universal hash functions are used in various branches of computer
Page  263: Attack Game 7.1 (universal hash function). For a keyed hash function H defined over
Page  263: (K, M, T ), and a given adversary A, the attack game runs as follows.
Page  263: We now define several di↵erent notions of UHF, which depend on the power of the adversary
Page  264: • We say that H is an ✏-bounded universal hash function, or ✏-UHF, if UHFadv[A, H]  ✏
Page  264: for all adversaries A (even inefficient ones).
Page  264: adversaries A.
Page  264: Statistical UHFs are secure against all adversaries, efficient or not: no adversary can win Attack
Page  264: consider computationally unbounded adversaries is that we can: unlike most other security notions
Page  264: restrictions on the adversary. Note that every statistical UHF is also a computational UHF, but
Page  264: It will be convenient to consider a generalization of a computational UHF. Here the adversary wins
Page  264: for H(k, ·). The point is that although the adversary may not know exactly which pair of messages
Page  264: given adversary A, the attack game runs as follows.
Page  264: the game. We call A a Q-query UHF adversary if it always outputs a list of size s  Q. 2
Page  264: efficient adversaries A, the quantity MUHFadv[A, H] is negligible.
Page  264: In particular, for every Q-query UHF adversary A, there exists a UHF adversary B, which is
Page  265: Proof. The UHF adversary B runs A and obtains s  Q distinct messages m1 , . . . , ms . It randomly
Page  265: The challenge in constructing good universal hash functions (UHFs) is to construct a function that
Page  267: Caution in using UHFs. UHFs can be brittle — an adversary who learns the value of the
Page  267: k + a1 an adversary who has both m and Hpoly (k, m) immediately obtains k 2 Zp . Consequently,
Page  267: in all our applications of UHFs we will always hide values of the UHF from the adversary, either
Page  268: In particular, for every UHF adversary A that plays Attack Game 7.1 with respect to PF , there
Page  268: exists a prefix-free PRF adversary B, which is an elementary wrapper around A, such that
Page  268: Proof. Let A be a UHF adversary attacking PF . We build a prefix-free PRF adversary B attack-
Page  268: ing PF . B plays the adversary in the PRF Attack Game 4.2. Its goal is to distinguish between
Page  268: We first give some intuition as to how B works. B starts by running the UHF adversary A to
Page  268: free adversary. However, the extendability property provides a simple solution: we extend both
Page  268: In more detail, adversary B works as follows:
Page  269: // At this point we know that m00 is not a proper prefix of m01 nor vice versa.
Page  269: Observe that B is a prefix-free PRF adversary that only makes two queries to f , as required.
Page  269: In particular, if |X | > `Q, then for every Q-query UHF adversary A, there exists a Q-query
Page  269: prefix-free PRF adversary B, which is an elementary wrapper around A, such that
Page  269: Proof. The proof is similar to the proof of Theorem 7.3. Adversary B begins by running the Q-
Page  269: query UHF adversary A to obtain distinct messages m1 , . . . , ms in X ` , where s  Q. Next, B
Page  269: free adversary B now queries the challenger at m01 , . . . , m0s and obtains t1 , . . . , ts in response. B
Page  270: In particular, for every Q-query UHF adversary A, there exist prefix-free PRF adversaries
Page  271: In particular, for every UHF adversary A, there exists a PRF adversary B, which is an elemen-
Page  271: The adversary A outputs two distinct messages U, V in X ` . Let u := |U | and v := |V |. We define
Page  272: As usual, there is a PRF adversary B such that
Page  272: In particular, suppose A is a PRF adversary that plays Attack Game 4.2 with respect to F 0 and
Page  272: issues at most Q queries. Then there exist a PRF adversary BF and a UHF adversary BH ,
Page  273: More generally, there exists a Q-query UHF adversary BH , which is an elementary wrapper
Page  273: But then F 0 is clearly not a secure PRF: the adversary could ask for t0 := F 0 ((k1 , k2 ), m0 ) and
Page  273: t1 := F 0 ((k1 , k2 ), m1 ) and then output 1 only if t0 = t1 . When interacting with F 0 the adversary
Page  273: adversary successfully distinguishes F 0 from a random function. This argument shows that for F 0
Page  273: the adversary ever finds two messages m0 , m1 that cause an internal collision (i.e., a collision on
Page  273: Proof idea. Let A be an efficient PRF adversary that plays Attack Game 4.2 with respect to F 0 .
Page  274: send ti to the adversary
Page  274: send ti to the adversary
Page  274: PRF adversary BF whose running time is about the same as that of A such that:
Page  274: is a Q-query UHF adversary BH        0 that wins Attack Game 7.2 with probability equal to Pr[Z].
Page  274: Adversary BH  0 simply emulates the challenger in Game 2 until A terminates and then outputs the
Page  274: adversary BH 0 can easily emulate the challenger in Game 2 without knowledge of k . By definition
Page  276: Recall that in PRF(UHF) composition the adversary’s advantage in breaking the MAC after
Page  276: the hash function. As an example, suppose that after signing Q := 232 messages the adversary’s
Page  278: if the adversary finds two messages m1 , m2 that collide on the hash function (i.e., H(k1 , m1 ) =
Page  278: tags for many messages the adversary can identify messages m1 and m2 that collide on the hash
Page  278: must use an ✏-UHF with a sufficiently small ✏ to ensure that with high probability the adversary will
Page  278: a CPA secure cipher we prevent the adversary from learning when a hash function collision occurred:
Page  278: H to satisfy a stronger property than universality (UHF). We refer to this stronger property as
Page  278: (K, M, T ), where T = ZN , and a given adversary A, the attack game runs as follows.
Page  278: DUFadv[A, H]  ✏ for all adversaries A (even inefficient ones).
Page  279: adversaries A.
Page  279: Clearly if H is an ✏-DUF then H is also an ✏-UHF: a UHF adversary can be converted into a
Page  279: DUF adversary that wins with the same probability (just set = 0).
Page  280: In particular, for every MAC adversary A that attacks ICW as in Attack Game 6.1, there exist
Page  280: a PRF adversary BF and a DUF adversary BH , which are elementary wrappers around A, such
Page  280: H(k1 , m1 ) = H(k1 , m0 ) + , without knowledge of k1 . The adversary could then ask for the tag on
Page  280: Proof idea. Let A be an efficient MAC adversary that plays Attack Game 6.1 with respect to
Page  280: the adversary’s advantage much. We then show that only three things can happen that enable the
Page  280: adversary to generate a forged message-tag pair and that the probability for each of those is small:
Page  280: 2. The adversary might output a MAC forgery m, (r, v) where r 2 R is a fresh randomizer
Page  280: 3. Finally, the adversary could output a MAC forgery m, (r, v) where r = rj for some uniquely
Page  280: But since H is an computational DUF, the adversary can find such a relation with only
Page  281: adversary in a particular execution of the attack game is s, which is at most Q.
Page  281: send (ri , vi ) to the adversary
Page  281: send (ri , vi ) to the adversary
Page  282: As usual, one can show that there is a PRF adversary BF , just as efficient as A, such that:
Page  282: for a DUF adversary BH that is just as efficient as A. To this end, consider what happens if A wins
Page  282: DUF adversary BH that succeeds with probability at least Pr[W20 ] in Attack Game 7.3. Adversary
Page  283: adversary chooses the nonces. The adversary, however, must never request a tag using a previously
Page  283: (S, V ), defined over (K, M, T , N ), and a given adversary A, the attack game runs as follows:
Page  285: security. To capture the one-time nature of the MAC we allow the adversary to issue only one
Page  285: signing query. We denote the adversary’s advantage in this restricted game by MAC1 adv[A, I].
Page  285: This game captures the fact that the adversary sees only one message-tag pair and then tries to
Page  285: Unconditional security means that MAC1 adv[A, I] is negligible for all adversaries A, even com-
Page  285: (K, M, T ), and a given adversary A, the attack game runs as follows.
Page  285: for short, if PUFadv[A, H]  ✏ for all adversaries A (even inefficient ones).
Page  286: Proof. Let A attack H 0 as a PUF. In response to its query m0 , adversary A receives t0 :=
Page  286: So we can define a DUF adversary B as follows: it runs A, and when A submits its query m0 ,
Page  286: B responds with a random t0 2 T ; when A outputs (m1 , t1 ), adversary B outputs (m0 , m1 , t1 t0 ).
Page  286: adversaries A (even inefficient ones), we have MAC1 adv[A, I]  ✏.
Page  287: PRF against non-adaptive adversaries (see Exercise 4.6), and the size of the output space of F is
Page  287: an adversary A that wins Attack Game 7.1 with probability greater than ✏. Your adversary is not
Page  288: (a) Suppose A is a MAC adversary that plays Attack Game 6.1 with respect to I and issues
Page  288: at most Q queries. Show that there exists a PRF adversary BF and UHF adversaries BH
Page  289: In this game, the adversary computes a message M and the challenger (independently) chooses
Page  289: a random hash key k0 2 K. The adversary wins the game if H(k0 , M ) = x0 , where x0 2 X
Page  289: is a constant, as above. We say that H is preimage resistant if every efficient adversary wins
Page  289: Show that following concrete security result: for every MAC adversary A that attacks ICW as in
Page  290: there exist a PRF adversary BF and a DUF adversary BH , which are elementary wrappers around
Page  290: that if the adversary obtains the tag on some one-block message m1 using nonce N and the tag
Page  290: insecure: the adversary can forge the MAC an any message of his choice with non-negligible
Page  290: that an adversary is free to re-use nonces at will. Show how to create an existential forgery.
Page  291: In particular, using the result of Exercise 7.12, show that for every adversary A that makes
Page  291: (b) Now suppose an adversary can re-use nonces at will. Show that for every such adversary
Page  292: 0. The original attack game: adversary makes a series of ideal permutation queries, which
Page  292: evaluate ⇧ and ⇧ 1 on points of the adversary’s choice. Then the adversary submits two
Page  292: adversaries: given H(k, m) for any message m, we can efficiently compute the key k.
Page  295: versary A, there exists a PRF adversary B, which is an elementary wrapper around A, such that
Page  295: exhibiting an adversary that wins Attack Game 7.5 with probability 1.
Page  295: is unconditionally secure, provided the adversary can make at most two queries. We say that a
Page  295: following holds: for all adversaries A (even inefficient ones) that make at most 2 queries in Attack
Page  296: ally CPA secure provided the adversary can make at most two queries in Attack Game 5.2.
Page  297: In the previous chapter we discussed universal hash functions (UHFs) and showed how they can be
Page  298: for H. If an adversary could find two long messages m0 and m1 such that H(m0 ) = H(m1 ) then
Page  298: is evil, say a virus infected program. The adversary would ask for the tag on the message m0 and
Page  298: (m1 , t). Hence, the adversary is able to forge a tag for m1 , which breaks the MAC. Even worse,
Page  300: and adversary A, the adversary takes no input and outputs two messages m0 and m1 in M.
Page  300: probability that A wins the game. Adversary A is called a collision finder. 2
Page  300: efficient adversaries A, the quantity CRadv[A, H] is negligible.
Page  300: An adversary A that simply prints m0 and m1 and exits is an efficient adversary that breaks the
Page  300: (M, T ) there exists some efficient adversary AH that breaks the collision resistance of H. Hence,
Page  300: cannot simply “hardwire” a fixed collision into an adversary: an e↵ective adversary must be able
Page  300: Some authors deal with this issue by have H take as input a randomly chosen key k, and giving k to the adversary
Page  301: Adversary A
Page  301: In particular, suppose A is a MAC adversary attacking I 0 (as in Attack Game 6.1). Then there
Page  301: exist a MAC adversary BI and an efficient collision finder BH , which are elementary wrappers
Page  302: It is clear that collision resistance of H is essential for the security of I 0 . Indeed, if an adversary
Page  302: Proof idea. Our goal is to show that no efficient adversary can win the MAC Attack Game 6.1 for
Page  302: our new MAC system I 0 . An adversary A in this game asks the challenger to MAC a few long
Page  302: game and let (m, t) 2 M ⇥ T be the adversary’s output, which we assume is not among the signed
Page  302: • Let X be the event that adversary A wins the MAC Attack Game 6.1 with respect to I 0 .
Page  302: To prove the theorem we construct a collision finder BH and a MAC adversary BI such that
Page  302: Both adversaries are straight-forward.
Page  302: Adversary BH plays the role of challenger to A in the MAC attack game, as follows:
Page  303: MAC Adversary BI attacking I
Page  303: MAC Challenger                                                     Adversary A
Page  303: Figure 8.4: Adversary BI in the proof of Theorem 8.1
Page  303: MAC adversary BI is just as simple and is shown in Fig. 8.4. When A outputs the final
Page  303: message-tag pair (m, t) adversary BI outputs (H(m), t). When event Z happens we know that
Page  304: We argue that when the adversary picks s := 2 N + 1 random messages in M, then with
Page  305: Put di↵erently, if after evaluating the hash function s times an adversary should obtain a
Page  309: which seems a bit odd since the adversary has full control over the message. Nevertheless, we will
Page  311: Attack Game 8.1 can be adapted to the ideal cipher model, so that before the adversary outputs
Page  311: • For a ⇧-query, the adversary submits a pair (k , a ) 2 K ⇥ X , to which the challenger responds
Page  311: • For a ⇧ 1 -query, the adversary submits a pair (k , b ) 2 K⇥X , to which the challenger responds
Page  311: After making these queries, the adversary attempts to output a collision, which in the case of
Page  311: The adversary A’s advantage in finding a collision for hDM in the ideal cipher model is denoted
Page  311: all efficient adversaries A.
Page  311: In particular, every collision finding adversary A that issues at most q ideal-cipher queries will
Page  312: p shows that Davies-Meyer is an optimal compression function: the adversary must
Page  312: If the adversary outputs a collision, then by our reasonableness assumption, for some distinct
Page  312: Consider any fixed indices i < j. Conditioned on any fixed values of the adversary’s coins and
Page  316: require many interactions between the adversary and honest users of the system. In general, o✏ine
Page  316: attacks are considered especially dangerous since an adversary can invest huge computing resources
Page  320: to hbot is used as the key for E. But m is chosen by the adversary and hence E is evaluated with
Page  320: a key that is completely under the control of the adversary. As a result, even though E is a secure
Page  320: For an adversary A, we define PRFic adv[A, F ] to be the advantage in the modified PRF attack
Page  320: adversaries.
Page  321: In particular, for every PRF adversary A attacking hbot and making at most a total of Qic ideal
Page  321: Let A be an adversary as in the statement of the theorem. Let pb be the probability that A
Page  322: adversary’s view).
Page  322: Thus, from the adversary’s point of view, the standard queries behave identically to a random
Page  325: In particular, for every collision finding adversary A, if the number of ideal-permutation queries
Page  325: Proof. As in the proof of Theorem 8.4, we assume our collision-finding adversary is “reasonable”,
Page  325: convert an arbitrary adversary into a reasonable one by forcing the adversary evaluate the hash
Page  325: upper bound on the total number of ideal permutation queries made by our reasonable adversary.
Page  325: So from now on, we assume a reasonable adversary A that makes at most q queries, and we bound
Page  325: We also assume that no queries are redundant. This means that if the adversary makes a ⇧-
Page  325: query on a yielding b = ⇧(a ), then the adversary never makes a ⇧ 1 -query on b , and never makes
Page  325: another ⇧-query on a ; similarly, if the adversary makes a ⇧ 1 -query on b yielding a = ⇧ 1 (b ), then
Page  325: the adversary never makes a ⇧-query on a , and never makes another ⇧ 1 -query on b . Of course,
Page  325: there is no need for the adversary to make such redundant queries, which is why we exclude them;
Page  325: It helps to visualize the adversary’s attack as building up a directed graph G. The nodes in G
Page  325: Note that the assumption that the adversary makes no redundant queries means that an edge
Page  329: explained that a collision resistant hash ensures that the adversary cannot tamper with the file
Page  329: Recall that read-only storage can be read, but not modified, by an adversary. It can be implemented as a
Page  330: We will show in Theorem 8.8 below that, if h is collision resistant, an adversary cannot exhibit
Page  331: system to manage the secret MAC key, and ensure that it is never read by the adversary. While
Page  332: function h is collision resistant, an adversary cannot convince the verifier that an x 2 T is not a
Page  333: We next define security. We let the adversary choose an arbitrary tuple T := (x1 , . . . , xn ) 2 X n ,
Page  333: ture scheme D = (H, P, V ) defined over (X n , Y), and a given adversary A, the attack game runs
Page  333: The adversary A outputs a tuple T := (x1 , . . . , xn ) 2 X n , an 1  i  n, an element
Page  333: adversaries A, the value ADSadv[A, D] is negligible.
Page  334: a user has a 128-bit key, but that 64 of the bits have been leaked to the adversary. The key
Page  334: is still fairly difficult to guess, but it is still not uniformly distributed from the adversary’s
Page  335: N , if x is chosen at random modulo N , and an adversary is given y := x3 mod N , it is
Page  335: we can view y as information that is leaked to the adversary. Even though the value of y
Page  335: a function I, so that an adversary trying to guess s knows the side information I(s).
Page  335: finite set S and let I be a function defined in S. For a given adversary A, the attack game runs as
Page  335: • the adversary outputs a guess ŝ for s, and wins the game if ŝ = s.
Page  335: side information given to the adversary, and the guessing advantage is 1/|D|, regardless of the
Page  335: computational power of the adversary.
Page  335: 64-bits of s. Clearly, any adversary, no matter how powerful, has guessing advantage no greater
Page  335: adversaries, the guessing advantage appears to be negligible.
Page  335: computationally indistinguishable. For an adversary A, let Distadv[A, P, I, H] be the adversary’s
Page  336: adversary A, there exists an adversary B (which is an elementary wrapper around A) such that
Page  336: H for all its computations, and in addition, the adversary is allowed to obtain the value of O at
Page  337: to property X) and an arbitrary adversary A. Presumably, in responding to certain queries, the
Page  337: adversaries A.
Page  337: adversary and the challenger have oracle access. More precisely, the game is modified as follows.
Page  337: • In addition to its standard queries, the adversary A may submit random oracle queries: it
Page  337: gives m 2 M to the challenger, who responds with t = O(m). The adversary may make any
Page  338: The adversary’s advantage is defined using the same rule as before, but is denoted Xro adv[A, S] to
Page  338: model means that Xro adv[A, S] should be negligible for all efficient adversaries A.
Page  338: that uses a hash function H defined over (M, T ) as an oracle. For a given adversary A, we define
Page  338: • The adversary submits a sequence of queries to the challenger.
Page  338: • The adversary computes and outputs a bit b̂ 2 {0, 1}.
Page  338: adversaries A, the value PRFro adv[A, F ] is negligible.
Page  339: In particular, if A is a random oracle PRF adversary, as in Attack Game 8.4, that makes at
Page  339: Proof idea. Once H is replaced with O, the adversary has to distinguish O(k k ·) from a random
Page  339: only hope the adversary has is to somehow use the information returned from queries to O. We
Page  339: of the function O(k k ·). Moreover, the probability that after Qro queries the adversary succeeds
Page  339: Game 8.4, but will be more convenient for us to analyze. We assume the adversary never makes the
Page  340: Let Z be the event that in Game 1, the adversary makes an O-query at a point m = (k k x̂). It is
Page  340: information I(s) is leaked to the adversary. We want to argue that if H is modeled as a random
Page  340: oracle, then the adversary’s advantage in distinguishing (I(s), H(s)) from (I(s), t), where t is truly
Page  340: random, is not too much more than the adversary’s advantage in guessing the secret s with only
Page  340: adversary to output a list of guesses ŝ1 , . . . , ŝQ , where and the adversary is said to win the game
Page  340: if ŝi = s for some i = 1, . . . , Q. An adversary A’s probability of winning in this game is called his
Page  340: Clearly, if an adversary A can win the above list guessing game with probability ✏, we can
Page  340: convert him into an adversary that wins the singleton guessing game with probability ✏/Q: we
Page  340: Theorem 8.10. If H is modeled as a random oracle, then for every distinguishing adversary A
Page  340: that makes at most Qro random oracle queries, there exists a list guessing adversary B, which is an
Page  340: and B outputs a list of size at most Qro . In particular, there exists a guessing adversary B 0 , which
Page  341: games result in the same outcome unless the adversary A in Game 1 makes an O-query at the
Page  341: point s. So our list guessing adversary B simply takes the value I(s) that it receives from its own
Page  342: Now consider a generic attack game defined by some challenger C and adversary A. Let us
Page  342: give both the challenger and adversary oracle access to the random function O, and we denote the
Page  342: oracle, then while the challenger accesses ⇢ only via the construction F , the adversary is allowed
Page  342: case), while the adversary can access ⇢ itself (the compression function h or the underlying block
Page  342: for every efficient challenger C and efficient adversary A, there exists an efficient ad-
Page  342: versary B, which is an elementary wrapper around A, such that
Page  342: O using F [⇢]: if an adversary A could break the scheme with F [⇢], then the adversary B above
Page  343: the solution is a surprising and elegant application of universal hash functions (see Section 7.1).
Page  343: lemma, the bound on the guessing probability must hold for all adversaries, even computationally
Page  344: cases where the distribution of s or the function I can be somehow biased by an adversary in a way
Page  344: that depends on k, which is assumed public and known to the adversary. Therefore, to apply the
Page  344: are met, then the lemma says that for any adversary A, even a computationally unbounded one,
Page  344: unconditionally against unbounded adversaries, or can only be heuristically estimated. So the
Page  345: secret s and cannot be manipulated by an adversary. The idea is that under these circumstances,
Page  346: OWadv[A, H] of an adversary A in defeating the one-wayness of H as the probability of winning
Page  346: • the adversary A outputs m0 2 M, and wins if H(m0 ) = t.
Page  346: H is one-way if OWadv[A, H] is negligible for every efficient adversary A.
Page  346: Similarly, we define the advantage SPRadv[A, H] of an adversary A in defeating the 2nd-
Page  346: • the adversary A outputs m0 2 M, and wins if H(m0 ) = H(m) and m0 6= m.
Page  346: H is 2nd-preimage resistant if SPRadv[A, H] is negligible for every efficient adversary A.
Page  347: Adversary A
Page  347: (K, M, T ) and adversary A, the attack game runs as follows:
Page  347: The adversary is said to win the game if m0 6= m1 and H(k, m0 ) = H(k, m1 ). We define A’s
Page  347: Casting the definition in our formal mathematical framework is done exactly as for universal
Page  348: In particular, for every TCR adversary A attacking Htcr as in Attack Game 8.5, there exists a
Page  348: Proof. The proof is a simple direct reduction. Adversary B emulates the challenger in Attack
Page  350: In particular, suppose A is a TCR adversary attacking H (as in Attack Game 8.5). Then there
Page  350: exists a TCR adversary B (whose running times are about the same as that of A) such that
Page  350: The adversary (the malware writer in this case) chooses which file F to attack. But this is precisely
Page  350: the TCR Attack Game 8.5 — the adversary chooses an F , gets a random key r, and must output
Page  351: In particular, for every MAC adversary A attacking I 0 (as in Attack Game 6.1) that issues
Page  351: at most Q signing queries, there exist an efficient MAC adversary BI and an efficient TCR
Page  351: adversary BH , which are elementary wrappers around A, such that
Page  351: Proof idea. Our goal is to show that no efficient MAC adversary can successfully attack I 0 . Such
Page  351: an adversary A asks the challenger to sign a few long messages m1 , m2 , . . . 2 M and gets back tags
Page  351: mi and m collide under the random key r. We will show that this lets us build an adversary BH
Page  351: Proof. Let X be the event that adversary A wins the MAC Attack Game 6.1 with respect to I 0 .
Page  351: responses. Furthermore, let (m, (t, r)) be the adversary’s final output. We define two additional
Page  352: To prove the theorem we construct a TCR adversary BH and a MAC adversary BI such that
Page  352: Adversary BI is essentially the same as in the proof of Theorem 8.1. Here we only describe the
Page  352: TCR adversary BH , which emulates a MAC challenger for A as follows:
Page  352: Hence, event Y is independent of the event j = u. For the same reason, if the adversary makes a
Page  355: work in the ideal cipher model. Show that for every adversary A that makes at most Q ideal-cipher
Page  358: SPR, if no efficient adversary, given a random x in X as input, can output y, x0 , y 0 such that
Page  358: is an enhanced TCR if no efficient adversary A can win the following game with non-negligible
Page  358: advantage: the adversary outputs m 2 M, is given random k 2 K, and outputs (k 0 , m0 ) such that
Page  359: poly. Specifically, for every adversary A that makes at most Qro queries to H, we have
Page  359: say that H is a weak collision resistant (WCR) if no efficient adversary can win the following
Page  359: adversary query the function H(k, ·) at any input of its choice. The adversary wins if it outputs a
Page  362: and a given adversary A, the attack game runs as follows:
Page  363: Challenger                            Adversary A
Page  363: Finally, we say that A is a Q-query adversary if A issues at most Q encryption queries. 2
Page  363: every efficient adversary A, the value CIadv[A, E] is negligible.
Page  364: In particular, we change Attack Game 9.1 so that the adversary can only obtain the encryption of
Page  364: efficient single-query adversary A, the value CIadv[A, E] is negligible.
Page  364: For starters, consider an eavesdropping adversary A. Since E is CPA-secure this does not help
Page  364: Now consider a more aggressive adversary A that attempts to make Bob receive a message that
Page  364: by A. The adversary’s goal is to create some ĉ such that m̂ := D(k, ĉ) 6= reject and m̂ 6= m.
Page  365: the adversary can recover m from m̂, in particular m̂ 6= reject. Ciphertext integrity, and therefore
Page  366: the adversary has all the power of an attacker in a chosen plaintext attack, but in addition, the
Page  366: adversary may obtain decryptions of ciphertexts of its choosing — subject to a restriction. Recall
Page  366: that in a chosen plaintext attack, the adversary obtains a number of ciphertexts from its challenger,
Page  366: in response to encryption queries. The restriction we impose is that the adversary may not ask
Page  366: attack game at all meaningful, it may also seem a bit unintuitive: if the adversary can decrypt
Page  366: for a given adversary A, we define two experiments. For b = 0, 1, we define
Page  366: • At the end of the game, the adversary outputs a bit b̂ 2 {0, 1}.
Page  366: phertext attack, or simply CCA-secure, if for all efficient adversaries A, the value CCAadv[A, E]
Page  366: we define security against an adversary that can only issue a single encryption query, but many
Page  367: Definition 9.6 (1CCA security). In Attack Game 9.2, if the adversary A is restricted to making
Page  367: efficient adversaries A, the value 1CCAadv[A, E] is negligible.
Page  367: runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
Page  367: And similarly, for adversaries restricted to a single encryption query, we have:
Page  367: In particular, suppose A is a CCA-adversary for E that makes at most Qe encryption queries
Page  367: and Qd decryption queries. Then there exist a CPA-adversary Bcpa and a CI-adversary Bci ,
Page  367: Proof idea. A CCA-adversary A issues encryption and allowed decryption queries. We first argue
Page  367: adversary ever issues a valid decryption query ci whose decryption is not reject, then this ci can be
Page  367: the adversary learns nothing by issuing decryption queries and they may as well be discarded. After
Page  367: removing decryption queries we end up with a standard CPA game. The adversary cannot win this
Page  367: Proof. Let A be an efficient CCA-adversary attacking E as in Attack Game 9.2, and which makes
Page  368: for efficient adversaries Bcpa and Bci . Then (9.3) follows from (9.4), along with (9.1) and (5.4).
Page  368: Moreover, as we shall see, the adversary Bcpa makes at most Qe encryption queries; therefore, if E
Page  368: Eventually the adversary outputs a guess b̂ 2 {0, 1}. We say that A wins the game if b = b̂ and we
Page  368: build a CI-adversary Bci that wins the CI attack game with probability at least Pr[Z]/Qd . Note
Page  368: that in Game 1, the decryption algorithm is not used at all. Adversary Bci ’s strategy is simply to
Page  368: attack game. More precisely, we can construct a CPA adversary Bcpa that plays the role of challenger
Page  370: The only way the adversary could distinguish this modification from the first is if he could
Page  370: modifications. Therefore, an adversary who can distinguish this modification from the second
Page  370: implementations are indistinguishable from the adversary’s point of view.
Page  371: In particular, for every ciphertext integrity adversary Aci that attacks EEtM as in Attack
Page  371: Game 9.1 there exists a MAC adversary Bmac that attacks I as in Attack Game 6.1, where
Page  371: For every CPA adversary Acpa that attacks EEtM as in Attack Game 5.2 there exists a CPA
Page  371: adversary Bcpa that attacks E as in Attack Game 5.2, where Bcpa is an elementary wrapper
Page  371: reduction. Suppose Aci is a ciphertext integrity adversary attacking EEtM . We construct a MAC
Page  371: adversary Bmac attacking I.
Page  371: Adversary Bmac plays the role of adversary in a MAC attack game for I. It interacts with
Page  371: a MAC challenger Cmac that starts by picking a random km R Km . Adversary Bmac works by
Page  372: Therefore, with probability CIadv[Aci , EEtM ] adversary Aci outputs a ciphertext (c, t) that makes
Page  372: the adversary wins the ciphertext integrity game. Even worse, if (r, c, t) is the encryption of a
Page  374: lets the adversary decrypt any ciphertext of its choice. It follows that EMtE need not be AE-secure,
Page  374: Suppose the adversary intercepts a valid ciphertext c := EMtE ( (ke , km ), m) for some unknown
Page  374: Lets us first show that the adversary can learn something about m[0] (the first 16-byte block
Page  375: We leave it as an instructive exercise to recast this attack in terms of an adversary in a chosen
Page  375: ciphertext query the adversary has advantage 1/256 in winning the game. This already proves that
Page  375: byte of the plaintext. Next, suppose the adversary could request an encryption of m shifted one
Page  375: cause the browser to repeatedly issue the request (9.10) giving the adversary the fresh encryptions
Page  376: Once the adversary learns one byte of the cookie it can shift the cookie one byte to the right
Page  377: against an adversary that makes a single chosen message query. Intuitively, the reason we can
Page  377: harder for the adversary to attack the MAC. Since one-time MACs are a little shorter and faster
Page  378: In particular, for every Q-query ciphertext integrity adversary Aci that attacks EMtE as in Attack
Page  378: Game 9.1 there exists two MAC adversaries Bmac and Bmac      that attack I as in Attack Game 6.1,
Page  378: and a PRF adversary Bprf that attacks F as in Attack Game 4.2, each of which is an elementary
Page  378: For every CPA adversary Acpa that attacks EMtE as in Attack Game 5.2 there exists a CPA
Page  378: adversary Bcpa that attacks E as in Attack Game 5.2, which is an elementary wrapper around
Page  378: integrity adversary. This adversary makes a series of queries, m1 , . . . , mQ . For each mi , the CI
Page  378: F . Here, ti is a MAC tag computed on mi . At the end of the attack game, adversary Aci outputs
Page  379: Note that after making this modification, the ti ’s are perfectly hidden from the adversary.
Page  379: c 6= cj , we have (m, t) 6= (mj , tj ). To turn Aci into a one-time MAC adversary, we have to
Page  379: For j = 0, 1, 2, 3, 4 we define Wj to be the event that the adversary wins in Game j.
Page  379: dent one-time pads. Since F is a secure PRF and 1/|X | is negligible, the adversary will not notice
Page  379: send ci to the adversary
Page  379: A standard argument shows that there exists an efficient PRF adversary Bprf such that:
Page  380: Game 2. Now we restrict the adversary’s winning condition to require that the IV used in the
Page  380: occurs. Based on these observations, we can easily construct an efficient MAC adversary Bmac    0   such
Page  380: that Pr[Z2 ]  MAC1 adv[Bmac , I]. Adversary Bmac runs as follows. It plays the role of challenger to
Page  380: adversary Bmac outputs a random pair in Y     |u|   `     `
Page  380: Game 3. We further constrain the adversary’s winning condition by requiring that the ciphertext
Page  380: the tags are encrypted using one-time pads the adversary cannot tell that he is given encryptions
Page  380: Since the adversary’s view in this game is identical to its view in Game 3 we have
Page  380: Adversary Bmac interacts with a MAC challenger C and works as follows:
Page  381: send ci to the adversary
Page  382: The challenger computes ci       E(k, mib , di , N i ), and sends ci to the adversary.
Page  382: the adversary’s control, as are the nonces N i , subject to the nonces being unique. For b = 0, 1, let
Page  382: against chosen plaintext attack, or simply CPA-secure, if for all efficient adversaries A,
Page  383: output a new triple (c, d, N ) that is accepted by the decryption algorithm. The adversary, however,
Page  383: (K, M, D, C, N ), and a given adversary A, the attack game runs as follows:
Page  383: for all efficient adversaries A, the value nCIad adv[A, E] is negligible.
Page  384: as follows. For encryption queries, in addition to a pair of messages (mi0 , mi1 ), the adversary
Page  384: queries, in addition to a ciphertext ĉj , the adversary submits associated data dˆj , and the challenger
Page  384: (c1 , d1 ), (c2 , d2 ), . . . corresponding to previous encryption queries. An adversary A’s advantage in
Page  384: negligible for all efficient adversaries A. If we restrict the adversary to a single encryption query,
Page  384: secure if this advantage is negligible for all efficient adversaries A.
Page  388: then the first ciphertext would be computed as c0     E(k, m0 , d, 0) where the adversary knows m0
Page  389: the participants this will look like the conversation ended normally. This can lead to a real-world
Page  390: In e↵ect, the adversary was able to make the browser receive a message that the server did not
Page  402: Your attack should work even if the adversary learns only one bit for every chosen-ciphertext query
Page  402: (a) Show that (E 0 , D0 ) is AE-secure even if the adversary knows k1 , but not k2 .
Page  402: (b) Show that (E 0 , D0 ) is not AE-secure if the adversary knows k2 but not k1 .
Page  402: AE-secure even if the adversary knows one of the keys, but not the other.
Page  402: attack, the adversary is allowed to make several decryption queries, such that in each query, the
Page  402: adversary only learns whether the result of the decryption was reject or not. Design an adversary
Page  404: weak version of CCA, namely where the adversary issues a single decryption query and is
Page  405: committing if an adversary can find a ciphertext c and two keys k0 , k1 such that c decrypts
Page  405: property means that the adversary can transmit c, but if he or she are later required to reveal the
Page  405: decryption key, say for an internal audit, the adversary can “open” the ciphertext in two di↵erent
Page  406: (i) the cipher is secure against a chosen plaintext attack (CPA security) when the adversary
Page  406: when the adversary knows nothing about k, and
Page  406: when the adversary is given a sub-key k 0 R K(k), but again knows nothing about k.
Page  408: a passive eavesdropping adversary cannot feasibly guess their shared key. The adversary can listen
Page  408: eavesdropper is listening to their conversation and hence they wish to encrypt the session. Since
Page  408: Thus, their initial goal is to generate a shared secret unknown to the adversary. They may later use
Page  408: where they take turns in sending messages to each other. The eavesdropping adversary can hear
Page  408: Alice and Bob should have a secret that is unknown to the adversary. The protocol itself provides
Page  409: which is a function of the random bits generated by A and B. The eavesdropping adversary A
Page  409: and a given adversary A, the attack game runs as follows.
Page  409: dropper if for all efficient adversaries A, the quantity AnonKEadv[A, P ] is negligible.
Page  409: This definition of security is extremely weak, for three reasons. First, we assume the adversary
Page  409: is unable to tamper with messages. Second, we only guarantee that the adversary cannot guess
Page  409: k in its entirety. This does not rule out the possibility that the adversary can guess, say, half
Page  410: scheme T = (G, F, I), defined over (X , Y), and a given adversary A, the attack game runs as
Page  410: and sends (pk , y) to the adversary.
Page  410: • The adversary outputs x̂ 2 X .
Page  410: We define the adversary’s advantage in inverting T , denoted OWadv[A, T ], to be the probability
Page  411: this protocol, in the sense of Definition 10.1. In Attack Game 10.1, the adversary sees the transcript
Page  411: consisting of the two messages pk and y. If the adversary could compute the secret x from this
Page  411: transcript with some advantage, then this very same adversary could be used directly to break the
Page  412: 10.3     A trapdoor permutation scheme based on RSA
Page  412: We now describe a trapdoor permutation scheme that is plausibly one-way. It is called RSA
Page  412: permutation. Despite many years of study, RSA is essentially the only known reasonable candidate
Page  412: RSA scheme).
Page  412: Here is how RSA works. First, we describe a probabilistic algorithm RSAGen that takes as
Page  412: RSAGen(`, e) :=
Page  412: Now we describe the RSA trapdoor permutation scheme TRSA = (G, F, I). It is parameterized
Page  412: G() :=   (n, d) R RSAGen(`, e),      pk    (n, e),   sk     (n, d)
Page  413: these schemes using RSA. Exercise 10.24 explores an idea that builds a proper trapdoor permutation
Page  413: scheme based on RSA.
Page  413: Ignoring this technical issue for the moment, let us first verify that TRSA satisfies the correctness
Page  413: So now we know that TRSA satisfies the correctness property of a trapdoor permutation scheme.
Page  413: However, it is not clear that it is one-way. For TRSA , one-wayness means that there is no efficient
Page  413: clear that if TRSA is one-way, then it must be hard to factor n; indeed, if it were easy to factor n,
Page  413: then one could compute d in exactly the same way as is done in algorithm RSAGen, and then use
Page  413: TRSA is to first factor n and then compute d as above. However, there is no known proof that the
Page  413: assumption that factoring n is hard implies that TRSA is one-way. Nevertheless, based on current
Page  413: evidence, it seems reasonable to conjecture that TRSA is indeed one-way. We state this conjecture
Page  413: Attack Game 10.3 (RSA). For given integers ` > 2 and odd e > 2, and a given adversary A,
Page  414: (n, d) R RSAGen(`, e),     x R Zn ,   y    xe 2 Zn
Page  414: and gives the input (n, y) to the adversary.
Page  414: • The adversary outputs x̂ 2 Zn .
Page  414: We define the adversary’s advantage in breaking RSA, denoted RSAadv[A, `, e], as the probability
Page  414: Definition 10.5 (RSA assumption). We say that the RSA assumption holds for (`, e) if for all
Page  414: efficient adversaries A, the quantity RSAadv[A, `, e] is negligible.
Page  414: We analyze the RSA assumption and present several known attacks on it later on in Chapter 17.
Page  414: RSAGen(`, e), and suppose that x 2 Zn and let y := xe . The number n is called an RSA modulus,
Page  414: exponent. We call (n, y) an instance of the RSA problem, and we call x a solution to this
Page  414: instance of the RSA problem. The RSA assumption asserts that there is no efficient algorithm that
Page  414: can e↵ectively solve the RSA problem.
Page  414: 10.3.1    Key exchange based on the RSA assumption
Page  414: TRSA . The protocol runs as follows:
Page  414: • Alice computes (n, d) R RSAGen(`, e), and sends (n, e) to Bob.
Page  414: RSA assumption, this is a secure anonymous key exchange protocol.
Page  414: We give a more mathematically precise definition of the RSA assumption, using the terminology
Page  414: putable functions of a security parameter . Likewise, RSAadv[A, `, e] is a function of . As usual,
Page  414: Definition 10.5 should be read as saying that RSAadv[A, `, e]( ) is a negligible function.
Page  414: the RSA scheme does not quite fit our definition of a trapdoor permutation scheme, as the definition
Page  414: Second, the specification of RSAGen requires that we generate random prime numbers of a given
Page  415: any efficient adversary wins with negligible probability): in this game, the challenger generates
Page  415: (pk , sk ) R G() and sends (pk , sk ) to the adversary; the adversary wins the game if he can output
Page  415: was invented by Diffie and Hellman. Just as with the protocol based on RSA, this protocol will
Page  418: by g 2 G. For a given adversary A, define the following attack game:
Page  418: and gives the value u to the adversary.
Page  418: • The adversary outputs some ↵
Page  418: (DL) assumption holds for G if for all efficient adversaries A the quantity DLadv[A, G] is neg-
Page  418: q generated by g 2 G. For a given adversary A, the attack game runs as follows.
Page  418: and gives the pair (u, v) to the adversary.
Page  418: • The adversary outputs some ŵ 2 G.
Page  419: tational Diffie-Hellman (CDH) assumption holds for G if for all efficient adversaries A the
Page  419: This is in contrast to the RSA problem: given an instance (n, e, y) of the RSA problem, and an
Page  419: generated by g 2 G. For a given adversary A, we define two experiments.
Page  419: and gives the triple (u, v, wb ) to the adversary.
Page  419: • The adversary outputs a bit b̂ 2 {0, 1}.
Page  419: Diffie-Hellman (DDH) assumption holds for G if for all efficient adversaries A the quantity
Page  421: group element u1 is uniformly distributed over G. Therefore, on input u1 , adversary A will output
Page  421: the RSA problem (in a more limited sense). These ideas are developed in the chapter exercises.
Page  422: group in which the DL, CDH, and DDH games are played. We will require that the adversary’s
Page  422: adversary’s advantage in breaking discrete-log in the group defined by should quickly go to zero.
Page  422: by the security parameter . In that game, the adversary is given the security parameter and
Page  423: adversary. Definition 10.6 should be read as saying that DLadv[A, G]( ) is a negligible function.
Page  423: It turns out that the RSA and DL assumptions are extremely versatile, and can be used in many
Page  423: hash functions based on the RSA and DL assumptions.
Page  423: efficient adversary A, its collision-finding advantage CRadv[A, H] is negligible. Here, CRadv[A, H]
Page  424: In particular, for every collision-finding adversary A, there exists a DL adversary B, which is
Page  424: Proof. We use the given collision-finding adversary A to build a DL adversary B as follows. When
Page  424: 10.6.2    Collision resistance based on RSA
Page  424: We shall work with an RSA encryption exponent e that is a prime. For this application, the bigger
Page  424: e is, the more compression we get. Let Ie := {0, . . . , e 1}. Let n be an RSA modulus, generated
Page  424: values e, n, and y are chosen once and for all, and together they determine a hash function Hrsa
Page  424: Hrsa (a, b) := ae y b .
Page  424: We will show that Hrsa is collision resistant under the RSA assumption. Note that Hrsa can be
Page  425: To analyze Hrsa , we will need a couple of technical results. The first result simply says that in
Page  425: the RSA attack game, it is no easier to compute an eth root of a random element of Z⇤n than it is
Page  425: Z⇤n . Denote by uRSAadv[A, `, e] the adversary A’s advantage in this modified attack game.
Page  425: Theorem 10.5. Let ` > 2 and odd e > 2 be integers. For every adversary A, there exists an adver-
Page  425: sary B, which is an elementary wrapper around A, such that uRSAadv[A, `, e]  RSAadv[B, `, e].
Page  425: Proof. Let A be a given adversary. Here is how B works. Adversary B receives a random element
Page  425: n, B can factor n, compute the RSA decryption exponent d, and then compute x := y d .
Page  425: Pr[W | y 2 Z⇤n ] = uRSAadv[A, `, e]
Page  425: Pr[W | y 2                 uRSAadv[A, `, e].       2
Page  425: The above theorem shows that the standard RSA assumption implies a variant RSA assumption,
Page  425: show the converse, that is, that this variant RSA assumption implies the standard RSA assumption.
Page  425: Theorem 10.7. The hash function Hrsa is collision resistant under the RSA assumption.
Page  425: In particular, for every collision-finding adversary A, there exists an RSA adversary B, which
Page  425: CRadv[A, Hrsa ]  RSAadv[B, `, e].                            (10.3)
Page  426: Proof. We construct an adversary B 0 that plays the alternative RSA attack game considered in
Page  426: Theorem 10.5. We will show that CRadv[A, Hrsa ] = uRSAadv[B 0 , `, e], and the theorem will the
Page  426: Our RSA adversary B 0 runs as follows. It receives (n, y) from its challenger, where n is an RSA
Page  426: modulus and y is a random element of Z⇤n . The values e, n, y define the hash function Hrsa , and
Page  426: adversary B 0 runs adversary A with this hash function. Suppose that A finds a collision. This is a
Page  426: completely falls apart in the presence of an active adversary who controls the network. The main
Page  426: problem. Protocols secure in this model can completely fall apart once the adversary can tamper
Page  427: Alice                                    Adversary                                Bob
Page  427: to the adversary. They use a protocol called Merkle puzzles (due to the same Merkle from the
Page  428: Security. The adversary sees the protocol transcript which includes all the puzzles and the quan-
Page  428: tity ` sent by Bob. Since the adversary does not know which puzzle Bob picked, intuitively, he
Page  428: needs to solve all puzzles until he finds puzzle P` . Thus, to recover s 2 M the adversary must solve
Page  428: L puzzles each one taking O(|K|) time to solve. Overall, the adversary must spend time O(L|K|).
Page  428: the analysis shows that if the adversary makes at most Q queries to the ideal cipher, then its
Page  428: Performance. Suppose we set L ⇡ |K|. Then the adversary must spend time O(L2 ) to break
Page  428: satisfy our definitions of security — with constant work the adversary has advantage about 1/L2
Page  429: than quadratic separation between the participants and the adversary. Unfortunately, a result by
Page  429: 10.1 (Computationally unbounded adversaries). Show that an anonymous key exchange
Page  429: protocol P (as in Definition 10.1) cannot be secure against a computationally unbounded adversary.
Page  430: adversary A, we have Distadv[A, Pdh , Pndh ]  DDHadv[A, G] + 1/q.
Page  431: That is, adversary B may output an incorrect answer, but for all inputs, the probability that its
Page  431: In particular, show that for every adversary A that distinguishes D and R, there exists a DDH
Page  431: adversary B (which is an elementary wrapper around A) such that
Page  432: In particular, show that for every adversary A that distinguishes D and R, there exists a DDH
Page  432: adversary B (which is an elementary wrapper around A) such that
Page  432: are computationally indistinguishable under the DDH. In particular, show that for every adversary
Page  432: A that distinguishes g R(k1 ) and g R(k2 ) there exists a DDH adversary B (which is an elementary
Page  433: could be chosen at random or adversarially chosen). Then we can generate a random element ū and
Page  433: a “trapdoor” ( , ⌧ ). Using this trapdoor, given group elements v, w, w̄ 2 G (possibly adversarially
Page  434: any random choices made by adversary. So we can think of the system parameter as a random
Page  434: made by the challenger and any random choices made by adversary. Let us call ⇤0 a “vulnerable”
Page  434: Note that even if an adversary breaks the DL with respect to a randomly generated system
Page  434: parameter, there could be many particular system parameters for which the adversary cannot
Page  434: or will not break the DL (it is helpful to imagine an adversary that is all powerful yet
Page  434: vulnerable system parameters for which the adversary breaks the DL.
Page  434: adversary to break the DL, CDH, or DDH assumptions. So to be on the safe side, we might insist
Page  435: every collision-finding adversary A, there exists a DL adversary B, which is an elementary wrapper
Page  435: is identical to Attack Game 10.6. Give an efficient adversary that has advantage 1/2 in solving the
Page  436: 10.22 (RSA variant (I)). Let n be an RSA modulus generated by RSAGen(`, e). Let X and
Page  436: 10.23 (RSA variant (II)). In Theorem 10.5, we considered a variant of the RSA assumption
Page  436: showed that the standard RSA assumption implies this variant RSA assumption. In this exercise,
Page  436: you are to show the converse. In particular, show that RSAadv[A, `, e]  uRSAadv[B, `, e] + 2 (` 2)
Page  436: for every adversary A.
Page  436: 10.24 (A proper trapdoor permutation scheme based on RSA). As discussed in Sec-
Page  436: tion 10.3, our RSA-based trapdoor permutation scheme does not quite satisfy our definitions,
Page  436: to patch things up. Let ` and e be parameters used for RSA key generation, and let G be the key
Page  436: generation algorithm, which outputs a pair (pk , sk ). Recall that pk = (n, e), where n is an RSA
Page  436: (b) Show under the RSA assumption, (G, F ⇤ , I ⇤ ) is one-way.
Page  436: 10.25 (Random self-reduction for RSA). Suppose we run (n, d) R RSAGen(`, e). There
Page  436: could be “weak” RSA moduli n for which an adversary can break the the RSA assumption with
Page  436: adversary then makes a sequence of queries. In each query, the adversary submits a proper subset
Page  437: The adversary wins the game if it outputs
Page  437: to the adversary. The adversary wins the game if it outputs g (↵ ) .
Page  437: Show that if there is an efficient adversary A that breaks n-product CDH with non-negligible
Page  437: probability, then there is an efficient adversary B that breaks n-power CDH with non-negligible
Page  437: Hdl and Hrsa , presented in Section 10.6, are trapdoor collision resistant.
Page  437: (b) Recall that Hrsa is defined as Hrsa (a, b) := ae y b 2 Zn , where n, e and y are parameters chosen
Page  437: 2nd-preimage resistance of Hrsa .
Page  437: can invert Hrsa . That is, given z 2 Zn as input, one can find (a, b) such that Hrsa (a, b) = z.
Page  437: Discussion: Part (c) shows that the factorization of n is a “stronger” trapdoor for Hrsa than the
Page  437: eth root of y. The latter only breaks 2nd-preimage resistance of Hrsa , whereas the former enables
Page  440: that this notion of security only models an eavesdropping adversary. We will discuss stronger
Page  440: (G, E, D), defined over (M, C), and for a given adversary A, we define two experiments.
Page  440: • The challenger computes (pk , sk ) R G(), and sends pk to the adversary.
Page  440: • The adversary computes m0 , m1 2 M, of the same length, and sends them to the challenger.
Page  440: • The challenger computes c R E(pk , mb ), and sends c to the adversary.
Page  440: • The adversary outputs a bit b̂ 2 {0, 1}.
Page  441: and the random choices made by the adversary. See Fig. 11.1 for a schematic diagram of Attack
Page  441: cure if for all efficient adversaries A, the value SSadv[A, E] is negligible.
Page  441: runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
Page  442: As usual, the proper interpretation of Attack Game 11.1 is that both challenger and adversary
Page  442: receive as a common input, and that the challenger generates ⇤ and sends this to the adversary
Page  442: To see why, suppose E is deterministic. Then the following adversary A breaks semantic security
Page  442: adversary always outputs 0. Similarly, when b = 1 it always outputs 1. Therefore
Page  443: this is because in the public-key setting, the adversary can encrypt any message he likes, without
Page  443: knowledge of any secret key material. The adversary does so using the given public key and never
Page  443: adversary cannot encrypt messages on his own.
Page  443: defined over (M, C), and for a given adversary A, we define two experiments.
Page  443: • The challenger computes (pk , sk ) R G(), and sends pk to the adversary.
Page  443: • The adversary submits a sequence of queries to the challenger.
Page  443: The challenger computes ci         E(pk , mib ), and sends ci to the adversary.
Page  443: • The adversary outputs a bit b̂ 2 {0, 1}.
Page  443: secure against chosen plaintext attack, or simply CPA secure, if for all efficient adversaries
Page  443: In particular, for every CPA adversary A that plays Attack Game 11.2 with respect to E, and
Page  443: which makes at most Q queries to its challenger, there exists an SS adversary B, where B is an
Page  444: orem 5.1. Suppose E = (G, E, D) is defined over (M, C). Let A be a CPA adversary that plays
Page  444: Next, we define an appropriate adversary B that plays Attack Game 11.1 with respect to E:
Page  444: adversary B can encrypt the relevant message using the public key.
Page  444: One can also consider multi-key CPA security, where the adversary sees many encryptions under
Page  445: Section 8.10.2). We then present a concrete instantiation of this scheme, based on RSA (see
Page  445: the challenger would normally evaluate H, it evaluates O instead. In addition, the adversary is
Page  445: adversary may make any number of such “random oracle queries” at any time of its choosing. We
Page  445: In particular, for every SS adversary A that attacks ETDF as in the random oracle version of
Page  445: Attack Game 11.1, there exist an inverting adversary Bow that attacks T as in Attack Game 10.2,
Page  446: and an SS adversary Bs that attacks Es as in Attack Game 2.1, where Bow and Bs are elementary
Page  446: Proof idea. Suppose the adversary sees the ciphertext (y, c), where y = F (pk , x). If H is modeled
Page  446: as a random oracle, then intuitively, the only way the adversary can learn anything at all about
Page  446: at the point x; however, if he could do this, we could easily convert the adversary into an adversary
Page  446: adversary’s point of view, k is completely random, and semantic security for ETDF follows directly
Page  446: to points at which the adversary actually queried the random oracle (as well as the point at which
Page  446: Game 0. Note that the challenger in Game 0 also has to respond to the adversary’s random oracle
Page  446: queries. The adversary can make any number of random oracle queries, but at most one encryption
Page  446: the adversary also has indirect access to the random oracle via the encryption query, where the
Page  446: Let Z be the event that the adversary queries the random oracle at the point x in Game 1.
Page  447: If event Z happens, then one of the adversary’s random oracle queries is the inverse of y under
Page  447: Thus, we can use adversary A to build an efficient adversary Bow that breaks the one-wayness
Page  447: Here is how adversary Bow works in detail. This adversary plays Attack Game 10.2 against a
Page  447: the adversary is essentially attacking Es as in the bit-guessing version of Attack Game 2.1 at this
Page  448: point. More precisely, we derive an efficient SS adversary Bs based on Game 1 that uses A as a
Page  448: Adversary Bs plays the bit-guessing version of Attack Game 2.1 against a challenger Cs , and plays
Page  448: 11.4.1     Instantiating ETDF with RSA
Page  448: Suppose we now use RSA (see Section 10.3) to instantiate T in the above encryption scheme ETDF .
Page  448: This scheme is parameterized by two quantities: the length ` of the prime factors of the RSA
Page  448: modulus, and the encryption exponent e, which is an odd, positive integer. Recall that the RSA
Page  448: is a fixed set into which we may embed Zn , for every RSA modulus n generated by RSAGen(`, e)
Page  448: The basic RSA encryption scheme is ERSA = (G, E, D), with message space M and ciphertext
Page  448: G() :=   (n, d) R RSAGen(`, e),     pk        (n, e),    sk     (n, d)
Page  449: Theorem 11.3. Assume H : X ! K is modeled as a random oracle. If the RSA assumption holds
Page  449: for parameters (`, e), and Es is semantically secure, then ERSA is semantically secure.
Page  449: In particular, for any SS adversary A that attacks ERSA as in the random oracle version of
Page  449: Attack Game 11.1, there exist an RSA adversary Brsa that breaks the RSA assumption for (`, e)
Page  449: as in Attack Game 10.3, and an SS adversary Bs that attacks Es as in Attack Game 2.1, where
Page  449: Brsa and Bs are elementary wrappers around A, such that
Page  449: SSro adv⇤ [A, ERSA ]  RSAadv[Brsa , `, e] + SSadv⇤ [Bs , Es ].
Page  450: In particular, for every SS adversary A that plays the random oracle version of Attack Game 11.1
Page  450: adversary Bcdh that plays Attack Game 10.5 with respect to G, and an SS adversary Bs that
Page  450: Proof idea. Suppose the adversary sees the ciphertext (v, c), where v = g . If H is modeled as
Page  450: a random oracle, then intuitively, the only way the adversary can learn anything at all about the
Page  450: point w = v ↵ ; however, if he could do this, we could convert the adversary into an adversary that
Page  450: at random from among all of the adversary’s random oracle queries. This is where the factor of Q in
Page  450: (11.8) comes from. So unless the adversary can break the CDH assumption, from the adversary’s
Page  450: Game 0. The adversary can make any number of random oracle queries, but at most one encryption
Page  450: queries, the adversary also has indirect access to the random oracle via the encryption query, where
Page  451: Let Z be the event that the adversary queries the random oracle at w in Game 1. Clearly,
Page  451: If event Z happens, then one of the adversary’s random oracle queries is the solution w to the
Page  451: to compute u and v, and nowhere else. Thus, we can use adversary A to build an adversary Bcdh
Page  451: to break the CDH assumption: we simply choose one of the adversary’s random oracle queries at
Page  451: In more detail, adversary Bcdh plays Attack Game 10.5 against a challenger Ccdh , and plays the
Page  452: to the reader to describe an efficient SS adversary Bs that uses A as a subroutine, such that
Page  452: Intuitively, H : G ! K is a secure KDF if no efficient adversary can e↵ectively distinguish
Page  452: given adversary A, we define two experiments.
Page  452: and sends yb to the adversary.
Page  452: • The adversary outputs a bit b̂ 2 {0, 1}.
Page  453: every efficient adversary A, the value KDFadv[A, F ] is negligible.
Page  453: Section 8.10.4 based on a universal hash function and the leftover hash lemma yields an uncondi-
Page  453: In particular, for every SS adversary A that plays Attack Game 11.1 with respect to EEG , there
Page  453: exist a DDH adversary Bddh that plays Attack Game 10.6 with respect to G, a KDF adversary
Page  453: Bkdf that plays Attack Game 11.3 with respect to H, and an SS adversary Bs that plays Attack
Page  453: Proof idea. Suppose the adversary sees the ciphertext (v, c), where v = g and c is a symmetric
Page  453: adversary cannot tell the di↵erence between u and w̃ and hence its advantage is only negligibly
Page  453: the adversary’s view, and therefore security follows by semantic security of Es . 2
Page  454: We describe an efficient DDH adversary Bddh that uses A as a subroutine, such that
Page  454: Adversary Bddh plays Attack Game 10.6 against a challenger Cddh , and plays the role of challenger
Page  454: then adversary A is playing our Game 0, and so p0 = Pr[W0 ], and if Cddh is running Experiment 1,
Page  454: We may easily derive an efficient KDF adversary Bkdf that uses A as a subroutine, such that
Page  454: Adversary Bkdf plays Attack Game 11.3 against a challenger Ckdf , and plays the role of challenger
Page  455: the adversary is essentially just playing the SS game with respect to Es at this point. We leave it
Page  455: to the reader to describe an efficient SS adversary Bs that uses A as a subroutine, such that
Page  455: nothing about the key sk , and should not help the adversary decrypt ciphertexts. Typical values
Page  455: In a 3-out-of-5 sharing, stealing only two shares should reveal nothing helpful to the adversary.
Page  455: single location. This ensures that there is no single point of failure that an adversary can attack
Page  456: A public-key threshold decryption scheme is secure if an adversary that completely compromises
Page  457: Exercise 11.17 we look at RSA threshold decryption.
Page  459: failure: an adversary who compromises the combiner during decryption will learn sk in the clear.
Page  460: Just as in Attack Game 11.1, our adversary will be allowed to make a single encryption query, in
Page  460: in addition to the public key, the adversary also gets to see t 1 shares of the secret key of its
Page  460: point of failure. To this end, we allow the adversary to make any number of combiner queries: in
Page  460: such a query, the adversary submits a single message to the challenger, and gets to see not only its
Page  460: Our security definition, given below, allows the adversary to eavesdrop on all traffic sent to the
Page  460: combiner. A more powerful adversary might completely compromise the combiner, and tamper
Page  461: with what it sends to the key servers. We do not consider such adversaries here, but will come
Page  461: decryption scheme E = (G, E, D, C) defined over (M, C), and for a given adversary A, we define
Page  461: • Setup: the adversary chooses a set S ✓ {1, . . . , s} of size t 1 and gives it to the challenger.
Page  461: The challenger runs (pk , sk 1 , . . . , sk s ) R G(s, t) and sends pk and {sk i }i2S to the adversary.
Page  461: • The adversary queries the challenger several times. Each query can be one of two types:
Page  461: i = 1, . . . , s. The challenger sends cj and c0j,1 , . . . , c0j,s to the adversary.
Page  461: – Single encryption query: The adversary sends m0 , m1 2 M, of the same length, to the
Page  461: challenger. The challenger computes c R E(pk , mb ), and sends c to the adversary. The
Page  461: adversary may only issue a single encryption query (which may be preceded or followed
Page  461: • The adversary outputs a bit b̂ 2 {0, 1}.
Page  461: scheme E is semantically secure if for all efficient adversaries A, the value thSSadv[A, E] is
Page  461: In particular, for every adversary A that attacks EthEG as in Attack Game 11.4, there exists an
Page  461: adversary B that attacks EEG as in Attack Game 11.1, such that
Page  462: Whenever A outputs a bit b̂ 2 {0, 1}, our adversary B outputs the same bit b̂.
Page  462: send them to the adversary, along with the ciphertext (v, c). 2
Page  462: adversary to break into all s servers within a short period of time, say ten minutes [66]. Otherwise,
Page  462: the adversary gets nothing. This is done by having the key servers proactively refresh the sharing
Page  463: G. In particular, you should show that for every adversary A attacking F as a PRF, there exists
Page  463: a DDH adversary B, which is an elementary wrapper around A, such that PRFro adv[A, F ] 
Page  463: 11.4 (Broken variant of RSA). Consider the following broken version of the RSA public-key
Page  463: encryption scheme: key generation is as in ERSA , but to encrypt a message m 2 Zn with public key
Page  463: pk = (n, e) do E(pk , m) := me in Zn . Decryption is done using the RSA trapdoor.
Page  463: adversary can recover the complete plaintext m from c using only 235 eth powers in Zn .
Page  464: ticular, you should show that the advantage of any adversary A in breaking the semantic
Page  464: security of EMEG is bounded by 2✏, where ✏ is the advantage of an adversary B (which is an
Page  465: you should show that the advantage of any adversary A in breaking the semantic security of EMMEG
Page  465: is bounded by 2(✏+1/q), where ✏ is the advantage of an adversary B (which is an elementary wrapper
Page  465: an adversary A, as follows. In Experiment b, for b = 0, 1, the challenger computes
Page  465: |Pr[W0 ] Pr[W1 ]|, and if this advantage is negligible for all efficient adversaries, we say that Ekem
Page  466: should prove a concrete security bound that says that for every adversary A attacking E,
Page  466: there are adversaries Bkem and Bs (which are elementary wrappers around A) such that
Page  466: encryption scheme to the multi-key setting. In this attack game, the adversary gets to obtain
Page  466: the advantage of any adversary A in breaking the multi-key CPA security of a scheme is at most
Page  466: Qk Qe · ✏, where ✏ is the advantage of an adversary B (which is an elementary wrapper around A)
Page  466: advantage of any adversary A in breaking the multi-key CPA security of EMEG is bounded by
Page  466: 2(✏ + 1/q), where ✏ is the advantage of an adversary B (which is an elementary wrapper around A)
Page  467: that makes use of RSA composites:
Page  468: (a) Define a security model that captures this requirement. The adversary should be given t
Page  468: a challenge ciphertext, the adversary should learn nothing about which of the t public keys
Page  468: (c) Show that the RSA public-key encryption system ERSA is not anonymous. Assume that all t
Page  468: public keys are generated using the same RSA parameters ` and e.
Page  468: 11.17 (RSA threshold decryption). Let us show how to enable simple threshold decryption for
Page  468: the RSA public key encryption scheme of Section 11.4.1.
Page  468: (a) Recall that the key generation algorithm generates numbers n, e, d, where n is the RSA
Page  469: vice-versa.
Page  469: (b) Assume that EEG is semantically secure. Show that the adversary cannot break semantic
Page  470: defined over (M, C), and for a given adversary A, we define two experiments.
Page  470: • The challenger computes (pk , sk ) R G() and sends pk to the adversary.
Page  471: • At the end of the game, the adversary outputs a bit b̂ 2 {0, 1}.
Page  471: the adversary makes only a single encryption query:
Page  471: Definition 12.2 (1CCA security). In Attack Game 12.1, if the adversary A is restricted to
Page  471: simply, 1CCA secure, if for all efficient adversaries A, the value 1CCAadv[A, E] is negligible.
Page  471: In particular, for every CCA adversary A that plays Attack Game 12.1 with respect to E, and
Page  471: which makes at most Qe encryption queries to its challenger, there exists a 1CCA adversary B
Page  471: Initialization phase: the challenger generates (pk , sk ) R G() and sends pk to the adversary.
Page  471: Phase 1: the adversary submits a series of decryption queries to the challenger; each such query
Page  472: Encryption query: the adversary submits a single encryption query (m0 , m1 ) to the challenger;
Page  472: Phase 2: the adversary again submits a series of decryption queries to the challenger; each such
Page  472: Finish: at the end of the game, the adversary outputs a bit b̂ 2 {0, 1}.
Page  472: random, and then runs Experiment b against the adversary A. In this game, we measure A’s bit-
Page  472: And similarly, for adversaries restricted to a single encryption query, we have:
Page  472: attack game, why can the adversary get any message decrypted except the ones he really wants
Page  473: CCA adversary A that has advantage 1 in the CCA security game. Here is how.
Page  473: • Create a pair of messages, each with the same header, but di↵erent bodies. Our adversary A
Page  476: jump from S to R, in spite of any information the adversary may glean by getting R to decrypt
Page  477: ideal implementations are indistinguishable from the adversary’s point of view.
Page  477: a CCA-secure system from RSA. Later, in Section 12.6, we will show how to construct suitable
Page  478: but where the adversary has access to an image oracle.
Page  478: given trapdoor function scheme T = (G, F, I), defined over (X , Y), and a given adversary A, the
Page  478: and sends (pk , y) to the adversary.
Page  478: • The adversary makes a series of image oracle queries to the challenger. Each such query is
Page  478: • The adversary outputs x̂ 2 X .
Page  478: We define the adversary’s advantage in inverting T given access to an image oracle, denoted
Page  478: if for all efficient adversaries A, the quantity IOWadv[A, T ] is negligible.
Page  478: evaluates O instead. In addition, the adversary is allowed to ask the challenger for the value of the
Page  478: function O at any point of its choosing. The adversary may make any number of such “random
Page  479: In particular, for every 1CCA adversary A that attacks ETDF    as in the random oracle version of
Page  479: Definition 12.2, there exist an inverting adversary Biow that breaks the one-wayness assumption
Page  479: for T as in Attack Game 12.2, and a 1CCA adversary Bs that attacks Es as in Definition 9.6,
Page  479: that the adversary Biow satisfies.
Page  479: Proof idea. The crux of the proof is to show that the adversary’s decryption queries do not help
Page  479: • If the adversary has previously queried the random oracle at x̂, and if k̂ was the output of
Page  479: • Otherwise, if the adversary has not made such a random oracle query, then the challenger
Page  479: does not know the correct value of the symmetric key — but neither does the adversary. The
Page  479: challenger must do some extra book-keeping to ensure consistency, so that if the adversary
Page  479: the one-wayness assumption for T , the adversary will never query the random oracle at x, and so
Page  479: from the adversary’s point of view, the symmetric key k used in the encryption query, and used in
Page  480: random oracle queries, in addition to encryption and decryption queries. The adversary can make
Page  480: oracle queries, the adversary also has indirect access to the random oracle via the encryption and
Page  480: queries made by the adversary: if Pre[ŷ] = x̂, this means that the adversary queried the oracle at
Page  480: that neither the adversary nor the challenger previously queried the random oracle at x̂, and
Page  480: the adversary later queries the random oracle at the point x̂, this same value of k̂ will be
Page  481: Let Z be the event that the adversary queries the random oracle at x in Game 1. Clearly,
Page  482: therefore, is use A to build an efficient adversary Biow that breaks the one-wayness assumption
Page  482: process decryption queries of the form (y, ĉ), where ĉ 6= c. As such, the adversary is essentially just
Page  482: efficient 1CCA adversary Bs based on Game 1 that uses A as a subroutine, such that
Page  482: This adversary Bs generates (pk , sk ) itself and uses sk to answer queries from A.
Page  482: 12.3.1    Instantiating ETDF with RSA
Page  482: Suppose we instantiate ETDF  using RSA just as we did in Section 11.4.1. The underlying trapdoor
Page  482: exactly the same scheme ERSA as was presented in Section 11.4.1. Second, the implementation of
Page  482: Theorem 12.3. Assume H : X ! K is modeled as a random oracle. If the RSA assumption holds
Page  482: for parameters (`, e), and Es is 1CCA secure, then ERSA is CCA secure.
Page  482: In particular, for every 1CCA adversary A that attacks ERSA as in the random oracle version
Page  482: of Definition 12.2, there exist an RSA adversary Brsa that breaks the RSA assumption for (`, e)
Page  482: as in Attack Game 10.3, and a 1CCA adversary Bs that attacks Es as in Definition 9.6, where
Page  482: Brsa and Bs are elementary wrappers around A, such that
Page  482: 1CCAro adv[A, ERSA ]  2 · RSAadv[Brsa , `, e] + 1CCAadv[Bs , Es ].
Page  482: We saw that the basic RSA encryption scheme ERSA could be shown to be CCA secure in the random
Page  482: oracle model under the RSA assumption (and assuming the underlying symmetric cipher was
Page  483: against chosen ciphertext attack, suppose the public key is u = g ↵ . Now, suppose an adversary
Page  483: for some arbitrary message m̂. Further, suppose the adversary can obtain the decryption m⇤ of the
Page  483: be e↵ectively used by the adversary to answer questions of the form “is (u, v̂, ŵ) a DH-triple?” for
Page  483: group elements v̂ and ŵ of the adversary’s choosing. In general, the adversary would not be able to
Page  483: of prime order q generated by g 2 G. For a given adversary A, the attack game runs as follows.
Page  483: and gives (u, v) to the adversary.
Page  483: • The adversary makes a sequence of DH-decision oracle queries to the challenger. Each query
Page  483: he sends “yes” to the adversary, and otherwise, sends “no” to the adversary.
Page  483: • Finally, the adversary outputs some ŵ 2 G.
Page  484: We stress that in the above attack game, the adversary can ask the challenger for help in
Page  484: adversaries A the quantity ICDHadv[A, G] is negligible.
Page  484: In particular, for every 1CCA adversary A that attacks EEG  as in the random oracle version
Page  484: of Definition 12.2, there exist an ICDH adversary Bicdh for G as in Attack Game 12.3, and
Page  484: a 1CCA adversary Bs that attacks Es as in Definition 9.6, where Bicdh and Bs are elementary
Page  485: Game 0. The logic of the challenger is shown in Fig. 12.2. The adversary can make any number
Page  485: adversary also has indirect access to the random oracle via the encryption and decryption queries,
Page  485: assign a random value k̂ to Map 0 [v̂], and then later, if the adversary queries the random oracle
Page  486: (u, v̂) of the CDH problem. However, if the adversary queries the random oracle at the point
Page  486: (v̂, ŵ), the adversary will see the same value k̂, and so consistency is maintained.
Page  486: therefore, is use A to build an efficient adversary Bicdh that breaks the CDH assumption for G,
Page  486: process decryption queries of the form (v, ĉ), where ĉ 6= c. As such, the adversary is essentially just
Page  486: efficient 1CCA adversary Bs based on Game 1 that uses A as a subroutine, such that
Page  488: 12.5.1    Universal projective hash functions
Page  488: is satisfied, then we say this scheme is a universal projective hash function.
Page  489: So this is a projective hash function. To show that it is universal, it suffices to show that h and
Page  490: Attack Game 12.4 (Universal distinguishing game). For a given adversary A, we define two
Page  490: • At the end of the game, the adversary outputs a bit b̂ 2 {0, 1}.
Page  490: Lemma 12.6. In Attack Game 12.4, Pr[W0 ] = Pr[W1 ] for all adversaries A.
Page  490: are independent, so replacing z0 by random z1 does not change the distribution of the adversary’s
Page  490: evaluation queries — we just assume that the adversary adheres to this restriction. In any case,
Page  490: the result of Lemma 12.6 applies to computationally unbounded adversaries, so this is not really
Page  490: an issue. Additionally, in our eventual application of Lemma 12.6, the adversary will in fact know
Page  490: 12.5.2     Universal2 projective hash functions
Page  490: universal, which is called universal2 . Again, we present the intuitive idea in terms of function
Page  490: We can easily extend our universal projective hash function scheme for Lu ✓ G2 in Section 12.5.1
Page  490: to obtain a universal2 projective hash function scheme for Lu . In this scheme, our “tags” will be
Page  491: as (h1 h⇢2 ) . The universal2 independence property is established by the following lemma, which
Page  491: Attack Game 12.5 (Universal2 guessing game). For a given adversary A, the game runs as
Page  492: Lemma 12.8. In Attack Game 12.5, for any adversary A that outputs at most Q tuples, the
Page  493: In particular, for every 1CCA adversary A that attacks ECS as in Definition 12.2, and makes at
Page  493: most Qd decryption queries, there exist a DDH adversary Bddh for G as in Attack Game 10.6,
Page  493: a 1CCA adversary Bs that attacks Es as in Definition 9.6, a KDF adversary Bkdf that attacks
Page  493: H as in Attack Game 11.3, and a collision-finding adversary Bcr that attacks H 0 as in Attack
Page  494: Game 0. The logic of the challenger is shown in Fig. 12.3. The adversary can make any number
Page  495: for an efficient DDH adversary Bddh , which works as follows. After it obtains its DDH problem
Page  495: instance (u, v, w) from its own challenger, adversary Bddh plays the role of challenger to A in
Page  495: for an efficient collision-finding adversary Bcr . Indeed, adversary Bcr just plays Game 4 and waits
Page  496: Using the claim, we will show how to design an adversary that wins Attack Game 12.5 with
Page  497: where Bkdf is an efficient adversary attacking H as a KDF, and Bs is a 1CCA adversary attacking
Page  497: function scheme, and in particular (in Section 12.3.1) with RSA. In Section 12.4, we saw how to
Page  498: adversary A, the attack game proceeds as follows:
Page  498: and sends (pk , y) to the adversary.
Page  498: • The adversary outputs x̂ 2 R.
Page  499: every efficient adversary A, the value OWadv[A, Ea ] is negligible.
Page  499: Note that because Ea may be probabilistic, an adversary that wins Attack Game 12.6 may not
Page  499: In particular, assume that Ea is ✏-unpredictable. Also assume that adversary A attacks TFO as
Page  499: its random oracle queries. Then there exists an adversary Bow that breaks the one-wayness
Page  499: modify an any adversary to ensure that it behaves this way, increasing its random-oracle queries
Page  499: representing the hash function U . The logic of the challenger is shown in Fig. 12.4. The adversary
Page  499: array Pre : Y ! X is used to track the adversary’s random oracle queries. Basically, Pre[ŷ] = x̂
Page  501: Let Z1 be the event that in Game 1, the adversary submits an image oracle query ŷ such that
Page  502: We sketch the design an efficient adversary B such that
Page  502: pk and y. Adversary B interacts with A just as the challenger in Game 3. The key observation is
Page  503: In particular, assume that Ea is ✏-unpredictable. Then for every 1CCA adversary A that attacks
Page  503: U , there exist an adversary Bow that breaks the one-wayness assumption for Ea as in Attack
Page  503: Game 12.6, and a 1CCA adversary Bs that attacks Es as in Definition 9.6, where Bow and Bs
Page  503: • Ea is one-way under the CDH assumption. Indeed, an adversary A that breaks the one-
Page  503: wayness assumption for Ea is easily converted to an adversary B that breaks the CDH with
Page  503: same advantage. Given an instance (u, v) 2 G2 of the CDH problem, adversary B plays the
Page  503: – when A outputs x 2 G, adversary B outputs w               y/x.
Page  504: In particular, for every 1CCA adversary A that attacks EFO     as in the random oracle version
Page  504: oracle for H, and QU queries to the random oracle for U , there exist an adversary Bcdh that
Page  504: breaks the CDH assumption for G as in Attack Game 10.5, and a 1CCA adversary Bs that
Page  505: to a pair of messages (mi0 , mi1 ), the adversary also submits associated data di , and the challenger
Page  505: computes ci R E(pk , mib , di ). For decryption queries, in addition to a ciphertext ĉj , the adversary
Page  505: tion queries. An adversary A’s advantage in this game is denoted CCAad adv[A, E], and the scheme
Page  505: is said to be CCA secure if this advantage is negligible for all efficient adversaries A. If we
Page  505: restrict the adversary to a single encryption query, as in Definition 12.2, the advantage is denoted
Page  505: efficient adversaries A.
Page  506: The most widely used public-key encryption scheme using RSA is described in a standard from
Page  506: RSA Labs called PKCS1. This scheme is quite di↵erent from the scheme ERSA we presented in
Page  506: Why does the PKCS1 standard not use ERSA ? The reason is that when encrypting a short
Page  506: message — much shorter than the RSA modulus n — a PKCS1 ciphertext is more compact than
Page  506: an ERSA ciphertext. The ERSA scheme outputs a ciphertext (y, c) where y is in Zn and c is a
Page  506: and nothing else. In these settings, schemes like PKCS1 are more space efficient than ERSA . It
Page  506: (although encryption time with ElGamal is typically higher than with RSA).
Page  507: When the trapdoor function T is RSA it will be convenient to call this scheme RSA-PS encryption.
Page  507: For example, when RSA is coupled with PKCS1 padding we obtain RSA-PKCS1 encryption.
Page  507: left-most 8 bits are zero. These zero bits are meant to accommodate a t-bit RSA modulus, so that
Page  507: all such strings are binary encodings of numbers that are less than the RSA modulus. The message
Page  507: shorter than the RSA modulus. For an RSA modulus of size 2048 bits, the message can be at most
Page  508: By coupling PKCS1 padding with RSA, as in (12.41), we obtain the RSA-PKCS1 encryption
Page  508: scheme. What can we say about the security of RSA-PKCS1? As it turns out, not much. In fact,
Page  508: 12.8.3    Bleichenbacher’s attack on the RSA-PKCS1 encryption scheme
Page  508: RSA-PKCS1 encryption is not secure against chosen ciphertext attacks. We describe an attack, due
Page  508: moved away from RSA encryption altogether (see Section 21.10).
Page  508: pre master secret, and encrypts it with RSA-PKCS1 under the server’s public-key. It
Page  508: attack on RSA-PKCS1. Suppose the attacker has a ciphertext c that it intercepted from an earlier
Page  508: SSL session with the server. This c is an encryption generated using the server’s RSA public key
Page  508: (n, e), with RSA modulus n and encryption exponent e. The attacker’s goal is to decrypt c. Let x
Page  509: Bleichenbacher showed that for a 2048-bit RSA modulus, this oracle is sufficient to recover
Page  509: This attack is a classic example of a real-world chosen ciphertext attack. The adversary has
Page  509: Px ). After enough queries, the adversary is able to obtain the decryption of c. Clearly, this attack
Page  509: would not be possible if RSA-PKCS1 were CCA-secure: CCA security implies that such attacks
Page  509: wide deployment of RSA-PKCS1 encryption, the question then is how to best defend against this
Page  509: The solution, implemented in TLS 1.0, changes the RSA-PKCS1 server-side decryption process
Page  509: 2.   decrypt the RSA-PKCS1 ciphertext to recover the plaintext m,
Page  510: In any case, a TLS server MUST NOT generate an alert if processing an RSA-encrypted
Page  510: The failure of RSA-PKCS1 leaves us with the original question: is there a padding scheme (P, U )
Page  510: measured in bytes. As before, in order to accommodate a t-bit RSA modulus, we insist that the
Page  512: Finally, the public-key encryption scheme RSA-OAEP is obtained by combining the RSA trap-
Page  512: the adversary is given pk and y     F (pk , x), for some pk and random x 2 X , and is asked to produce
Page  512: x. In the game defining a partial one-way function, the adversary is given pk and y, but is only
Page  512: asked to produce, say, certain bits of x. If no efficient adversary can accomplish even this simpler
Page  512: the adversary is asked to produce a particular function f of x. This is captured in the following
Page  512: X ! Z, and a given adversary A, the attack game runs as follows:
Page  512: and sends (pk , y) to the adversary.
Page  512: • The adversary outputs ẑ 2 Z.
Page  512: We define the adversary’s advantage, denoted POWadv[A, T , f ], to be the probability that ẑ = f (x).
Page  512: way with respect to f : X ! Z if, for all efficient adversaries A, the quantity POWadv[A, T , f ]
Page  512: Clearly, a partial one-way trapdoor function is also a one-way trapdoor function: if an adversary
Page  513: Given Theorem 12.13 the question is then: is RSA a partial one-way function? We typically
Page  513: assume RSA is one-way, but is it partial one-way when the adversary is asked to compute only
Page  513: (t h 8) bits of the pre-image? As it turns out, if RSA is one-way then it is also partial one-
Page  513: way. More precisely, suppose there is an efficient adversary A that given an RSA modulus n
Page  513: significant bits of x. Then there is an efficient adversary B that uses A and recovers all the bits of
Page  513: As a result of this wonderful fact, we obtain as a corollary of Theorem 12.13 that RSA-OAEP is
Page  513: CCA-secure in the random oracle model assuming only that RSA is a one-way function. However,
Page  513: the concrete security bounds obtained when proving CCA security of RSA-OAEP based on the
Page  513: one-wayness of RSA are quite poor.
Page  513: Manger’s timing attack. RSA-OAEP is tricky to implement securely. Suppose the OAEP
Page  513: consider again the setting of Bleichenbacher’s attack on PKCS1. The adversary has a ciphertext c,
Page  513: generated using under the server’s RSA public key, with RSA modulus n and encryption exponent
Page  513: e. The adversary wants to decrypt c. It can repeatedly interact with the server, sending it c0      c·re
Page  513: in Zn , for various values of r of the adversary’s choice. By measuring the time that the server takes
Page  513: In the previous section we saw that RSA-OAEP is CCA-secure assuming RSA is a one-way function.
Page  513: For RSA specifically, it is possible to use a simpler CCA-secure padding scheme. This simpler
Page  513: slightly longer than half the size of the modulus, that is, slightly more than t/2 bits. RSA-SAEP+
Page  513: is CCA-secure, in the random oracle model, assuming the RSA function is one-way [25]. It provides
Page  513: a simple alternative padding scheme for RSA.
Page  514: cise 11.5 is not CCA secure. Your adversary should have an advantage of 1 in the 1CCA attack
Page  514: that E 0 is not CCA secure. Your adversary should have an advantage of 1 in the 1CCA attack
Page  514: adversary in this attack is poly-bounded and its success probability is 1 ✏, where ✏ is negligible.
Page  514: To simplify the analysis of your adversary’s success probability, you may model H : Z⇤p ⇥ Z⇤p ! K
Page  515: Ekem in terms of an attack game, played between a challenger and an adversary A, as follows. In
Page  515: and sends (kb , ckem ) to A. Next, the adversary submits a sequence of decryption queries to the
Page  515: adversaries, we say that Ekem is 1CCA secure.
Page  515: concrete security bound that says that for every adversary A attacking E, there are adversaries
Page  516: encryption scheme to the multi-key setting. In this attack game, the adversary gets to obtain
Page  516: is a bound on the number of encryption queries per key. That is, the advantage of any adversary A
Page  516: of an adversary B (which is an elementary wrapper around A) that breaks the scheme’s 1CCA
Page  517: adversary that attacks the multi-key CCA security of xEEG0 . Show that A’s advantage is at most
Page  517: where ✏icdh is that advantage of an ICDH adversary Bicdh attacking G and ✏s is the advantage of a
Page  517: 1CCA adversary Bs attacking Es (where both Bicdh and Bs are elementary wrappers around A).
Page  518: adversary where ŷ 6= y. If Es provides ciphertext integrity, then in testing whether ŷ is in the image
Page  518: of F (pk , ·), we can instead test if the adversary queried the random oracle at a preimage x̂ of ŷ. If
Page  518: the adversary does not learn why. In particular, the adversary must not learn if decryption failed
Page  518: by the adversary, then the analysis in this exercise no longer applies. By contrast, the analysis
Page  518: in Theorem 12.2 is una↵ected by this side-channel leak: the adversary is given an image oracle
Page  519: adversarially chosen, we have to modify the attack game in Exercise 12.5, so that the adversary is
Page  519: by a sequence of additional decryption queries. In the encryption query, the adversary supplies d,
Page  519: (k0 , ckem ) or (k1 , ckem ) to the adversary. Decryption queries work just as in Exercise 12.5, except
Page  519: the adversary chooses the associated data dˆ as well as the ciphertext ĉkem , with the restriction that
Page  520: 12.20 (Baby Bleichenbacher attack). Consider an RSA public key (n, e), where n is an RSA
Page  520: (b) Suppose an attacker obtains an RSA public key and an element c 2 Zn . It wants to compute
Page  520: adversary can recover the eth root of c.
Page  521: 12.23 (RSA is partial one-way). Consider an RSA public key (n, e), where n is an RSA
Page  521: of x. Hence, A is an RSA partial one-way adversary for the most significant bits.
Page  521: Discussion: This result shows that if RSA is one-way, then an adversary cannot even
Page  522: for every adversary A that makes at most Qd decryption queries. Conclude that ESCS is CCA
Page  522: Games 12.4 and 12.5, allowing the adversary to choose the values (v, w) (and ⇢) adaptively.
Page  522: (a) Consider a variant of Attack Game 12.4 in which the adversary first submits u 2 G to the
Page  522: challenger (which defines Lu ), obtaining the auxiliary information h; then the adversary makes
Page  522: some number of evaluation queries; at some point, the adversary submits (v, w) 2 G2 \ Lu to
Page  522: the challenger, obtaining zb ; finally, the adversary continues making evaluation queries, and
Page  522: (b) Consider a variant of Attack Game 12.5 in which the adversary first submits u 2 G to the
Page  522: challenger (which defines Lu ), obtaining the auxiliary information (h1 , h2 ); then the adversary
Page  522: makes some number of evaluation queries; at some point, the adversary submits (v, w) 2
Page  522: G2 \ Lu and ⇢ 2 Zq to the challenger, obtaining z; finally, the adversary continues making
Page  522: notion of CCA security, corresponding to a variant of the CCA attack game in which the adversary
Page  523: as we did for ordinary CCA security, it suffices to assume that the adversary makes just a single
Page  523: 12.28 (Generalizing universal projective hash functions). This exercise develops a construc-
Page  523: tion for universal projective hash functions that generalizes the one presented in Section 12.5.1. Let
Page  524: 12.29 (A universal projective hash function for EMCS ). Consider the encryption scheme
Page  524: Design a universal projective hash function for L with outputs in G. The algorithm to evaluate
Page  524: (IHDH) assumption holds for (G, H) if it is infeasible for an efficient adversary to distinguish
Page  524: and sends (u, v, k) to the adversary. After that, the adversary is allowed to make a series queries.
Page  524: and sends k̃ to the adversary. Experiment 1 is exactly the same as Experiment 0, except that the
Page  525: adversary A in this attack game.
Page  525: assumption. In particular, show that for every I2CDH adversary A, there exists a CDH
Page  525: adversary B (where B is an elementary wrapper around A), such that
Page  525: show that for every 1CCA adversary A attacking E2cdh , there exist an I2CDH adversary Bi2cdh
Page  525: and a 1CCA adversary Bs , where Bi2cdh and Bs are elementary wrappers around A, such that
Page  525: that Es is 1CCA secure). In particular, show that for every 1CCA adversary A attacking
Page  525: E2cdh , there exist a CDH adversary Bcdh and a 1CCA adversary Bs , where Bcdh and Bs are
Page  526: queries. Let A be an adversary that attacks the multi-key CCA security of xE2cdh . Show that A’s
Page  526: where ✏cdh is that advantage of a CDH adversary Bcdh attacking G and ✏s is the advantage of a
Page  526: 1CCA adversary Bs attacking Es (where both Bcdh and Bs are elementary wrappers around A).
Page  527: must require that an adversary, who has pk , cannot generate a valid signature on a fake update
Page  530: Challenger                                    Adversary A
Page  530: adversary the power to mount a chosen message attack, namely the attacker can request the
Page  530: signature on any message of his choice. Even with such power, the adversary should not be able
Page  530: pair (m, ) for some new message m. Here “new” means a message that the adversary did not
Page  530: adversary A. The game is described below and in Fig. 13.1.
Page  530: over (M, ⌃), and a given adversary A, the attack game runs as follows:
Page  530: We say that the adversary wins the game if the following two conditions hold:
Page  531: the game. Finally, we say that A is a Q-query adversary if A issues at most Q signing queries.
Page  531: Definition 13.2. We say that a signature scheme S is secure if for all efficient adversaries A, the
Page  531: In case the adversary wins Attack Game 13.1, the pair (m, ) it outputs is called an existential
Page  531: tag verification queries do not help the adversary forge MACs. In the case of digital signatures,
Page  531: verification queries are a non-issue — the adversary can always verify message-signature pairs for
Page  531: pk 1 help the adversary forge signatures for pk 2 ? If that were possible then our definition of secure
Page  531: the definition of secure MACs. Here we only require that the adversary cannot forge a signature
Page  531: on a new message m. We do not preclude the adversary from producing a new signature on m
Page  531: from some other signature on m. That is, a signature scheme is secure even if the adversary can
Page  531: In contrast, for MAC security we insisted that given a message-tag pair (m, t) the adversary
Page  531: queries do not help the adversary (see Theorem 6.1 and Exercise 6.7).
Page  531: Attack Game 13.2. For a given signature scheme S = (G, S, V ), and a given adversary A, the
Page  532: Strong security ensures that for a secure signature scheme, the adversary cannot create a new
Page  533: given to both the adversary and the challenger. The advantage SIGadv[A, S] is then a function of .
Page  534: In particular, suppose A is a signature adversary attacking S 0 (as in Attack Game 13.1). Then
Page  534: there exist an efficient signature adversary BS and an efficient collision finder BH , which are
Page  534: In particular, for every signature adversary A attacking S 0 (as in Attack Game 13.1) that issues
Page  534: at most Q signing queries, there exist an efficient signature adversary BS and an efficient TCR
Page  534: adversary BH , which are elementary wrappers around A, such that
Page  535: concrete signature scheme from the only trapdoor permutation we have, namely RSA. Recall that
Page  535: would normally evaluate H, it evaluates O instead. In addition, the adversary is allowed to ask the
Page  535: challenger for the value of the function O at any point of its choosing. The adversary may make
Page  535: In particular, let A be an efficient adversary attacking SFDH in the random oracle version of
Page  535: signing queries. Then there exists an efficient inverting adversary B that attacks T as in Attack
Page  536: adversary has to compute = I(sk , y), where y = H(m). With H modeled as a random oracle, the
Page  536: adversary can get arbitrary messages signed before producing its forgery. Again, since H is modeled
Page  536: as a random oracle, this e↵ectively means that to break the signature scheme, the adversary must
Page  536: hash outputs on various messages), the adversary can ask to see preimages of some of the yi ’s
Page  536: points the adversary will invert F (pk , ·). This is where the factor Qro + Qs + 1 in (13.3) comes
Page  536: forge a signature, the adversary simply chooses a random 2 X and computes m                 F (pk , ).
Page  536: 13.3.1    Signatures based on the RSA trapdoor permutation
Page  536: RSA. We obtain the RSA full domain hash signature scheme, denoted SRSA-FDH . Recall that
Page  536: parameters for RSA are generated using algorithm RSAGen(`, e) which outputs a pair (pk , sk )
Page  536: where pk = (n, e). Here n is a product of two `-bit primes. The RSA trapdoor permutation
Page  536: For each public key pk = (n, e), the SRSA-FDH system needs a hash function H that maps
Page  536: Y := {1, . . . , 22` 2 } which, when embedded in Zn , covers a large fraction of Zn , for all the RSA
Page  536: moduli n output by RSAGen(`, e).
Page  536: We describe the signature scheme SRSA-FDH using a hash function H defined over (M, Y). We
Page  536: chose Y as above so that |Y|        n/4 for all n output by RSAGen(`, e). This is necessary for the
Page  537: proof of security. Because an RSA modulus n is large, at least 2048 bits, the hash function H
Page  537: For a given hash function H : M ! Y, the SRSA-FDH signature scheme works as follows:
Page  537: G() :=    (n, d) R RSAGen(`, e),     pk     (n, e),   sk   (n, d)
Page  537: RSA has the fastest signature verification algorithm among all the standardized signature schemes.
Page  537: This makes RSA very attractive for applications where a signature is generated o✏ine, but needs
Page  537: where fast verification is attractive. We discuss ways to speed-up the RSA signing procedure in
Page  537: Signature size. One downside of RSA is that the signatures are much longer than in other
Page  537: signature schemes, such as the ones presented in Chapter 19. To ensure that factoring the RSA
Page  537: RSA signatures are 256 bytes, which is considerably longer than in other schemes. This causes
Page  537: to be encoded as a two dimensional bar code on the stamp. RSA signatures were quickly ruled
Page  537: message. In particular, consider the unhashed RSA system where a signature on m 2 Zn is
Page  537: We can greatly strengthen the attack on this unhashed RSA using the random self-reducibility
Page  537: property of RSA (see Exercise 10.25). In particular, we show that an attacker can obtain the
Page  538: Let (n, e) be an RSA public key and let m 2 Zn be some message. As the reader should verify, we
Page  538: The attack shows that by fooling the user into signing a random message m̂ the adversary can
Page  538: obtain the signature on a message m of his choice. We say that unhashed RSA signatures are
Page  538: universally forgeable and thus should never be used.
Page  538: Security of RSA full domain hash. Recall that the security proof for the general full domain
Page  538: hash SFDH (Theorem 13.3) was very loose: an adversary A with advantage ✏ in attacking SFDH gives
Page  538: an adversary B with advantage ✏/(Qro + Qs + 1) in attacking the underlying trapdoor permutation.
Page  538: Can we do better? Indeed, we can: using the random self-reducibility property of RSA, we
Page  538: reduction for SRSA-FDH than for SFDH with a general trapdoor permutation. However, even for
Page  538: SRSA-FDH the reduction is not tight due to the Qs factor. We will address that later in Section 13.5.
Page  538: As in the proof of SFDH , our security proof for SRSA-FDH models the hash function H : M ! Y
Page  538: on the base of the natural logarithm e ⇡ 2.718 (not to be confused with the RSA public exponent
Page  538: Theorem 13.4. Let H : M ! Y be a hash function, where Y = {1, . . . , 22` 2 }. If the RSA
Page  538: assumption holds for (`, e), then SRSA-FDH with parameters (`, e) is a secure signature scheme,
Page  538: In particular, let A be an efficient adversary attacking SRSA-FDH in the random oracle version
Page  539: exists an efficient RSA adversary B as in Attack Game 10.3, where and B are elementary
Page  539: SIGro adv[A, SRSA-FDH ]  2.72 · (Qs + 1) · RSAadv[B, `, e]              (13.5)
Page  539: Consider the following, seemingly easier, problem: we give the adversary f (x1 ), . . . , f (xt ) and
Page  539: allow the adversary to request some, but not all, of the xi ’s. To win, the adversary must produce
Page  539: adversary A, the game runs as follows:
Page  539: and sends (y1 , . . . , yt ) to the adversary.
Page  539: • Eventually, A the adversary outputs (⌫, x), where ⌫ 2 {1, . . . , t} and x 2 X .
Page  539: Lemma 13.5. For every t-repeated one-way adversary A there exists a standard one-way adversary
Page  540: example, in the proof of Theorem 6.1. We want to use A to build an adversary B that breaks the
Page  540: will ultimately choose. Our adversary B then prepares values y1 , . . . , yt 2 Y as follows: for i 6= !,
Page  540: Proof. In more detail, our adversary B is given y⇤ := f (x⇤ ) for a random x⇤ 2 X , and then plays
Page  540: Moreover, it is clear that OWadv[B, f ] = Pr[W1 ]. Adversary B is really just playing Game 1 — it
Page  541: any adversary, the respective advantages in the corresponding attack games are equal. Technically,
Page  541: A tighter reduction for RSA. For a general one-way function f , the concrete bound in
Page  541: Lemma 13.5 is quite poor: if adversary A has advantage ✏ in winning the t-repeated one-way
Page  541: When f is derived from the RSA function we can obtain a tighter reduction using the random
Page  541: self-reducibility property of RSA. We replace the factor t by a factor of (about) Q, where Q is the
Page  541: We first restate Attack Game 13.3 as it applies to the RSA function. We slightly tweak the game
Page  541: RSA parameters ` and e, we set Y := {1, 2, . . . , 22` 2 } so that for all n generated by RSAGen(`, e),
Page  541: Attack Game 13.4 (t-repeated RSA). For given RSA parameters ` and e, a given positive
Page  541: integer t, and a given adversary A, the game runs as follows:
Page  541: (n, d) R RSAGen(`, e)
Page  541: • Eventually the adversary outputs (⌫, x), where ⌫ 2 {1, . . . , t} and x 2 Zn .
Page  541: A’s advantage, denoted rRSAadv[A, `, e, t], as the probability that A wins the game. 2
Page  541: We show that the t-repeated RSA problem is equivalent to the basic RSA problem, but with a
Page  541: Lemma 13.6. Let ` and e be RSA parameters. For every t-repeated RSA adversary A that makes
Page  541: at most Q reveal queries, there exists a standard RSA adversary B, where B is an elementary
Page  541: rRSAadv[A, `, e, t]  2.72 · (Q + 1) · RSAadv[B, `, e].                 (13.7)
Page  541: plugged position. Now, using the random self-reducibility property for RSA, we take the challenge
Page  542: Proof. We describe an adversary B that is given (n, e) and a random y⇤ 2 Zn , and then attempts
Page  542: So from now on, we assume y⇤ 2 Z⇤n . Adversary B uses A to compute an eth root of y⇤ as
Page  543: RSAadv[B, `, e]     Pr[W1 ].
Page  543: Indeed, when B’s input y⇤ lies in Z⇤n , adversary B is essentially just playing Game 1: the distributions
Page  543: adversary B always wins.
Page  543: Armed with Lemma 13.5, the proof of Theorem 13.3 is quite straightforward. Let A be an adversary
Page  543: attacking SFDH as in the theorem statement. Using A, we wish to construct an adversary B that
Page  544: to an adversary A0 that does, increasing the number of random oracle queries by at most Qs + 1.
Page  544: So from now on, let us work with the more convenient adversary A0 , which makes at most
Page  544: SFDH is the same as that of A. From A0 , we construct an adversary B 0 that wins the t-repeated
Page  544: Adversary B 0 works as follows. It obtains (y1 , . . . , yt ) from its own t-repeated one-way challenger.
Page  544: 13.5      An RSA-based signature scheme with tighter security proof
Page  544: Theorem 13.4 shows that SRSA-FDH is a secure signature scheme in the random oracle model, but
Page  544: with a relatively loose security reduction. In particular, let A be an adversary attacking SRSA-FDH
Page  544: that issues at most Qs signing queries and succeeds in breaking SRSA-FDH with probability ✏. Then
Page  544: A can be used to break the RSA assumption with probability about ✏/Qs . It is unlikely that
Page  544: SRSA-FDH has a tighter security reduction to the RSA assumption.
Page  544: Surprisingly, a small modification to SRSA-FDH gives a signature scheme that has a tight reduc-
Page  544: tion to the RSA assumption in the random oracle model. The only di↵erence is that instead of
Page  544: modified signature scheme SRSA-FDH    .
Page  544: We describe SRSA-FDH using the notation of Section 13.3.1. Let M0 := {0, 1} ⇥ M. We will
Page  544: The SRSA-FDH    signature scheme is defined as follows:
Page  544: • The key generation algorithm G uses fixed RSA parameters ` and e, and runs as follows:
Page  544: G() :=    k R K, (n, d) R RSAGen(`, e)
Page  545: Security. The SRSA-FDH       system can be shown to be secure under the RSA assumption, when H
Page  545: is modeled as a random oracle. The security proof uses the random self reduction of RSA to obtain
Page  545: a tight reduction to the RSA problem. The point is that the factor 2.72(Qs + 1) in Theorem 13.4
Page  545: Theorem 13.7. Let H : M0 ! Y be a hash function. Assume that the RSA assumption holds for
Page  545: (`, e), and F is a secure PRF. Then SRSA-FDH is a secure signature scheme when H is modeled as
Page  545: In particular, let A be an efficient adversary attacking SRSA-FDH   . Then there exist an efficient
Page  545: RSA adversary B and a PRF adversary BF , where B and BF are elementary wrappers around A,
Page  545: SIGro adv[A, SRSA-FDH  ]  2 · RSAadv[B, `, e] + PRFadv[F, BF ]
Page  545: uses an existential forger A to break the RSA assumption. Let (n, d) R RSAGen(`, e), x⇤ R Zn ,
Page  545: The solution comes from the extra bit in the signature. Recall that in SRSA-FDH      every message
Page  545: We will use the random self reduction of RSA to ensure that any such signature enables B to solve
Page  546: In e↵ect, B uses the random self reduction of RSA to map the original challenge y⇤ to a
Page  546: construction of O, the adversary learns no information about the function f . In particular, f (m)
Page  546: is a random bit, and is independent of the adversary’s view. Therefore, b 6= f (m) happens with
Page  546: So what does this mean? The SRSA-FDH   0         system is a minor modification of SRSA-FDH . Signa-
Page  546: tures include an additional bit which leads to a tighter reduction to the RSA assumption. Despite
Page  546: this tighter reduction, SRSA-FDH   has not gained much acceptance in practice. Most practitioners
Page  546: SRSA-FDH   is any more secure than SRSA-FDH for any particular instantiation of H. This is an open
Page  546: SRSA-FDH   is no less secure than SRSA-FDH .
Page  546: The most widely deployed standard for RSA signatures is known as PKCS1 version 1.5 mode 1.
Page  546: This RSA signing method is commonly used for signing X.509 certificates. Let n be an t-bit RSA
Page  547: Figure 13.3: PKCS1 signatures: the quantity D signed by RSA
Page  547: idea. Suppose PKCS1 directly signed a 256-bit message digest with RSA, without first expanding
Page  547: (⇤)     given an RSA modulus n, output a pair (y, x) where y is uniformly
Page  548: is far smaller than n2/3 (and for RSA we use e > 2).
Page  548: In summary, although PKCS1 v1.5 is a widely used standard for signing using RSA, we cannot
Page  548: prove it secure under the standard RSA assumption. An updated version of PKCS1 known as
Page  548: PKCS1 v2.1 includes an additional RSA-based signature method called PSS, discussed in the
Page  548: implementation of PKCS1 that illustrates its fragility. Let pk = (n, 3) be an RSA public key for
Page  548: the PKCS1 signature scheme: n is an t-bit RSA modulus and the signature verification exponent
Page  549: This attack applies to RSA public keys that use a small public exponent, such as e = 3. When
Page  551: confidentiality: an adversary who does not have Alice’s or Bob’s secret key cannot break semantic
Page  551: thenticity: an adversary who does not have Alice’s or Bob’s secret key cannot make Bob accept a
Page  551: In both games the adversary is active. In addition to asking Alice to encrypt messages intended
Page  551: for Bob, and asking Bob to decrypt messages supposedly coming from Alice, the adversary is free
Page  551: to ask Alice to encrypt messages intended for any other user of the adversary’s choosing, and to
Page  551: ask Bob to decrypt messages supposedly coming from any other user of the adversary’s choosing.
Page  551: may also be receiving messages from other users. Therefore, the adversary is free to ask Alice to
Page  551: decrypt messages supposedly coming from any other user of the adversary’s choosing. Similarly,
Page  551: modeling the fact that Bob may also be playing the role of sender, the adversary is free to ask Bob
Page  551: to encrypt messages intended for any other user of the adversary’s choosing.
Page  551: defined over (M, C, I), and a given adversary A, the attack game runs as follows:
Page  551: • The adversary chooses two distinct identities id S (the sender identity) and id R (the receiver
Page  552: SCI for short, if for every efficient adversary A, the value SCIadv[A, SC] is negligible.
Page  552: over (M, C, I), and for a given adversary A, we define two experiments.
Page  552: • The adversary chooses two distinct identities id S (the sender identity) and id R (the receiver
Page  553: • At the end of the game, the adversary outputs a bit b̂ 2 {0, 1}.
Page  553: against a chosen ciphertext attack, or simply CCA secure, if for all efficient adversaries A,
Page  553: the adversary to choose the user IDs of the sender and receiver (that is, id S and id R ), this choice
Page  553: would allow a more “adaptive” strategy, in which the adversary gets to choose these IDs after seeing
Page  553: arguably, honest users choose their IDs in a manner that is not so much under an adversary’s
Page  557: in Definition 13.3. Without this, the scheme can be vulnerable to a CCA attack: if an adversary,
Page  557: the adversary can win the CCA attack game by asking for a decryption of (c, 0 ). To prevent this,
Page  557: In particular, for every ciphertext integrity adversary Aci that attacks SC EtS as in Attack
Page  557: Game 13.5 there exists a strong signature adversary Bsig that attacks S as in Attack Game 13.2,
Page  557: In addition, for every CCA adversary Acca that attacks SC EtS as in Attack Game 13.6 there
Page  557: exists a CCA adversary Bcca that attacks E as in Definition 12.7, and a strong signature adver-
Page  557: Proving ciphertext integrity. We begin by constructing adversary Bsig that interacts with a
Page  558: SC EtS challenger and an adversary Acca . We then define Game 1, which is the same as Game 0, ex-
Page  558: an adversary Bsig 0  whose advantage in strong existential forgery game against S is equal to the
Page  558: We now construct an adversary Bcca whose CCA advantage is the same as Acca ’s advantage in
Page  558: Adversary Bcca first obtains an encryption public key pk ⇤ENC from its own challenger.
Page  558: Processing encryption queries. Adversary Bcca answers an S ! R encryption query for message
Page  559: Processing decryption queries. Consider first an S ! R decryption query (ĉ, ˆ ). Our adversary
Page  559: In particular, for every ciphertext integrity adversary Aci that attacks SC EtS as in Attack
Page  559: Game 13.5 there exists a signature adversary Bsig that attacks S as in Attack Game 13.1, and
Page  559: a CCA adversary Bcca that attacks E as in Definition 12.7, where Bsig and Bcca are elementary
Page  559: In addition, for every CCA adversary Acca that attacks SC EtS as in Attack Game 13.6 there
Page  559: exists a CCA adversary Bcca that attacks E as in Definition 12.7, where Bcca is an elementary
Page  560: Call this Game 1. Under CCA security for E, the adversary’s advantage in breaking CI in Game 0
Page  561: group of prime order q generated by g 2 G. For a given adversary A, the attack game runs as
Page  561: and gives (u, v) to the adversary.
Page  561: • The adversary makes a sequence of queries to the challenger. Each query is one of the following
Page  561: In either case, if equality holds the challenger sends “yes” to the adversary, and otherwise,
Page  561: sends “no” to the adversary.
Page  561: • Finally, the adversary outputs some ŵ 2 G.
Page  561: if for all efficient adversaries A the quantity I2 CDHadv[A, G] is negligible.
Page  561: In particular, for every ciphertext integrity adversary Aci that attacks SC DH as in the random
Page  561: oracle variant of Attack Game 13.5, there exists a ciphertext integrity adversary Bci that attacks
Page  561: E as in Attack Game 9.1, and an I2 CDH adversary Bdh for G, where Bci and Bdh are elementary
Page  561: In addition, for every CCA adversary Acca that attacks SC DH as in the random oracle variant
Page  561: of Attack Game 13.6, there exists a CCA adversary Bcca that attacks E as in Attack Game 9.2,
Page  561: and an I2 CDH adversary Bdh  for G, where Bcca and Biidh
Page  562: the adversary corrupts Alice and steals her secret key. Bob’s key remains intact and only known
Page  562: to Bob. One might reasonably expect that the adversary should not be able to decrypt c using
Page  562: tion forward secrecy. The goal is to ensure that CCA security is maintained even if the adversary
Page  562: giving the adversary the public keys pk S and pk R , the challenger gives the adversary the sender’s
Page  562: adversaries A, the value SCCA0 adv[A, SC] is negligible.
Page  562: need for a strongly secure signature scheme, if the adversary obtains a ciphertext (c, ) in response
Page  563: rules of the CCA attack game, the adversary would be free to submit (c, 0 ) as an S ! R decryption
Page  563: the adversary can use the sender’s signing key to generate a di↵erent signature on an inner S ! R
Page  563: allowing the adversary to submit a decryption query for the ciphertext (c, 0 ) in the attack game.
Page  563: adversary corresponds to the notion of gCCA security discussed in Exercise 12.2, and is actually
Page  563: secret key of the sender, the adversary can decrypt any ciphertext generated by the sender. Fortu-
Page  564: In particular, for every ciphertext integrity adversary Aci that attacks SC 0DH as in the random
Page  564: oracle variant of Attack Game 13.5, there exists a ciphertext integrity adversary Bci that attacks
Page  564: E as in Attack Game 9.1, and an I2 CDH adversary Bdh for G, where Bs and Bdh are elementary
Page  564: In addition, for every CCA adversary Acca that attacks SC DH as in the random oracle variant
Page  564: of Attack Game 13.6, there exists a 1CCA adversary B1cca that attacks E as in Definition 9.6,
Page  564: and an ICDH adversary Bdh0
Page  564: proof of CCA security with forward secrecy, where the adversary is given the sender’s secret key, is
Page  564: even if the adversary obtains the recipient’s secret key. The modified game is as follows:
Page  565: adversary the public keys pk S and pk R , the challenger gives the adversary the receiver’s secret key
Page  565: efficient adversaries A, the value SCI0 adv[A, SC] is negligible.
Page  566: Inc., who used its RSA signing key to sign the certificate using the PKCS1 standard with SHA256
Page  568: adversary to mount a man-in-the-middle attack on traffic to Facebook and eavesdrop on all traffic
Page  571: SCT from a trusted certificate log. This will e↵ectively force universal adoption of certificate
Page  575: (b) Show that every efficient adversary A that wins your multi-key attack game with probability ✏
Page  575: can be transformed into an efficient adversary B that wins Attack Game 13.1 with probability
Page  575: 13.4 (DSKS attack on RSA). Let us show show that SFDH is vulnerable to the DSKS attack
Page  575: message m. Then e = H(m) in Zn . Show that an adversary can efficiently come up with a
Page  576: problem in Z⇤p is easy. Show that by forming n0 as a product of two such primes, the adversary can
Page  576: adversary has to commit ahead of time to the message m for which it will forge a signature. Let
Page  576: adversary sending a message m 2 M to the challenger. The challenger runs (pk , sk ) R G() and
Page  576: sends pk to the adversary. The adversary then issues a sequence of signing queries m1 , . . . , mQ , as
Page  576: in Attack Game 13.1, where m 6= mi for all i = 1, . . . , Q. The adversary wins if it can produce
Page  576: a valid signature on m, and the scheme (G, S, V ) is selectively secure if no efficient adversary
Page  576: adversary has to commit to the message m before it even sees the public key pk .
Page  577: is existentially unforgeable. In particular, for every existential forgery adversary A against S 0 =
Page  577: (G, S 0 , V 0 ) there exists a selective forgery adversary B against S = (G, S, V ) such that
Page  577: 13.9 (FDH variant). Show that the signature scheme SRSA-FDH        (defined in Section 13.5) is no
Page  577: less secure than the signature scheme SRSA-FDH (defined in Section 13.3.1). You should show that
Page  577: if A is an adversary that succeeds with probability ✏ in breaking SRSA-FDH    (which has message
Page  577: space M), then there exists an adversary B (whose running time is roughly the same as that of
Page  577: A) that succeeds with probability ✏ in breaking SRSA-FDH (with message space M0 = {0, 1} ⇥ M).
Page  577: G() := {(n, d) R RSAGen(`, e), pk := (n, e), sk := (n, d), output (pk , sk )} ;
Page  577: Show that this signature is secure if the RSA assumption holds for (`, e), the quantity 1/|R| is
Page  577: negligible, and H is modeled as a random oracle. Moreover, the reduction to inverting RSA is
Page  577: Discussion: While SRSA-FDH   , from Section 13.5, also has a tight reduction, the construction here
Page  577: 13.11 (Batch RSA). Let us show how to speed up signature generation in SRSA-FDH .
Page  577: (c) Explain how to use parts (a) and (b) to speed up the SRSA-FDH signature algorithm. Specif-
Page  577: other under the public key (n, 5). This method generalizes to fast RSA signature generation
Page  578: cost of long signatures with RSA.
Page  578: 13.14 (Blind signatures). At the end of Section 13.3.1 we mentioned the RSA signatures can
Page  578: Let (n, d) R RSAGen(`, e) and set (n, e) as Bob’s RSA public key and (n, d) as his corresponding
Page  579: (a) We say that a blind signature protocol is secure if the adversary, given a public key and the
Page  579: (b) Show that the RSA blind signature is secure assuming the RSA assumption holds for (`, e),
Page  579: 13.15 (Threshold RSA signatures). In Exercise 11.17 we showed how a secret RSA decryption
Page  579: but a single share reveals nothing. In this exercise we show that the same can be done for RSA
Page  579: (b) Use Exercise 11.17 to construct a 2-out-of-3 threshold RSA signature scheme.
Page  579: iMessage [53]. Because every plaintext message m included a checksum (CRC), an adversary could
Page  580: the adversary is allowed to choose the sender and receiver user IDs adaptively, after seeing one or
Page  580: an adversary that makes at most one signing query in Attack Game 13.2 (we will see very
Page  580: that we only need security against 1-query adversaries means that it is possible to very efficiently
Page  581: the secret key, an adversary cannot lie about the value of the VRF at the point x.
Page  581: adversary. The adversary then makes a number of function queries and a single test query (with
Page  581: any number of function queries before and after the test query). In a function query, the adversary
Page  581: submits x 2 X and obtains (y, ⇡)       F (sk , x). In the test query, the adversary submits x̃ 2 X :
Page  581: the adversary cannot distinguish the two experiments.
Page  582: tion like RSA. In this chapter we return to more basic primitives, and construct signature schemes
Page  582: signatures, can be much faster to generate and verify than RSA signatures. An important feature
Page  582: of hash-based signatures is that, with suitable parameters, they are secure against an adversary
Page  582: who has access to a quantum computer. The RSA trapdoor permutation is insecure against such
Page  582: than RSA signatures. As such, they are well suited for applications like signing a software update
Page  582: efficient signature adversaries A that issue at most q signature queries, the value SIGadv[A, S]
Page  583: all efficient signature adversaries A that issue at most q signature queries, the value stSIGadv[A, S]
Page  583: If f is a one-way function then an adversary cannot recover x0 or x1 from the public-key pk .
Page  583: Hence, just given pk , the adversary cannot forge a signature on either one of the two messages
Page  583: in {0, 1}. Similarly, the signature on a message m 2 {0, 1} does not help the adversary forge a
Page  583: Proof. Let A be a one-time signature adversary that attacks S1bit . The adversary asks for the
Page  585: want the adversary to make at least 2128 evaluations of Hetcr to win the enhanced TCR game with
Page  585: These attacks show that a quantum adversary can win the enhanced TCR game for a v-bit hash
Page  585: Of course, if one is only concerned with classical adversaries then v = 130 is sufficient.
Page  586: Specifically, for what functions P is this a secure one-time signature scheme? The adversary sees
Page  586: adversary to find distinct messages m and m0 such that P (m) contains P (m0 ). For now we focus
Page  586: on functions where such containment is not possible, no matter how powerful the adversary is.
Page  586: In particular, suppose A is a signature adversary attacking SP that issues at most one signature
Page  586: query. Then there exist an efficient adversary Bf attacking the one-wayness of f , and a PRF
Page  586: adversary BF , where Bf and BF are elementary wrappers around A, such that
Page  586: defined in Section 13.4.1. We construct an adversary B that uses A to win the repeated one-way
Page  590: Security. For what functions P is this system a secure one-time signature? Suppose the adversary
Page  590: to forge signatures: given a signature on m the adversary can compute everything needed for a
Page  590: Hence, at a minimum we must insist that it be difficult for the adversary to find distinct
Page  590: adversary A, the attack game runs as follows:
Page  590: • The adversary chooses j 2 {1, . . . , d} and sends j to the challenger.
Page  590: • The adversary outputs x 2 X .
Page  590: We say A wins the game if f (x) = y. We define the adversary’s advantage iOWadv[A, f, d] to be
Page  590: iOWadv[A, f, d] is negligible for all efficient adversaries A.
Page  591: In particular, suppose A is a signature adversary attacking Swin that issues at most one signature
Page  591: query. Then there exist efficient adversaries Bf , BG , BH , where all three are elementary wrappers
Page  594: One-time signatures constructed from one-way functions can be much faster than RSA signatures.
Page  594: More precisely, let S1 = (G1 , S1 , V1 ) be a long-term signature system such as SRSA-FDH .
Page  595: single expensive RSA signature over many packets.
Page  595: Let S1 = (G1 , S1 , V1 ) be a long-term signature system such as SRSA-FDH . Let S1 = (G1 , S1 , V1 )
Page  596: Game 13.1) with one restriction — the adversary can issue up to q signature queries for messages
Page  596: (ui , mi ), but u1 , . . . , uq must all be distinct. In other words, once the adversary issues a signature
Page  596: query for (u, m) no other signature query can use the same u. As usual, the adversary wins this
Page  596: S is a secure q-indexed signature if for all efficient q-query signature adversaries A, the quantity
Page  599: In particular, suppose A is a Q-query signature adversary attacking SMerkle . Then there exist
Page  599: an efficient q-query adversary B and a PRF adversary BF , where B and BF are elementary
Page  599: system SMerkle contains q d independent instances of the Sq system. The adversary A issues at most
Page  599: We construct adversary B to break Sq using a basic “plug-and-pray” argument. B is given a Sq
Page  600: to attack. Adversary A issues signature queries m1 , . . . , mq for SMerkle . For the ith query mi ,
Page  600: adversary B picks a random leaf vi and assigns public-keys in pk 0 , . . . , pk ` 1 to internal nodes on
Page  601: because throughout the interaction with A, adversary B never generated a signature with index i
Page  601: same nonce N . That is, the system is existentially unforgeable, as long as the adversary does not
Page  602: are four times longer. In comparison, RSA signatures are far shorter, only 256 bytes per signature,
Page  602: then an efficient Merkle tree traversal algorithm can be used to quickly generate the Merkle
Page  603: of these keys, an adversary can forge the signature for one of the given public keys with
Page  603: probability 1/2. This gives the adversary advantage 1/2 in winning the multi-key security
Page  605: be an adversary that takes as input sets s1 , . . . , sq in Sets[n, `] and outputs a pair (r, x) 2 R ⇥ M
Page  605: the parameters n, ` so that a bounded adversary succeeds with only negligible probability.
Page  605: we want the adversary’s advantage in defeating the HORST 2-time signature to be at most
Page  606: (a) Show that an adversary that breaks the scheme can be used to find two di↵erent represen-
Page  606: Hint: Show that for any adversary, its advantage in breaking (G0 , S, V ) is identical to its
Page  606: m1 in Zq , the adversary can forge the signature on every message m 2 Zq of its choice.
Page  607: adversary A:
Page  607: riOWadv[A, f, t, d] be the probability that A wins the game. Prove that for every adversary A in
Page  607: this game there exists a (single instance) iterated one-way adversary B such that
Page  607: bounded d. In particular, an adversary that makes Qro queries to the random oracle has
Page  608: 14.18 (Merkle tree traversal). Consider a Merkle tree with q leaves, where q is a power of two.
Page  608: of that leaf. The Merkle tree traversal problem is to compute the q items
Page  608: sequentially one after the other. Show an algorithm for the Merkle tree traversal problem that runs
Page  608: Discussion: Merkle tree traversal can speed up the signing algorithm of the nonce-based q-indexed
Page  608: traversal algorithm. Better Merkle tree travesal algorithms [119, 29] run in worst-case time log2 q
Page  614: that wishes to use P256. They might worry that the seed was chosen adversarially so that the
Page  617: some b1 2 Fp such that V1 is a point on the curve E1 : y 2 = x3 + ax + b1 . Then, if the adversary
Page  617: (a) Suppose that |E1 (Fp )| is divisible by t. Show that the adversary can learn ↵ mod t, with
Page  617: (b) Use part (a) to show an efficient CCA adversary that learns the secret key ↵ with probability
Page  617: To simplify the analysis of your adversary’s success probability, you may model H : F4p ! K as a
Page  620: that interacts with the car. An adversary could eavesdrop on the radio channel and observe one
Page  620: or more conversations between the wireless key fob and the car. Nevertheless, this eavesdropping
Page  620: adversary should not be able to unlock the car itself.
Page  620: Using a fake ATM, the adversary can interact with Alice in an attempt to steal her credential, and
Page  620: later use the credential to authenticate as Alice. We call this an active adversary. We aim to design
Page  620: identification protocols that ensure that even this active adversary cannot succeed.
Page  620: password. As in the ATM example, an adversary can clone the bank’s web site and fool Alice into
Page  620: identifying herself to the adversary’s site. This attack, called phishing, is another example where
Page  620: the adversary can play an active role while interacting with Alice. The adversary tries to steal
Page  621: to the real bank. Again, we aim to ensure that even a phishing adversary cannot learn a working
Page  621: prover and a verifier that are in close physical proximity. Suppose that the adversary cannot
Page  621: eavesdrop on this conversation. Then using no information other than what is publicly avail-
Page  621: able, the adversary must somehow impersonate the prover to the verifier. A simple password
Page  621: • Eavesdropping attacks: In the wireless car key example the adversary can eavesdrop on
Page  621: adversary that interacts with the prover. The adversary uses the interaction to try and learn
Page  622: to prevent an adversary from impersonating Alice without Alice’s assistance. When defining the
Page  622: security of identification protocols, we may allow the adversary to eavesdrop and possibly interact
Page  622: with Alice; however, when it comes time to impersonate Alice, the adversary must do so without
Page  622: insecure channel: the adversary controls the channel and can block or inject messages at will. The
Page  622: adversary waits for Alice to run the identification protocol with her bank and relays all protocol
Page  622: adversary sends requests to the bank that appear to be originating from Alice’s computer. The
Page  622: bank honors these requests, thinking that they came from Alice. In e↵ect, the adversary uses Alice
Page  622: the adversary from injecting messages on behalf of Alice.
Page  624: sk is simple sk := pw . Clearly this protocol should only be used if the adversary cannot eavesdrop
Page  625: I = (G, P, V ) and a given adversary A, the attack game runs as follows:
Page  625: We say that the adversary wins the game if V outputs accept at the end of the interaction. We
Page  625: for all efficient adversaries A, the quantity ID1adv[A, I] is negligible.
Page  625: Note that the adversary in Attack Game 18.1 is given the verifier’s key vk . As a result, a
Page  625: Proof sketch. To attack the protocol the adversary must come up with a password pw 0 such that
Page  625: H(pw 0 ) = H(pw ). Note that pw 0 may be di↵erent from pw . An adversary who can come up with
Page  625: adversary can eavesdrop on just a single instance of the protocol.
Page  626: Suppose an adversary suspects that a certain user’s password is weak, and belongs to some small
Page  626: dictionary D of common passwords. Then the adversary can mount an online dictionary attack
Page  627: So, suppose an adversary manages to obtain a verification key vk = H(pw ) for some user. If
Page  627: adversary can mount an o✏ine dictionary attack, by performing the following computation:
Page  627: If pw belongs to D, then using this procedure the adversary will obtain pw , or possibly some pw 0
Page  627: The o✏ine dictionary attack discussed above can be made even better for the adversary by prepro-
Page  628: Recall that the password statistics cited in Section 18.3.1.2 suggest that an adversary can find
Page  629: attacks much harder for the adversary.
Page  629: With salts in place, the adversary has two strategies for attacking hashed passwords in a pass-
Page  630: the first. This should hold even if the adversary uses a time-space tradeo↵ to preprocess D ⇥ S. To
Page  630: usual inversion adversary A into two separate adversaries A0 and A1 . Adversary A0 has unbounded
Page  630: running time and implements the preprocessing phase. Adversary A1 is efficient and does the
Page  630: OWspro adv[A, H] of an adversary A = (A0 , A1 ) in defeating the one-wayness of H in the prepro-
Page  630: Note that the adversary A1 is given both L and the salt s. It needs to find a pre-image of y
Page  630: random oracle and where |D|  |Y|. Let A = (A0 , A1 ) be an adversary as in Definition 18.3, where
Page  631: We can make the adversary’s task harder by adding artificial entropy to human passwords. The
Page  631: hundredth of a second and is unnoticeable by the user. More importantly, the adversary has to do
Page  631: The secret salt makes an o✏ine dictionary attack harder because now the adversary has to
Page  632: However, the adversary’s work to hash all words in the dictionary is increased by a factor of 10,000.
Page  633: evaluations per second, then the adversary can try 1012 passwords per second per chip. Even if
Page  634: tradeo↵ does not help the adversary. It lets the adversary pack ↵ times as many Scrypt engines
Page  634: to use. This again would enable the adversary to pack many Scrypt engines into a single chip, all
Page  636: with Scrypt. Suppose the adversary gains low-privilege access to this server; the adversary can
Page  636: Now, suppose the adversary captures a hash value y which is the result of applying the Scrypt
Page  636: PBKDF in (18.8) to some password pw with a public salt. Normally the adversary would need to
Page  636: if the adversary also has the memory access pattern of process P as it was computing the Scrypt
Page  636: hash of pw , then the adversary can mount a dictionary attack on pw with very little memory.
Page  636: int(y0 ) mod (d + 1). By observing P ’s accesses to memory, the adversary can see what memory
Page  636: page was read when Step (5) was first executed. This gives the adversary an approximate value ja
Page  636: for j. The adversary does not learn the exact value of j because a single memory page may contain
Page  636: If the test fails then pw 0 is not the correct password. This procedure lets the adversary discard most
Page  637: method and the slow hashing method increase the adversary’s work load. One should use one
Page  637: at multiple web sites. Ideally, all of these servers take proper precaution to prevent an adversary
Page  637: should the adversary obtain this file. Unfortunately, the designers of low-security servers (e.g., a
Page  637: hashes in the clear, and the adversary retrieves all the passwords, even strong ones. Consequently,
Page  637: an adversary can break in to a low-security server and retrieve some, or even all, user ID/passwords
Page  638: challenger                                           adversary A
Page  638: The password protocols in the previous section are easily compromised if an adversary can eavesdrop
Page  638: phase” in which the adversary is allowed to request a number of transcripts of the interaction
Page  638: protocol I = (G, P, V ) and a given adversary A, the attack game runs as follows:
Page  638: • Eavesdropping phase. The adversary requests some number, say Q, of transcripts of conver-
Page  639: The challenger sends these transcripts T1 , . . . , TQ to the adversary.
Page  639: We say that the adversary wins the game if the verification protocol V outputs accept at the end of
Page  639: attacks if for all efficient adversaries A, the quantity ID2adv[A, I] is negligible.
Page  639: Keeping vk secret. The adversary in Attack Game 18.2 is given the verification key vk , meaning
Page  639: Game 18.2 where the challenger does not send vk to the adversary. A small complication when vk
Page  639: is kept secret is that we must now allow the adversary to make multiple impersonation attempts.
Page  639: concurrently. In this chapter, we shall insist that they proceed sequentially. The adversary wins
Page  639: adversary could emulate the verifier itself.
Page  639: let wID2adv[A, I] denote the adversary’s advantage in winning this weaker version of Attack
Page  639: ping attacks if for all efficient adversaries A, the quantity wID2adv[A, I] is negligible.
Page  639: we allow the adversary to eavesdrop on several conversations between P and V . Also, we allow
Page  639: allow the adversary to make several impersonation attempts (although, if vk is not kept secret,
Page  639: the stateless case, we could assume without loss of generality that the adversary obtained all of the
Page  639: longer the case, and we have to allow the adversary to interleave eavesdropping and impersonation
Page  640: attempts. That is, the attack game proceeds in rounds. In each round the adversary can choose to
Page  640: attempts succeeds (in which case the adversary wins the game). Recall that we are assuming that
Page  640: Proof sketch. Since F is a secure PRF, the adversary cannot distinguish between a challenger who
Page  641: (a) RSA SecurID token                                (b) Google authenticator
Page  642: TOTP requires that the verification key vk stored on the server remain secret. If an adversary
Page  643: Proof sketch. Since vk is public, we can assume that the adversary eavesdrops on, say, Q conversa-
Page  643: generate Q valid conversations with respect to the initial verification key vk = H (n+1) (k). If our
Page  643: guess for Q is correct, and the adversary succeeds in its impersonation attempt, the adversary will
Page  643: find for us a pre-image of y. Thus, if the adversary impersonates with probability ✏, we win Attack
Page  643: We now consider a more powerful attack in which the adversary actively impersonates a legitimate
Page  643: verifier. For example, the adversary may clone a banking site and wait for a user (i.e., prover)
Page  643: to visit the site and run the ID protocol with the adversary. As a result, the adversary gets to
Page  643: adversary’s goal is to gain information about the prover’s key sk . After several such interactions,
Page  643: the adversary turns around and attempts to authenticate as the prover to a legitimate verifier.
Page  643: We say that the ID protocol is secure against active attacks if the adversary still cannot fool the
Page  643: active attacks. By impersonating a verifier, the adversary will learn a fresh one-time password
Page  644: challenger                                           adversary A
Page  644: from the prover that the adversary can then use to authenticate to the verifier. In fact, a moments
Page  644: col I = (G, P, V ) and a given adversary A, the attack game, shown in Fig. 18.10, runs as follows:
Page  644: • Active probing phase. The adversary requests to interact with the prover. The challenger
Page  644: complies by interacting with the adversary in an ID protocol with the challenger playing
Page  644: the role of the prover by running algorithm P initialized with sk . The adversary plays
Page  644: the role of verifier, but not necessarily following the verifier’s algorithm V . The adversary
Page  644: We say that the adversary wins the game if the verification protocol V outputs accept at the end of
Page  644: efficient adversaries A, the quantity ID3adv[A, I] is negligible.
Page  645: we allow the adversary to interact concurrently with many instances of the prover. One could
Page  645: vk to the adversary. Just as in Section 18.5, if vk is kept secret, then we must now allow the
Page  645: adversary to interact with the verifier, since such interactions could potentially leak information
Page  645: about vk . Therefore, in the active probing phase, we allow the adversary to interact concurrently
Page  645: the verifier, the adversary learns if the verifier outputs accept or reject. In addition, during the
Page  645: impersonation attempt, we let the adversary interact concurrently with several verifiers, and the
Page  645: adversary wins the game if at least one of these verifiers accepts.
Page  645: We let wID3adv[A, I] denote the adversary’s advantage in winning this weaker version of Attack
Page  645: attacks if for all efficient adversaries A, the quantity wID3adv[A, I] is negligible.
Page  646: the probability that the adversary receives a challenge message that it has seen before (in a previous
Page  646: interaction with the prover) is negligible. So either that unlikely event happens, or the adversary
Page  646: attack. After eavesdropping on a single conversation (c, t) between prover and verifier, the adversary
Page  647: adversary must forge a signature, rather than a MAC. 2
Page  649: point pwf . The the traversal on line (4) would then give an inverse of y. The total running time to
Page  652: (a) Security against direct attacks is defined using an attack game where the adversary is given
Page  652: 18.2 (An attack on PBKDF2). Let pw 2 P be a password. Suppose the adversary obtains a
Page  652: for some d. Show that the adversary can recover pw in time O(|P|), independent of the difficulty d.
Page  652: oracle. We say that the PBKDF is secure if no adversary that makes at most d 1 queries to h
Page  652: • The adversary A sends to the challenger a positive difficulty d 2 Z. The challenger chooses a
Page  652: • The adversary then issues a sequence of queries, where for i = 1, 2, . . . query i is one of:
Page  652: – an Hh query: the adversary sends pw i 2 P. In response, the challenger chooses salt i R S
Page  652: sends (yi , salt i ) to the adversary.
Page  652: – an h query: the adversary sends xi 2 X and gets back h(xi ).
Page  652: • Finally, the adversary A outputs a bit b̂ 2 {0, 1}.
Page  652: respect to Hh as Pr[W0 ] Pr[W1 ] . We say that Hh is a secure PBKDF if no adversary that
Page  653: password y, this B lets the adversary quickly discard about half the password candidates in the
Page  653: More discussion: A more complete definition would allow the adversary A to preprocess the
Page  653: phase should not improve the adversary’s advantage by more than a negligible amount.
Page  653: in Exercise 18.3 so that the adversary can specify an arbitrary difficulty d for every Hh query. That
Page  654: let N := |X |. For a given `, construct an adversary A = (A0 , A1 ) where A0 preprocesses ⇡ and
Page  654: outputs an advice string L containing ` elements of X . Then for y := ⇡(x), where x R X , adversary
Page  654: n R RSAGen(`, e) is an RSA modulus treated as a system parameter.
Page  654: explains why when vk is kept secret, it is necessary to allow the adversary in Attack Game 18.2
Page  654: secure against eavesdropping (and even secure against active attacks) if the adversary can only
Page  654: adversary can make two impersonation attempts.
Page  654: It should be clear that if the adversary can make two impersonation attacks then the protocol
Page  655: when vk is kept secret, it is necessary to allow an active adversary in Attack Game 18.3 to interact
Page  655: with the verifier during the probing phase. We describe a protocol that is secure if the adversary
Page  655: (a) Show that this ID protocol is (weakly) secure against an active adversary playing Attack
Page  655: Game 18.9 where the adversary cannot interact the verifier during the probing phase.
Page  655: (b) Show that the protocol is insecure against an active adversary playing Attack Game 18.9
Page  655: where the adversary can interact the verifier.
Page  659: An interaction between P (↵) and V (u) generates a conversation (ut , c, ↵z ) 2 G ⇥ C ⇥ Zq . We
Page  659: call such a conversation an accepting conversation for u if V ’s check passes, i.e., if g ↵z = ut · uc .
Page  659: It is easy to see that an interaction between P and V always generates an accepting conversation,
Page  660: that any efficient adversary that can succeed in a direct impersonation attack with non-negligible
Page  660: In particular, suppose A is an efficient impersonation adversary attacking Isch via a direct attack
Page  660: adversary B (whose running time is about twice that of A), with advantage ✏0 := DLadv[B, G],
Page  660: challenger generates the verification key u = g ↵ . In his impersonation attempt, the adversary A
Page  660: generates the first flow ut of the protocol using some arbitrary adversarial strategy. Now, to succeed,
Page  660: If all of this happens, then we obtain two accepting conversations (ut , c, ↵z ) and (ut , c0 , ↵z0 ) for a
Page  660: we can easily compute Dlogg u. Indeed, since both conversations are accepting, we have the two
Page  660: from two accepting conversations is essentially the same idea as was used in Fact 10.3. Indeed,
Page  661: so far in this text. Indeed, in the proof of this theorem, while we show that every adversary A
Page  661: that breaks Isch can be converted into an adversary B that breaks the discrete logarithm problem,
Page  661: the adversary B that we construct is not an elementary wrapper around A. Adversary B has
Page  662: Proof of Theorem 19.1. Using the impersonation adversary A, which has advantage ✏, we build a
Page  662: DL adversary B, with advantage ✏0 , as follows. Adversary B is given an instance u = g ↵ of the
Page  662: as the verification key. The goal of B in this step is to compute two accepting conversations for u
Page  662: function f in the lemma is defined to be 1 if the resulting conversation is an accepting conversation
Page  662: for u, and 0 otherwise. So f (X, Y) = 1 if (ut , c, ↵z ) is an accepting conversation for u, and f (X, Y0 ) = 1
Page  662: if (ut , c0 , ↵z0 ) is an accepting conversation for u. Applying the lemma, we find that the probability
Page  662: that B gets two accepting conversations with di↵erent challenges is at least ✏2 ✏/N .
Page  663: So now assume that B has successfully computed two such conversations (ut , c, ↵z ) and
Page  663: (ut , c0 , ↵z0 ). In the second stage of its computation, B uses these two conversations to compute
Page  663: conversations (ut , c, ↵z ) and (ut , c0 , ↵z0 ) where c 6= c0 . Rewinding the prover A is possible inside the
Page  663: is secure against eavesdropping attacks as well. Now, in an eavesdropping attack, the adversary
Page  663: obtains vk and a list of transcripts — conversations between P (on input sk ) and V (on input
Page  663: vk ). The idea is to show that these conversations do not help the adversary, because the adversary
Page  663: could have efficiently generated these conversations by himself, given vk (but not sk ). If we can
Page  663: show this, then we are done. Indeed, suppose A is an adversary whose advantage in carrying out
Page  663: another adversary B, that works the same as A, except that B generates the transcripts by himself,
Page  663: vk is identical to the distribution of a transcript of a conversation between P (on input sk ) and V
Page  663: that an adversary learns nothing from P , because an adversary can simulate conversations on his
Page  663: this simulation only works for conversations between P and the actual, “honest” verifier V , and
Page  664: Direct adversary B
Page  664: Direct Challenger                                                         Adversary A
Page  664: Figure 19.2: Adversary B in the proof of Theorem 19.3.
Page  664: In particular, if I is HVZK with simulator Sim, then for every impersonation adversary A that
Page  664: there is an adversary B that attacks I via a direct attack, as in Attack Game 18.1, where B is
Page  664: generates the transcripts itself using Sim. Adversary B is shown in Fig. 19.2. 2
Page  664: Proof. The idea is that in generating a simulated conversation (ut , c, ↵z ), we do not need to generate
Page  664: the messages of the conversation in the given order, as in a real conversation between P and V .
Page  664: and outputs the conversation (ut , c, ↵z ).
Page  665: In particular, for every impersonation adversary A that attacks Isch via an eavesdropping attack,
Page  665: as in Attack Game 18.2, there is an adversary B that attacks Isch via a direct attack, as in Attack
Page  665: vk , and yet be easy to generate a conversation, also knowing only vk ? The answer is that in
Page  665: carrying out an impersonation attack, the verifier V is actively involved in the conversation, and
Page  665: the timing and ordering of the messages is critical: the adversary (playing the role of a prover)
Page  665: small: in its impersonation attempt, an adversary could use the simulator to prepare an accepting
Page  665: conversation (ut , c, ↵z ), send ut to V , and then hope that the challenge chosen by V is equal to its
Page  665: prepared challenge c, and if so, the adversary could then respond with ↵z , and so make V accept.
Page  665: where (ut , c, ↵z ) is an accepting conversation for the verification key u in Schnorr’s identification
Page  666: in which we allow the adversary to make many impersonation attempts (against several instances of
Page  666: tocol I = (G, P, V ), positive integer r, and adversary A, the attack game runs as follows. The key
Page  666: The only di↵erence is that in the impersonation phase, the adversary A is allowed to interact
Page  666: which use the same verification key as generated during the key generation phase. The adversary
Page  667: versary A, there exists a standard eavesdropping adversary B, where B is an elementary wrapper
Page  667: Proof sketch. The is a simple “guessing argument”. Adversary B simply chooses ! 2 {1, . . . , r}
Page  667: challenger the verification key as well transcripts of several conversations, and passes these along
Page  667: to A. During the impersonation phase, for the jth instance of the verifier, if j 6= !, our adversary
Page  667: In particular, let A be an adversary attacking Ssch as in the random oracle version of Attack
Page  667: queries. Then there exists a (Qro + 1)-impersonation adversary B that attacks Isch via an
Page  667: Proof idea. The goal is to convert an adversary A that forges a signature into an adversary B that
Page  667: key. This is done by using the transcripts from eavesdropped conversations to build the required
Page  667: Once we have gotten rid of the signing queries, we argue that if the adversary successfully
Page  668: was first queried explicitly by the adversary, rather than (implicitly) by the signing algorithm. The
Page  668: Suppose that the adversary submits (m, ut , ↵z ) as its forgery attempt, and that m is di↵erent
Page  668: from all the mi ’s submitted as signing queries. By our by our simplifying assumption, the adversary
Page  668: by the adversary, or indirectly via a previous signing query) is at most (Qs + Qro + 1)/q. Another
Page  668: At this point, it is easy to construct an adversary B that plays the r-impersonation eavesdrop-
Page  670: obtain eavesdropped conversations (uti , ci , ↵zi ) for i = 1, . . . , Qs from challenger
Page  670: Figure 19.4: Adversary B
Page  671: Let A be an efficient adversary attacking Ssch as in the random oracle version of Attack
Page  671: queries. Then there exists an efficient DL adversary B (whose running time is about twice that
Page  671: impersonation eavesdropping adversary A attacking Isch , with advantage ✏ := rID2adv[A, I, r], there
Page  671: exists an efficient DL adversary B (whose running time is about twice that of A), with advantage
Page  671: challenger gives to A several transcripts of conversations. Third, A enters the impersonation phase,
Page  671: that (uti , ci , ↵z ) is an accepting conversation for the verification
Page  671: Note that we have assumed a somewhat simplified behavior for the adversary in the imperson-
Page  671: ation phase. However, since the adversary can see for himself whether a conversation is accepting
Page  671: or not, this is not really a restriction: any adversary can be put in the form described without
Page  671: r-impersonation adversary constructed in the proof of Theorem 19.7 is already essentially of this
Page  671: We now describe our DL adversary B, which is given u 2 G, and is tasked to compute Dlogg u.
Page  671: B generates transcripts of conversations, using the simulator from Theorem 19.4, and gives these
Page  671: challenges c1 , . . . , cr . If A outputs a pair (i, ↵z ) such that (uti , ci , ↵z ) is an accepting conversation
Page  672: verifier. Instead of the challenge ci , our adversary B responds with a fresh, random challenge c0 2 C.
Page  672: conversation, and c0 6= ci , then B uses these two accepting conversations to compute Dlogg u, just
Page  672: accepting conversation; similarly, f (X, Y0 ) = 1 if i0 = j and (utj , c0 , ↵z0 ) is an accepting conversation.
Page  675: strictly as a function of the statement y and the conversation (t, c, z). In particular, V
Page  675: the statement y and its conversation (t, c, z) with the prover. If the output is accept we call the
Page  675: conversation (t, c, z) an accepting conversation for y. Of course, interactions between the verifier
Page  675: and an honest prover only produce accepting conversations; non-accepting conversation can arise,
Page  676: ↵ 2 Zq such that g ↵ = u. Thus, every statement has a unique witness. An accepting conversation
Page  676: with two accepting conversations (t, c, z) and (t, c0 , z 0 ) for y, where c 6= c0 , algorithm Ext always
Page  676: with two accepting conversations (ut , c, ↵z ) and (ut , c0 , ↵z0 ) for u, with c 6= c0 . Just as we did in
Page  676: conversations as ↵/ c 2 Zq , where ↵ := ↵z ↵z0 and c := c c0 . 2
Page  676: accepting conversations (t, c, z) and (t, c0 , z 0 ) for y, with c 6= c0 , and then use the witness extractor
Page  677: (x, y) 2 R, a conversation between P (x, y) and V (y) should not reveal anything about the witness
Page  677: conversations between P (x, y) and V (y) without knowing the witness x. However, we will add a
Page  677: an accepting conversation for y;
Page  677: then (t, c, z) has the same distribution as that of a transcript of a conversation between P (x, y)
Page  677: accepting conversation even when the statement y does not have a witness. These two properties
Page  679: accepting conversation for u 2 G is of the form
Page  679: versation. This is easy to verify, since if
Page  679: Suppose we have two accepting conversations
Page  679: from these two conversations. The computation here is very similar to that in Schnorr’s protocol.
Page  679: and outputs (ut , (↵z , z )). Observe that the output always yields an accepting conversation, as
Page  679: the right distribution. The key observation is that in a real conversation, c, ↵z , and z are mutually
Page  680: versation for (u, v, w) 2 G3 is of the form
Page  680: verifier always produces an accepting conversation.
Page  680: Knowledge soundness. Suppose we have two accepting conversations
Page  681: and outputs ((vt , wt ), z ). Observe that the output always yields an accepting conversation, as
Page  681: ((u, v, w), c) has the right distribution. The key observation is that in a real conversation, c and
Page  683: 19.5.4     A Sigma protocol for RSA
Page  683: present one related to RSA.
Page  683: Let (n, e) be an RSA public key, where e is a prime number. We will view (n, e) as a system
Page  683: A witness for a statement y 2 Z⇤n is x 2 Z⇤n such that xe = y. Since (n, e) is an RSA public key,
Page  683: Proof. An accepting conversation for y is of the form (xt , c, xz ), where xez = yt · y c . The reader
Page  683: an honest verifier always produces an accepting conversation.
Page  683: we have two accepting conversations (xt , c, xz ) and (xt , c0 , x0z ) for the statement y, where c 6= c0 . We
Page  684: The reader should observe that the technique presented here for computing an RSA inverse from
Page  684: two accepting conversations is essentially the same idea that was used in the proof of Theorem 10.7.
Page  684: Indeed, the two accepting conversations yield a collision ((xz , c mod e), (x0z , c0 mod e)) on the
Page  684: hash function Hrsa (a, b) := ae y b .
Page  684: and outputs (yt , xz ). The key observation is that in a real conversation, c and xz are independent,
Page  685: R ✓ X ⇥ Y. For a given adversary A, the attack game runs as follows:
Page  685: We say that the adversary wins the game if (x̂, y) 2 R. We define A’s advantage with respect to
Page  685: Example 19.5. Consider the GQ protocol in Section 19.5.4. Recall that the RSA public key
Page  685: algorithm is one-way under the RSA assumption (see Theorem 10.5). 2
Page  685: In particular, suppose A is an efficient impersonation adversary attacking I via a direct attack
Page  685: adversary B attacking G as in Attack Game 19.2 (whose running time is about twice that of
Page  685: Proof. We can just mimic the proof of Theorem 19.1. Using the impersonation adversary A, we
Page  685: build an adversary B that breaks the one-wayness of G, as follows. Adversary B is given a public
Page  685: with probability at least ✏2 ✏/N , adversary B obtains two accepting conversations (t, c, z) and
Page  686: conversations with probability at least ✏2 ✏/N .
Page  686: In the second stage of the computation, B feeds these two conversations into a witness extractor
Page  686: In particular, for every impersonation adversary A that attacks I via an eavesdropping attack,
Page  686: as in Attack Game 18.2, there is an adversary B that attacks I via a direct attack on, as in
Page  686: eavesdropping attacks under the RSA assumption (provided the challenge space is large). 2
Page  686: • a Sigma protocol (P, V ) for a relation R ✓ X ⇥ Y; we assume that conversations are of the
Page  687: conversation for y.
Page  687: and suppose that all conversations (t, c, z) lie in T ⇥C ⇥Z. We say that (P, V ) has -unpredictable
Page  687: P (x, y) and V (y) produces a conversation (t, c, z) with t = t̂. We say that (P, V ) has unpredictable
Page  687: In particular, let A be an adversary attacking S as in the random oracle version of Attack
Page  687: impersonation adversary B that attacks I via an eavesdropping attack as in Attack Game 19.1,
Page  687: (G, P, V ). Suppose A is an efficient r-impersonation eavesdropping adversary attacking I, as in
Page  687: Attack Game 19.1, with advantage ✏ := rID2adv[A, I, r]. Then there exists an efficient adversary B
Page  688: Let A be an efficient adversary attacking S as in the random oracle version of Attack Game 13.1.
Page  688: there exists an efficient adversary B attacking G as in Attack Game 19.2 (whose running time
Page  688: gives us a new signature scheme based on RSA. The scheme makes use of an RSA public key (n, e)
Page  688: under the RSA assumption (provided the challenge space is large). Also, we observe that the GQ
Page  688: corresponding signature scheme is secure in the random oracle model, under the RSA assumption.
Page  688: The advantage of GQ signatures over RSA signatures, such as SRSA-FDH , is that the signing
Page  688: algorithm is much faster. Signing with SRSA-FDH requires a large exponantiation. Signing with GQ
Page  689: 4. V checks that (t0 , c, z0 ) is an accepting conversation for y0 and that (t1 , c, z1 ) is an accepting
Page  689: conversation for y1 .
Page  691: 4. V computes c1            c c0 , and checks that (t0 , c0 , z0 ) is an accepting conversation for y0 , and
Page  691: that (t1 , c1 , z1 ) is an accepting conversation for y1 .
Page  691: the extractor Ext for (P, V ) takes as input (y0 , y1 ) and a pair of accepting conversations
Page  691: accepting conversation. This is one of the main reasons for this aspect of the definition. 2
Page  691: either the DL or RSA assumptions (and without relying on the random oracle heuristic).
Page  692: Y. For a given adversary A, we define an experiment (x, y) for each (x, y) 2 R. Experiment (x, y)
Page  692: • Initially, the adversary is given the value y.
Page  692: • The adversary then interacts with several instances of the prover P (x, y) — in each of these
Page  692: interactions, the challenger carries out the provers’ computations, while the adversary plays
Page  692: tions may be concurrent (in particular, the adversary may issue challenges that depend on
Page  692: • At the end of the game, the adversary outputs some value s, which belongs to a finite output
Page  692: witness independent if for every adversary A, for every y 2 Y, for every x, x0 2 X such that
Page  692: This definition captures in a very strong sense the idea that the adversary’s behavior depends
Page  692: and sk = (x, y), and then run Experiment (x, y) in Attack Game 19.3 with an adversary A. Let us
Page  692: • S represents the adversary’s output s 2 S.
Page  693: Proof. Let (P, V ) be a Sigma protocol for R ✓ X ⇥ Y. Suppose that all conversations (t, c, z) lie in
Page  693: Consider the probability that a real conversation between P (x, y) and V (y) produces a particular
Page  693: conversation (t, c, z). This is precisely
Page  693: outputs (t, z). The probability that the conversation produced by running the simulator on a
Page  693: random challenge is equal to a particular conversation (t, c, z) is precisely
Page  693: Now consider Experiment (x, y) of Attack Game 19.3, and assume that the adversary A interacts
Page  694: adversary. The adversary’s logic can be characterized by a function 0 that maps (y, t⇤ , z ⇤ , coins 0 )
Page  694: to (c⇤ , s). Here, (t⇤ , c⇤ , z ⇤ ) 2 T Q ⇥ C Q ⇥ Z Q , s 2 S is the adversary’s output, and coins 0 denotes
Page  694: the particular random choices made by the adversary.
Page  695: In particular, suppose A is an impersonation adversary attacking I 0 via an active attack as in
Page  695: Attack Game 18.3, with advantage ✏ := ID3adv[A, I 0 ]. Then there exists an efficient adversary
Page  695: key pk 0 = Y and a secret key sk 0 = (X, Y ), and sends pk 0 to the adversary A.
Page  695: Active probing phase. The adversary interacts with the prover P 0 (sk 0 ). Here, the challenger
Page  695: plays the role of the prover, while the adversary plays the role of a possibly “cheating” verifier.
Page  695: The adversary may interact concurrently with many instances of the prover.
Page  695: Impersonation attempt. As in a direct attack, the adversary now interacts with the verifier
Page  695: adversary plays the role of a possibly “cheating” prover. In this phase, the adversary (acting as
Page  695: challenge. The adversary wins the game if its response to the random challenge yields an accepting
Page  695: conversation.
Page  695: We now describe our adversary B for breaking the one-wayness assumption for G. To start
Page  695: Our adversary B begins by playing the role of challenger to A, running A once through all three
Page  696: this results in two accepting conversations with distinct challenges, then by knowledge soundness,
Page  696: RSA assumption. These two actively secure protocols are roughly twice as expensive (in terms of
Page  697: In particular, suppose A is an impersonation adversary attacking IO via an active attack as in
Page  697: Attack Game 18.3, with advantage ✏ := ID3adv[A, IO ]. Then there exists an efficient adversary
Page  697: Suppose A has advantage ✏ in attacking IO in Attack Game 18.3. Our DL adversary B receives
Page  697: Our adversary B begins by playing the role of challenger to A, running A once through all three
Page  697: phases of Attack Game 18.3. Our adversary B uses the group element h as the system parameter for
Page  697: two accepting conversations with distinct challenges, then by knowledge soundness, B can extract a
Page  697: Our adversary B succeeds if it extracts a witness from A that is di↵erent from (↵, ). By
Page  698: m1 it choose ↵t1 as ↵t1     a · ↵t0 + b for some known a, b 2 Zq . Show that if the adversary obtains
Page  698: • For i = 1, . . . , k, the verifier V k verifies’ that (ti , ci , zi ) is an accepting conversation for y.
Page  699: the RSA assumption.
Page  699: 19.7 (Okamato’s RSA-based Sigma protocol). Okamoto’s protocol (see Section 19.5.1) is
Page  699: the RSA problem. By way of analogy, Okamoto’s DL-based protocol was a “proof of knowledge”
Page  700: of a pre-image of the hash function Hdl in Section 10.6.1, and Okamato’s RSA-based protocol is a
Page  700: “proof of knowledge” of a pre-image of the hash function Hrsa in Section 10.6.2.
Page  700: The setup is as follows. Let (n, e) be an RSA public key, where the encryption exponent e is a
Page  700: if for every pair of accepting conversations (t, c, z) and (t, c, z 0 ), for any statement y, we must have
Page  701: witness extractor algorithm Ext that on input y 2 Y, along with any two accepting conversations
Page  701: More precisely, for a given adversary A, we define cSKSadv[A, ⇧, Ext] to be the probability
Page  701: that A outputs two accepting conversations (t, c, z) and (t, c0 , z 0 ) with (c, z) 6= (c0 , z 0 ), such that
Page  701: adversary A, the value cSKSadv[A, ⇧, Ext] is negligible.
Page  701: an adversary that can find two accepting conversations for some statement with di↵erent
Page  701: (b) Prove that Okamoto’s RSA-based protocol (see Exercise 19.7) provides computational strong
Page  701: knowledge soundness, under the RSA assumption. You should show that an adversary that
Page  701: can find two accepting conversations for some statement with di↵erent responses, but with
Page  702: conversations (t, c, z) lie in the set T ⇥C ⇥Z, then for every (y, c, z) 2 Y ⇥C ⇥Z, there exists a unique
Page  702: t 2 T such that (t, c, z) is an accepting conversation for y; moreover, the function f mapping (y, c, z)
Page  702: the form (t, z), where (t, c, z) 2 T ⇥ C ⇥ Z is an accepting conversation, and c := H(m, t).
Page  702: Note: For both parts, you should show that any adversary that breaks the optimized scheme can
Page  703: applied to Schnorr’s protocol. The hash function Hrsa in Section 10.6.2 can be viewed as a special
Page  703: adversary A to win the following game:
Page  703: (d) Consider Okamoto’s RSA-based Sigma protocol (P, V ) in Exercise 19.7. Define the key gen-
Page  703: u R ay y b . Show that G is second-preimage resistant (under the RSA assumption) and type
Page  703: (G, P, V ) is secure against active attacks, under the RSA assumption.
Page  704: interacts with an adversary A as in Experiment (x, y) of Attack Game 19.3, at the end of which
Page  704: the adversary outputs a bit b̂ 2 {0, 1}.
Page  704: active attacks, in the sense that any impersonation adversary that breaks the security of
Page  704: (d) Consider Okamoto’s RSA-based identification protocol (G, P, V ) in part (d) of Exercise 19.16.
Page  704: Sigma protocol for a relation R ✓ X ⇥ Y, and that (P, V ) has conversations in T ⇥ C ⇥ Z. Let G0
Page  705: verification algorithm checks that (t, c, z) is an accepting conversation for y.
Page  705: then (t, c, z) has the same distribution as that of a transcript of a conversation between P (x, y) and
Page  706: (t` , c` , z` ) is an accepting conversation for y` .
Page  709: X ⇥ Y. For a given adversary A, the attack game runs as follows:
Page  709: • The adversary chooses a statement y 2 Y and gives this to the challenger.
Page  709: • The adversary now interacts with the verifier V (y), where the challenger plays the role of
Page  709: verifier and the adversary plays the role of a possibly “cheating” prover.
Page  709: We say that the adversary wins the game if V (y) outputs accept but y 2   / LR . We define A’s
Page  709: Definition 20.2. We say that ⇧ is existentially sound if for all efficient adversaries A, the
Page  709: In particular, for every adversary A, we have
Page  709: conversation (t, c, z) for y. Observe that if there were two such challenges, then there would be two
Page  709: accepting conversations (t, c, z) and (t, c0 , z 0 ) for y, with c 6= c0 , and knowledge soundness would
Page  709: We point out that the above theorem holds unconditionally, for arbitrarily powerful adversaries.
Page  718: Let ⇧ = (P, V ) be a Sigma protocol for a relation R ✓ X ⇥ Y. Assume that conversations
Page  718: • on input (y, (t, z)) 2 Y ⇥ (T ⇥ Z), Check verifies that (t, c, z) is an accepting conversation for
Page  719: non-interactive proof system for R ✓ X ⇥ Y with proof space PS. To attack , an adversary A
Page  719: We say that the adversary wins the gave if Check (y, ⇡) = accept but y 2
Page  719: Definition 20.4. We say that       is existentially sound if for all efficient adversaries A, the
Page  719: In particular, let A be an adversary attacking the soundness of FS-⇧ as in the random oracle
Page  719: queries. Then there exists an adversary B that attacks the existential soundness of ⇧ as in
Page  719: means that (t, c, z) is a valid conversation for y, where c is the output of the random oracle at the
Page  719: adversary B then starts out by guessing (in advance) which of the A’s random oracle queries will
Page  720: Our definition of non-interactive zero knowledge (niZK) says that an efficient adversary cannot
Page  720: function H is modeled as a random oracle, and the adversary gets to make random oracle queries,
Page  720: as above. For a given adversary A, we define two experiments, Experiment 0 and Experiment 1.
Page  720: In both experiments, the adversary makes a series of queries to the challenger, each of which is of
Page  721: adversary A, the value niZKadv[A, , Sim] is negligible.
Page  721: We note that in the simulated world in Attack Game 20.3, for the proof queries, the adversary
Page  721: In particular, there exists a simulator Sim such that if A is an adversary that attacks FS-⇧ and
Page  723: always outputs a pair (t, z) such that (t, c, z) is an accepting conversation for y.
Page  723: with challenge space C. Let Sim be a simulator for ⇧, as above. For a given adversary A, we
Page  723: resulting conversation (t, c, z) to A.
Page  723: and gives the simulated conversation (t, c, z) to A.
Page  723: there exists a simulator Sim for ⇧, such that for every efficient adversary A, the value
Page  724: Q real conversations by Q simulated conversations.
Page  726: along with any two accepting conversations (t, c, z) and (t, c0 , z 0 ) with c 6= c0 , outputs a witness x
Page  726: statement y, along with two accepting conversations (t, c, z) and (t, c0 , z 0 ), with c 6= c0 , for y.
Page  726: More precisely, for a given adversary A, we define cSSadv[A, ⇧] to be the probability that A
Page  726: outputs y 2 Y \ LR and two accepting conversations (t, c, z) and (t, c0 , z 0 ), with c 6= c0 , for y. We
Page  726: adversaries A.
Page  726: challenge space of size N , then ⇧ is existentially sound. In particular, suppose A is an adversary
Page  726: Then there exists an efficient adversary B (whose running time is about twice that of A), such that
Page  729: conversation, and c := H(y, t).
Page  729: is the optimized Fiat-Shamir proof system. Specifically, you should show that any adversary
Page  730: adversary to come up with a triple (m, y, ⇡) such that (y, ⇡) such that V 0 (g k , m, y, ⇡) = accept
Page  730: infeasible to find a false statement y, along with two accepting conversations (t, c, z) and (t, c0 , z 0 ),
Page  730: More precisely, for a given adversary A, we define cSSSadv[A, ⇧] to be the probability that A
Page  730: outputs y 2 Y \ LR and two accepting conversations (t, c, z) and (t, c0 , z 0 ), with (c, z) 6= (c0 , z 0 ), for
Page  730: for all efficient adversaries A.
Page  730: allowing the adversary to output many attempts (y1 , ⇡1 ), . . . , (yr , ⇡r ), winning the game if
Page  730: / LR for some j = 1, . . . , r. For such a r-attempt adversary A,
Page  731: (a) Let    be an non-interactive proof system. Show that for every r-attempt adversary A at-
Page  731: tacking as above, there exists an adversary B attacking as in Attack Game 20.2, where
Page  731: oracle queries, then there exists an adversary B attacking ⇧ as in Attack Game 20.1, where
Page  731: Consider the following attack game played between an adversary A and a challenger. The adversary
Page  731: niZKadv[A, , Sim] is negligible for all efficient adversaries A and simESadv[A, , Sim] is negli-
Page  731: gible for all efficient adversaries A.
Page  732: (a) Suppose that ⇧ has unique responses (see Exercise 19.9). Show that for every adversary A
Page  732: soundness attack game, there exists an adversary B attacking ⇧ as in Attack Game 20.1, where
Page  732: (b) More generally, show that for every adversary A that makes at most Qro random oracle
Page  732: an adversary B attacking ⇧ as in Attack Game 20.1 and an adversary B 0 attacking ⇧ as in
Page  733: adversary A attacking S as in the random oracle version of Attack Game 13.2, and making
Page  733: at at most Qs signing queries and Qro random oracle queries, there exists a DDH adversary
Page  737: even if the user’s office file server is hacked, and an adversary is able to retrieve the session key from
Page  737: Tuesday, this should not compromise the session key from Monday or Wednesday. The adversary
Page  737: di↵erent streams, an adversary can mount a “two time pad” attack to obtain information about
Page  737: First, let us consider the powers of the adversary. Of course, an adversary may eavesdrop on
Page  737: available to any adversary. We shall also assume that an adversary may be able to modify messages,
Page  737: adversary to have complete control over the network. Of course, this is an overly pessimistic point
Page  737: of view, and a typical real-world adversary will not have this much power, but as usual, in analyzing
Page  737: purposes, we shall just assume that all such corrupt users are under the control of a single adversary.
Page  737: that k is known to the adversary. On the other hand, if Q is an honest user, we want the following
Page  737: secrecy: from the adversary’s point of view, the key k is indistinguishable from a random key;
Page  737: moreover, this should hold even if the adversary sees the session keys from other user instances.
Page  737: the adversary. In the weakest security definition, the adversary never compromises the long-term
Page  738: against an adversary that is able to compromise long-term keys of honest users. An even stronger
Page  738: notion, called “HSM security,” defends against an adversary that can read the ephemeral random
Page  738: To attack this protocol, an adversary might wait until steps 1 and 2 complete, and then “hijack”
Page  738: the session. Indeed, suppose that after step 2, the adversary steps in between P and Q, runs one
Page  738: the adversary can easily play “man in the middle”: whenever P encrypts a message under k1 ,
Page  738: the adversary decrypts the resulting ciphertext, and then re-encrypts the message, possibly after
Page  738: encrypted. Thus, the adversary is able to read the entire conversation between P and Q, modifying
Page  738: However, an adversary can also easily attack this protocol by playing “man in the middle”:
Page  738: 1. The adversary generates a shared key k1 with P , and other shared key k2 with Q;
Page  738: is encrypted under k1 , the adversary decrypts the corresponding ciphertext, and then re-
Page  739: 3. Similarly, whenever Q sends a message to P , which is encrypted under k2 , the adversary
Page  739: When the attack completes, the adversary can simply continue playing “man in the middle.”
Page  739: security, in which the adversary never compromises the long-term secret key of any honest user.
Page  741: Instead of ElGamal, one could use any other CCA-secure encryption scheme, such as ERSA .
Page  742: data ends up stored in memory which an adversary can read at some later time, then the adversary
Page  742: then it is important that Q securely erases the value — if this leaks, the adversary can obviously
Page  742: signature (see Section 19.2) are leaked, then the adversary can trivially compute the long-term
Page  742: single session key, the adversary can impersonate a user at any time, as often as he likes, to any
Page  742: • the adversary intercepts the message (c, , Cert Q ) from Q to P ;
Page  742: • the adversary computes c0 R Enc P (k 0 , id Q ), where k 0 is a session key of his choosing, and
Page  743: In the diagram, we write k to indicate a message blocked by the adversary, and k to indicate
Page  743: a message generated by the adversary. At the end of the attack, Q is holding the session key k,
Page  743: which is unknown to the adversary; however, P is holding the session key k 0 , which is known to the
Page  743: adversary.
Page  743: This type of attack, where the adversary is able to recover (or in this case, even choose) a session
Page  743: adversary knows only k 0 , but not k. Suppose that after the AKE protocol is run, the session key
Page  743: is used to secure a conversation between P and Q, using authenticated encryption. If P sends
Page  743: the first message in this conversation, then the adversary can obviously decrypt and read this
Page  743: message. Alternatively, if P receives the first message in the conversation, the adversary can make
Page  743: the conversation between the bank and customer is a sequence of request/response pairs: the
Page  743: made by the customer can be read by the adversary. Obviously, such a request may contain private
Page  743: want to share with an adversary. On the other hand, suppose P is the bank and Q is the customer.
Page  743: In this case, the adversary can send the bank a request to perform some arbitrary transaction on
Page  743: account controlled by the adversary.
Page  744: • first, the adversary eavesdrops on a conversation between P and Q; suppose P sent the
Page  744: by the adversary;
Page  744: • at some later time, the adversary, initiates a new run of the protocol with P ; P sends out
Page  744: the message Cert P ; the adversary intercepts this message, throws it away, and sends P the
Page  744: that the adversary does not obtain any direct information about k, nor is there a new instance of
Page  744: Q that shares this key. This type of attack, where the adversary is able to force a user instance to
Page  744: Even though the adversary obtains no direct information about k from the attack, this attack
Page  744: uses a stream cipher in its implementation. In this way, the adversary might be able to get P to
Page  744: Section 3.3.1, this might allow the adversary to obtain information about the encrypted data, via
Page  744: conversation between P and Q. Indeed, returning to our bank example, suppose P is the bank
Page  744: and Q is the customer. Then if in the first conversation, the customer requested certain amount of
Page  744: money to be transferred to a third party’s account, the adversary could simply replay this request,
Page  745: • after obtaining P ’s public key by some means, the adversary registers a new user R with the
Page  745: (r, Cert P ), the adversary intercepts this message, and instead delivers the message (r, Cert R )
Page  745: • when Q sends the message (c, , Cert Q ), the adversary delivers this message to P .
Page  745: At the end of the attack, P and Q share the session key k, which is unknown to the adversary;
Page  745: Note that to carry out the attack, R needs to “hijack” P ’s public key; that is, the adversary
Page  746: Here, R is a corrupt user, under control of the adversary. However, we assume that R has registered
Page  746: he has a corresponding secret key. Thus, in the last flow, the adversary may easily generate the
Page  746: At the end of this attack, P and Q share the session key k, which is unknown to the adversary;
Page  746: As in Variation 4, R is a corrupt user, under the control of the adversary, but we assume that
Page  747: attack on PKCS1 from Section 12.8.3 to recover a secret session key. The adversary could record
Page  747: • a key recovery attack, in which an adversary is able to recover (or even choose) a session
Page  747: • a replay attack, in which an adversary is able to force a user instance to re-use an old session
Page  747: • an identity misbinding attack, in which an adversary is able to make two users instances
Page  748: If an adversary obtains a user’s long-term secret key, the adversary may impersonate that user
Page  748: tion key is obtained by an adversary, then all previous session keys encrypted under that user’s
Page  748: encryption key become available to the adversary.
Page  749: only for signing, not encrypting. So compromising a signing key should not allow the adversary to
Page  749: Just as for protocol AKE1, we could use ERSA instead of ElGamal. However, this is not very
Page  749: practical, as key generation for RSA is much slower that ElGamal, and the key generation algorithm
Page  749: or P ’s value ↵ is leaked, the adversary can obviously recover the session key. Worse, if ↵ leaks,
Page  749: the adversary can even do more damage: he can impersonate P at any time, as often as he likes,
Page  749: to any user. This is because the adversary has both ↵ and P ’s signature on u = g ↵ , and this is all
Page  750: scheme is semantically secure. It is instructive to see why this is the case. Suppose the adversary lets
Page  750: suppose the adversary blocks this last message, but that Q uses the session key k to encrypt a
Page  750: plaintext m1 , sending the resulting ciphertext c1 out over the network for the adversary to see. At
Page  750: this point in time, neither P nor Q is compromised, and so we expect that the adversary should
Page  750: So suppose that at some later time, the adversary is able to obtain Q’s signing key. This allows
Page  750: the adversary to send a message (c0 , 20 , Cert Q ) to P , where c0 6= c and 20 is a valid signature on
Page  750: key k 0 that may be di↵erent from but related to k. For example, the adversary may be able to
Page  750: make k 0 k =           for some   6= 0 of the adversary’s choice. Now, suppose P encrypts a plaintext
Page  750: m2 under k 0 , and sends the resulting ciphertext c2 out over the network for the adversary to see.
Page  750: The adversary may now be able to carry out a related key attack on the symmetric cipher,
Page  750: run of the protocol is somehow leaked to an adversary, the consequences are devastating: using sk
Page  750: and P ’s signature on pk , the adversary can impersonate P at any time, as often as he likes, to any
Page  751: One might object to this whole line of inquiry: if an adversary is able to read this ephemeral data,
Page  751: limited time window we allow the adversary to evaluate f (LTS P , x) for any x of its choice, but not
Page  751: Of course, as in PFS security, we also consider a permanent compromise where the adversary
Page  751: (iii) P is an honest user, but the adversary accessed P ’s HSM during the (presumably short)
Page  751: the adversary ran the protocol, accessing the HSM just as the protocol itself would. However, it
Page  751: can also model much stronger attacks, in which the adversary can actively probe the HSM with
Page  753: indirectly, by an honest user instance, or directly, by the adversary. This is essential to achieve
Page  753: easily adapted to the public key setting), we require that it holds even for adversarially chosen
Page  753: demonstrates an attack in the HSM model, where the adversary makes a single oracle query to the
Page  754: Here, the adversary generates c0 by encrypting a session key k 0 of his choosing under pk . At the end
Page  754: of the protocol, P has the session key k 0 , which is known to the adversary; however, the adversary
Page  754: and Q thinks he is talking to R. To carry out this attack, the adversary needs the help of a corrupt
Page  755: — after all, it is Q’s key that is compromised, not P ’s. However, in this situation, the adversary
Page  755: • the adversary intercepts the message (pk , Cert P ) from P to Q;
Page  755: • the adversary runs the key generation algorithm to obtain (pk 0 , sk 0 )                R
Page  755: Sig Q (1, pk 0 , c, id P ), the adversary blocks this message and sends instead the message
Page  755: • when P responds with a signature            2 := Sig P (2, c, id Q ), the adversary simply forwards this
Page  755: At the end of the attack, P is holding the session key k 0 , which is unknown to the adversary;
Page  755: however, Q is holding the session key k, which is known to the adversary.
Page  756: Here is a key exposure attack that exploits the fact that the adversary can access P ’s HSM.
Page  756: First, the adversary runs (pk , sk ) R G(). It then somehow queries P ’s HSM to get a signature
Page  756: In the each run of the protocol, the adversary makes Q think he shares the key k with P , but in
Page  756: fact, Q shares the key k with the adversary. The adversary can get k by decrypting c using sk .
Page  757: and Q thinks he is talking to P . To carry out this attack, the adversary needs the help of a corrupt
Page  757: user Q, while in reality, he shares a key with the adversary. In some settings, it may be reasonable
Page  757: Very roughly speaking, identity protection means that an adversary cannot learn the identity of
Page  757: either one or both the users that are running the AKE protocol. Here, the adversary could either
Page  758: In the case where the adversary is a passive observer, and the two users running the protocol
Page  758: are honest, the goal is to prevent the adversary from learning the identity of either one or both of
Page  758: the users. We call this eavesdropping identity protection. When the adversary is one of the
Page  758: stations. Identity protection should prevent an adversary from tracking the location of a given
Page  758: mobile device. Certainly, identity protection against an eavesdropping adversary will help to prevent
Page  758: this. However, a more aggressive adversary may try to interact with a mobile device, pretending
Page  758: far enough for the adversary to have learned the identity of the mobile device.
Page  763: • The simulator must be able to simulate not only the conversation but also the session key.
Page  763: of this key, together with the conversation, could potentially be used by Q to implicate P .
Page  765: the conversation between P and Q may provide Q with evidence that it interacted with P . For
Page  766: fresh session key, which from an adversary’s point of view, should essentially appear to be uniformly
Page  767: two experiments to be indistinguishable from the point of view of an adversary. In each experiment,
Page  767: the adversary is interacting with a challenger, which is slightly di↵erent in each experiment.
Page  767: adversary is completely arbitrary; however, one can think of the adversary as really playing three
Page  767: Because our formal adversary also plays the role of higher level protocols that use session keys, we
Page  767: allow the adversary free access to session keys obtained by honest users, which may at first seem
Page  767: counter-intuitive, since one normally thinks of session keys as being hidden from the adversary. See
Page  767: TTP. Now the adversary can make a number of queries to the challenger:
Page  767: by the adversary. Behind the scenes, the challenger runs an instance of the registration protocol
Page  767: with the TTP. This protocol is run in a secure fashion: the adversary cannot see or influence
Page  767: Register corrupt user: Here, the adversary essentially is allowed to run the registration protocol
Page  767: ated with a previously registered honest user I.user = U , which is specified by the adversary.
Page  767: The adversary also supplies a role I.role 2 {left, right}. The challenger initializes the internal
Page  767: Deliver protocol message: The adversary specifies a running honest user instance I along with
Page  768: finished status — are handed to the adversary.
Page  768: Deliver TTP message: This is only used in the online TTP setting. The adversary gives a
Page  768: to the logic of the TTP. Any resulting message mout is given to the adversary.
Page  768: There is one further restriction: the adversary is never allowed to register an honest user’s ID
Page  768: does not maintain any state information for corrupt users: this is the adversary’s responsibility.
Page  768: Note that the adversary is never allowed to obtain the long-term secret key of an honest user or
Page  768: the adversary sees. We shall define the log as a sequence of entries, generated as follows.
Page  769: when a user instance I finishes with a session key I.sk, instead of giving the adversary I.sk, the
Page  769: challenger instead gives the adversary an e↵ective session key I.esk, which is determined (in part)
Page  769: That finishes the description of Experiments 0 and 1. If Wb is the event that an adversary A
Page  769: efficient adversaries A, the value sKEadv[A, ⇧, pf] is negligible.
Page  770: authors use the notion of “matching conversations”, which roughly means that two user instances
Page  770: are partners if their conversations match up bit-by-bit. This can sometimes be overly restrictive,
Page  770: as it may require the use of strongly secure signatures to ensure that conversations are highly non-
Page  770: malleable. Instead of matching conversations, some authors use a notion of “session IDs” to specify
Page  770: Roughly speaking, such a requirement says that if an adversary interacts with the challenger as in
Page  770: Experiment 0 above, then for any pair of honest user instances, if the adversary faithfully transmits
Page  770: is the adversary given the session keys when the goal of the protocol is supposedly to protect the
Page  771: adversary B attacking the AKE protocol comprises not only our original attacker A, but also the
Page  771: logic of the honest users, outside of the internals of the AKE protocol itself. This adversary B does
Page  772: Compromise user: The adversary specifies an honest user U . The challenger gives the long-term
Page  772: secret key U.ltk to the adversary. Although we still say that U is an honest user, we say U is
Page  772: These are the only changes. We denote by pfsKEadv[A, ⇧, pf] an adversary A’s advantage
Page  773: exists an efficiently computable partner function pf such that for all efficient adversaries A, the
Page  773: Remark 21.3. Even after an honest user is compromised, the adversary may continue delivering
Page  773: messages to user instances belonging to that user. We must allow this, as the adversary does not
Page  773: user instances. For consistency and simplicity, we also allow the adversary to continue to initialize
Page  774: adversarial access to the HSM:
Page  774: Access HSM: The adversary specifies an honest user U and a value x. The challenger responds
Page  774: (ii) the total number of adversarial HSM accesses on user U is greater than the number of
Page  774: We denote by hsmKEadv[A, ⇧, pf] an adversary A’s advantage against a protocol ⇧ in this
Page  776: seen by a right instance J. If there is no such instance J, then the adversary must have himself
Page  776: We show how this adversary can be used to solve the CDH problem for the problem instance
Page  777: attack (directly by the adversary, as well as by honest user instances). We can still use the adversary
Page  777: has to be modified. One can make a similar argument as above, but now we use the adversary to
Page  777: session keys by a random session keys is not detectable, unless the adversary can compute g µ⌫ given
Page  777: the analysis, in using the adversary to compute g ↵⌫ from g ↵ and g ⌫ , we will have to be able to
Page  778: adversarially chosen.
Page  780: • A signature (under server’s signing key) on the conversation so far.
Page  780: • A tag computed using HMAC (see Section 8.7) on the conversation so far.
Page  780: • A signature (under client’s signing key) on the conversation so far. This message is only sent
Page  780: • A tag computed using HMAC (see Section 8.7) on the conversation so far.
Page  782: steps in (21.7). This ensures forward secrecy in case the pre-shared key is leaked to an adversary
Page  783: adversary can record the first flow from the client to the server and replay it at a later time.
Page  783: balance. The adversary can replay this first flow at any time and count the number of bytes in
Page  783: account balance, the adversary can monitor P ’s account balance by repeatedly replaying the request
Page  783: from P . Even worse, if the request from P is a bill payment, the adversary can replay the request
Page  783: large distributed web application. The adversary can record a client request in North America and
Page  784: Suppose a client has an account with a server, and that an adversary wants to discover the client’s
Page  784: a phishing attack, an adversary bypasses the secure channel by simply tricking the client into
Page  784: entering his user ID and password on a fake login page that belongs to the adversary, rather than
Page  784: but end up at a web site controlled by the adversary.
Page  784: to do. Of course, the adversary has to design the page so that the content displayed is very
Page  785: even a more discerning client may be easily fooled if, for example, the adversary controls
Page  785: valid certificate that was issued by the CA to the adversary, rather than to the server;
Page  785: First of all, the client identifies himself to the server, but not vice versa, so if the client is tricked
Page  785: into visiting the fake login page, the adversary may not get his password, but may be able to cause
Page  785: Worse, the adversary could mount a man-in-the-middle attack. Again, the adversary sets up
Page  785: authenticated secure channel with the server. Now the adversary simply plays “man in the middle,”
Page  785: forwarding the messages in the identification protocol from the server to client, and vice versa. Now,
Page  785: not only can the adversary try to obtain sensitive information from the client, as above, but since
Page  785: the adversary is now logged into the server under the client’s user ID, he can also cause damage
Page  785: on the server side. For example, if the server is a bank, and the client is a customer, the adversary
Page  785: can transfer money from the customer’s account to a bank account controlled by the adversary.
Page  786: belongs to a relatively small dictionary D of common passwords. Also suppose that the adversary
Page  786: establishes a channel with the client, via phishing. Playing the role of server, the adversary sends
Page  786: a nonce r to the client, who responds with s, v := H(pw , chb, 0, r, s). At this point, the adversary
Page  786: quits the protocol. Having obtained v, the adversary now performs a brute-force search for the
Page  786: clients password, as follows: for each pw 0 2 D, the adversary computes H(pw 0 , r), and tests if this
Page  786: is equal to v. If he finds such a pw 0 , it is very likely that pw 0 = pw , and so the adversary has
Page  786: makes matters even worse: now the adversary can simply set up a normal one-sided authenti-
Page  786: H(pw , chb, 0, r, s) to the adversary. Now the adversary can carry out an o✏ine dictionary attack
Page  786: Finally, we remark that even without phishing, the adversary can always perform an online
Page  787: not very secure in practice: via a phishing attack, an adversary can trick a client into divulging
Page  787: his password to the adversary; moreover, we saw that even if a challenge-response identification
Page  787: protocol is used, a phisher can still obtain enough information from the client so that the adversary
Page  787: protocol is inherently vulnerable to an online dictionary attack : an adversary can always guess
Page  787: possible; in particular, we might hope that an adversary cannot mount an o✏ine dictionary attack.
Page  787: Unfortunately, if pw is a weak password, then an eavesdropping adversary can easily carry out
Page  787: Our adversary eavesdrops on a run of the protocol between P and Q, obtaining the values r
Page  788: encryption c of m under the key k. The adversary intercepts c, and then does the following:
Page  788: session key that may be leaked to the adversary, besides a plaintext/ciphertext pair. For example,
Page  788: attack by an eavesdropping adversary. We next present a PAKE protocol that does not su↵er from
Page  788: by an eavesdropping adversary.
Page  789: Suppose an adversary eavesdrops on a conversation between P and Q. He obtains random
Page  789: Intuitively, for a dictionary attack to succeed, the adversary will have to query the random oracle
Page  789: adversary tell whether pw 0 = pw , for example, by using the value k 0 of the oracle at that point to
Page  789: cle at any relevant point is negligible. Indeed, if an adversary can make a relevant query with
Page  789: non-negligible probability, then we could use this adversary to solve the CDH problem with non-
Page  789: and v := t, and give u and v to our eavesdropping adversary. Now, the adversary will make a
Page  789: employ this guessing strategy; nevertheless, if the adversary has a non-negligible chance of making
Page  789: by an eavesdropping adversary. However, as we now illustrate, protocol PAKE1 does not provide
Page  789: security against a dictionary attack by a active adversary, that is, an adversary that participates
Page  789: Our adversary works as follows. First, he plays the role of Q in PAKE1 . The honest user P sends
Page  789: u to the adversary, who simply follows the protocol, computing
Page  790: sends out an encryption c of m under the key k. The adversary intercepts c, and then does the
Page  790: o✏ine dictionary attack by an eavesdropping adversary, it is vulnerable to an o✏ine dictionary
Page  790: attack by an active adversary.
Page  790: attack, by both passive and active adversaries. Like PAKE1 , protocol PAKE2 makes use of a cyclic
Page  790: attacks by either an eavesdropping or active adversary, under the CDH assumption, and modeling
Page  790: First, consider an adversary that eavesdrops on a run of the protocol between honest user P
Page  790: and honest user Q. He obtains a conversation (u, v). The session key computed by P and Q is
Page  791: Intuitively, the adversary’s goal is to query the random oracle at as many relevant points as possible,
Page  791: Now suppose we have an adversary that can efficiently solve the problem in the statement of
Page  791: the lemma with non-negligible probability. We show how to use this adversary to solve the CDH
Page  791: and then we give the adversary
Page  791: Suppose now that the adversary computes for us 2 Zq and w 2 G such that w = [u/a , v/b ].
Page  791: Next, consider an active adversary that engages in the protocol with an honest user. We consider
Page  791: the case where the adversary plays the role of Q, and the honest user is P — the argument in the
Page  791: Now, in the adversary’s attack, he obtains the first message u from P , which is just a random
Page  791: group element. Next, the adversary computes a group element v in some way, and sends this to P
Page  791: — the adversary may compute v in any way he likes, possibly in some devious way that depends on
Page  791: u. As usual, P now computes the session key as in (21.8), and the adversary’s goal is to evaluate
Page  791: the random oracle H at as many relevant points, as in (21.9), as possible. Of course, an adversary
Page  792: Proof. We consider two types of adversaries: a Type I adversary solves the problem with 1 = 0
Page  792: and 2 6= 0, and a Type II adversary solves the problem with 1 6= 0, 2 6= 0, and 1 6= 2 . We
Page  792: show how an adversary of either type can be used to solve the CDH problem.
Page  792: Suppose we have a Type I adversary, and that we are given an instance (s, t) of the CDH
Page  792: and give the adversary
Page  792: The adversary computes for us      6= 0 and w1 , w2 such that
Page  792: Now suppose we have a Type II adversary, and that we are given an instance (s, t) of the CDH
Page  792: and give the adversary
Page  792: The adversary computes for us     1 , 2 and w1 , w2 such that                   1 6= 0,     2 6= 0, and   1 6=   2 , and
Page  793: like to provide some defense against is a server compromise, in which an adversary obtains the
Page  793: server’s password file. Given the password file, the adversary can certainly impersonate the server;
Page  793: however, we would like to make it as hard as possible for the adversary to impersonate a client,
Page  793: Given the password file, an adversary can always mount an o✏ine dictionary attack to discover
Page  793: a given client’s password: the adversary can just run both the client and server side of the protocol,
Page  793: on the server’s side. Ideally, this would be all the adversary could do.
Page  793: by both eavesdropping and active adversaries. The roles of the two users in that protocol are quite
Page  793: this implementation is undesirable, as an adversary that compromises the server immediately the
Page  793: the best an adversary can do to impersonate a client is an o✏ine dictionary attack. Like PAKE2 ,
Page  793: if the server Q is compromised in protocol PAKE+          2 , and the adversary obtains ⇡0 and c. At this
Page  793: point, the adversary could attempt an o✏ine dictionary attack, as follows: evaluate H 0 at points
Page  793: If this succeeds, then with high probability, pw 0 = pw , and the adversary can easily impersonate
Page  793: under the CDH assumption, the adversary cannot impersonate the client.
Page  793: To prove this property, first suppose that an adversary compromises the server, then attempts
Page  793: that the adversary obtains ⇡0 and c = g ⇡0 . Now suppose the dictionary attack fails, which means
Page  793: that the adversary has not evaluated H 0 at the point (pw , id P , id Q ). The value ⇡1 is completely
Page  793: random, and the adversary has no other information about ⇡1 , other than the fact that c = g ⇡1 .
Page  794: with v := g b⇡0 for random 2 Zq . Now, the adversary knows ⇡0 , and therefore can compute the
Page  794: are random group elements from the adversary’s point of view, computing [c, e] is tantamount to
Page  794: The complication we have not addressed in this argument is that the adversary may also interact
Page  794: oracle, essentially giving the adversary an oracle for recognizing DH-tuples of the form (g, g ⇡1 , ·, ·).
Page  794: As it is now, if an adversary runs protocol PAKE2 or PAKE+    2 with an honest user, using a guess at
Page  794: the password that turns out to be wrong, then the adversary will have the wrong session key, but
Page  795: a phishing adversary can still attempt to bypass the PAKE protocol entirely. For example, an
Page  795: adversary may lure a client to a fake login page on his web browser, and the client enters his
Page  795: adversary, rather than being processed by a PAKE protocol. This problem can be defended against
Page  795: way, an adversary’s ability to attack the PAKE protocol will be limited by his ability to mount a
Page  796: (a) Suppose the signatures Sig Q (u, v) and Sig P (u, v) are not encrypted. Show that an adversary
Page  796: session key to encrypt messages using E and the key k, and that the adversary can force
Page  796: to encrypt the signatures and k is used as the session key. Show that an adversary can still
Page  796: Hint: The adversary does not follow the usual protocol for registration with the CA.
Page  797: couple of queries to user P ’s HSM, the adversary can impersonate P at will to any user Q, and
Page  797: the adversary will know the low-order bit of each resulting session key. Assume session keys are bit
Page  797: password chosen at random from some small dictionary D. Show that an active adversary can
Page  797: recover psk after a single attempted key exchange with Q. You may assume that the adversary
Page  812: [25] D. Boneh. Simplified OAEP for the RSA and rabin functions. In Advances in Cryptology -
Page  813: [29] J. A. Buchmann, E. Dahmen, and M. Schneider. Merkle tree traversal revisited. PQCrypto,
Page  813: [38] Y. Desmedt and A. Oldyzko. A chosen text attack on the rsa cryptosystem and some dis-
Page  814: [51] E. Fujisaki, T. Okamoto, D. Pointcheval, and J. Stern. RSA-OAEP is secure under the RSA
Page  814: [54] D. Genkin, A. Shamir, and E. Tromer. RSA key extraction via low-bandwidth acoustic crypt-
Page  815: [72] J. Jonsson and B. Kaliski. On the security of RSA encryption in TLS. In CRYPTO, pages
Page  816: [82] J. Manger. A chosen ciphertext attack on RSA optimal asymmetric encryption padding
Page  818: [112] V. Shoup. A composition theorem for universal one-way hash functions. In Proceedings of
Page  818: [119] M. Szydlo. Merkle tree traversal in log space and time. In Eurocrypt, volume 3027, pages
